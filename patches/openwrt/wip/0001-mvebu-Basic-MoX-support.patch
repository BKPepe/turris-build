From c6f10ebba861af03e23231260d8a3f76d2d45dba Mon Sep 17 00:00:00 2001
From: Michal Hrusecky <Michal@Hrusecky.net>
Date: Wed, 7 Mar 2018 17:10:32 +0100
Subject: [PATCH] mvebu: Basic MoX support

Adding basic support for CZ.NICs MoX board.

Signed-off-by: Michal Hrusecky <Michal@Hrusecky.net>
---
 target/linux/mvebu/config-4.14                |     2 +
 target/linux/mvebu/image/cortex-a53.mk        |    11 +
 .../0034-From-Ken-Ma-make-marvell.com.patch   |    54 +
 ...v88e6xxx-88E6141-6341-SERDES-support.patch |    87 +
 ...-support-for-Turris-Mox-SFP-cage-mod.patch |   316 +
 ...etif_receive_skb-instead-of-GRO-on-A.patch |    45 +
 ...support-for-Armada-37xx-CPU-watchdog.patch |   440 +
 ...no-uhs-voltage-cap-to-MMC-host-struc.patch |    57 +
 ...l-Add-DTS-files-for-Turris-Mox-and-i.patch |   463 +
 ...ecure-remove-null-check-before-kfree.patch |    33 +
 ...cure-do-not-use-areq-result-for-part.patch |    63 +
 ...nside-secure-remove-extra-empty-line.patch |    28 +
 ...-inside-secure-fix-typo-in-a-comment.patch |    29 +
 ...-inside-secure-remove-useless-memset.patch |    30 +
 ...cure-refrain-from-unneeded-invalidat.patch |    91 +
 ...cure-EBUSY-is-not-an-error-on-async-.patch |    35 +
 ...cure-move-cipher-crypto-mode-to-requ.patch |    76 +
 ...cure-remove-unused-parameter-in-inva.patch |    74 +
 ...cure-move-request-dequeueing-into-a-.patch |   204 +
 ...cure-use-threaded-IRQs-for-result-ha.patch |   136 +
 ...-secure-dequeue-all-requests-at-once.patch |   179 +
 ...inside-secure-increase-the-ring-size.patch |    37 +
 ...cure-acknowledge-the-result-requests.patch |    62 +
 ...cure-handle-more-result-requests-whe.patch |    70 +
 ...cure-retry-to-proceed-the-request-la.patch |   103 +
 ...9-crypto-inside-secure-EIP97-support.patch |   841 ++
 ...cure-make-function-safexcel_try_push.patch |    38 +
 ...cure-do-not-overwrite-the-threshold-.patch |    40 +
 ...cure-do-not-process-request-if-no-co.patch |    41 +
 ...cure-keep-the-requests-push-pop-sync.patch |   136 +
 ...cure-unmap-the-result-in-the-hash-se.patch |    42 +
 ...cure-move-hash-result-dma-mapping-to.patch |   115 +
 ...cure-move-cache-result-dma-mapping-t.patch |   152 +
 ...cure-fix-missing-unlock-on-error-in-.patch |    36 +
 ...-secure-improve-clock-initialization.patch |    48 +
 ...cure-fix-clock-resource-by-adding-a-.patch |   146 +
 ...cure-move-the-digest-to-the-request-.patch |   161 +
 ...cure-fix-typo-s-allways-always-in-a-.patch |    45 +
 ...secure-fix-a-typo-in-a-register-name.patch |    45 +
 ...e-secure-improve-the-send-error-path.patch |    50 +
 ...cure-do-not-access-buffers-mapped-to.patch |    46 +
 ...de-secure-improve-the-skcipher-token.patch |    36 +
 ...cure-the-context-ipad-opad-should-us.patch |    42 +
 ...to-inside-secure-hmac-sha256-support.patch |   174 +
 ...to-inside-secure-hmac-sha224-support.patch |   110 +
 .../0088-mox-new-prototype-changes.patch      | 11417 ++++++++++++++++
 ...metic-fixes-to-phylink-sfp-sfp-bus.c.patch |   233 +
 .../9085-sfp-fix-sparse-warning.patch         |    30 +
 ...-t-guess-support-from-connector-type.patch |    57 +
 ...rt-for-1000Base-PX-and-1000Base-BX10.patch |    60 +
 50 files changed, 16866 insertions(+)
 create mode 100644 target/linux/mvebu/patches-4.14/0034-From-Ken-Ma-make-marvell.com.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0036-net-dsa-mv88e6xxx-88E6141-6341-SERDES-support.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0037-drivers-gpio-Add-support-for-Turris-Mox-SFP-cage-mod.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0038-net-mvneta-Use-netif_receive_skb-instead-of-GRO-on-A.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0039-watchdog-Add-support-for-Armada-37xx-CPU-watchdog.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0040-drivers-mmc-Add-no-uhs-voltage-cap-to-MMC-host-struc.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0041-ARM64-dts-marvell-Add-DTS-files-for-Turris-Mox-and-i.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0043-crypto-inside-secure-remove-null-check-before-kfree.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0044-crypto-inside-secure-do-not-use-areq-result-for-part.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0045-crypto-inside-secure-remove-extra-empty-line.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0046-crypto-inside-secure-fix-typo-in-a-comment.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0047-crypto-inside-secure-remove-useless-memset.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0048-crypto-inside-secure-refrain-from-unneeded-invalidat.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0049-crypto-inside-secure-EBUSY-is-not-an-error-on-async-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0050-crypto-inside-secure-move-cipher-crypto-mode-to-requ.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0051-crypto-inside-secure-remove-unused-parameter-in-inva.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0052-crypto-inside-secure-move-request-dequeueing-into-a-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0053-crypto-inside-secure-use-threaded-IRQs-for-result-ha.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0054-crypto-inside-secure-dequeue-all-requests-at-once.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0055-crypto-inside-secure-increase-the-ring-size.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0056-crypto-inside-secure-acknowledge-the-result-requests.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0057-crypto-inside-secure-handle-more-result-requests-whe.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0058-crypto-inside-secure-retry-to-proceed-the-request-la.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0059-crypto-inside-secure-EIP97-support.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0060-crypto-inside-secure-make-function-safexcel_try_push.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0061-crypto-inside-secure-do-not-overwrite-the-threshold-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0064-crypto-inside-secure-do-not-process-request-if-no-co.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0066-crypto-inside-secure-keep-the-requests-push-pop-sync.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0067-crypto-inside-secure-unmap-the-result-in-the-hash-se.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0068-crypto-inside-secure-move-hash-result-dma-mapping-to.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0069-crypto-inside-secure-move-cache-result-dma-mapping-t.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0071-crypto-inside-secure-fix-missing-unlock-on-error-in-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0072-crypto-inside-secure-improve-clock-initialization.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0073-crypto-inside-secure-fix-clock-resource-by-adding-a-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0074-crypto-inside-secure-move-the-digest-to-the-request-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0075-crypto-inside-secure-fix-typo-s-allways-always-in-a-.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0076-crypto-inside-secure-fix-a-typo-in-a-register-name.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0077-crypto-inside-secure-improve-the-send-error-path.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0078-crypto-inside-secure-do-not-access-buffers-mapped-to.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0079-crypto-inside-secure-improve-the-skcipher-token.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0080-crypto-inside-secure-the-context-ipad-opad-should-us.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0081-crypto-inside-secure-hmac-sha256-support.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0082-crypto-inside-secure-hmac-sha224-support.patch
 create mode 100644 target/linux/mvebu/patches-4.14/0088-mox-new-prototype-changes.patch
 create mode 100644 target/linux/mvebu/patches-4.14/9084-net-phy-Cosmetic-fixes-to-phylink-sfp-sfp-bus.c.patch
 create mode 100644 target/linux/mvebu/patches-4.14/9085-sfp-fix-sparse-warning.patch
 create mode 100644 target/linux/mvebu/patches-4.14/9086-sfp-don-t-guess-support-from-connector-type.patch
 create mode 100644 target/linux/mvebu/patches-4.14/9087-sfp-add-support-for-1000Base-PX-and-1000Base-BX10.patch

diff --git a/target/linux/mvebu/config-4.14 b/target/linux/mvebu/config-4.14
index f5e9876..8c77688 100644
--- a/target/linux/mvebu/config-4.14
+++ b/target/linux/mvebu/config-4.14
@@ -34,6 +34,7 @@ CONFIG_ARM=y
 CONFIG_ARMADA_370_CLK=y
 CONFIG_ARMADA_370_XP_IRQ=y
 CONFIG_ARMADA_370_XP_TIMER=y
+CONFIG_ARMADA_37XX_WATCHDOG=y
 CONFIG_ARMADA_38X_CLK=y
 CONFIG_ARMADA_THERMAL=y
 CONFIG_ARMADA_XP_CLK=y
@@ -223,6 +224,7 @@ CONFIG_GPIOLIB=y
 CONFIG_GPIOLIB_IRQCHIP=y
 CONFIG_GPIO_GENERIC=y
 CONFIG_GPIO_GENERIC_PLATFORM=y
+CONFIG_GPIO_MOX_SFP=y
 CONFIG_GPIO_MVEBU=y
 CONFIG_GPIO_PCA953X=y
 CONFIG_GPIO_PCA953X_IRQ=y
diff --git a/target/linux/mvebu/image/cortex-a53.mk b/target/linux/mvebu/image/cortex-a53.mk
index 89c3363..2095a5c 100644
--- a/target/linux/mvebu/image/cortex-a53.mk
+++ b/target/linux/mvebu/image/cortex-a53.mk
@@ -13,6 +13,17 @@ define Device/globalscale-espressobin
 endef
 TARGET_DEVICES += globalscale-espressobin
 
+define Device/cznic-mox
+  KERNEL_NAME := Image dtbs
+  KERNEL := kernel-bin
+  DEVICE_TITLE := MOX (CZ.NICs Marvell Armada 3720 Based Board)
+  DEVICE_PACKAGES := kmod-usb-core kmod-usb2 kmod-usb3 kmod-usb-storage
+  DEVICE_DTS := armada-3720-turris-mox-sd armada-3720-turris-mox-sd-sfp armada-3720-turris-mox-sd-topaz
+  DTS_DIR := $(DTS_DIR)/marvell
+  SUPPORTED_DEVICES := cznic,mox
+endef
+TARGET_DEVICES += cznic-mox
+
 define Device/armada-3720-db
   KERNEL_NAME := Image
   KERNEL := kernel-bin
diff --git a/target/linux/mvebu/patches-4.14/0034-From-Ken-Ma-make-marvell.com.patch b/target/linux/mvebu/patches-4.14/0034-From-Ken-Ma-make-marvell.com.patch
new file mode 100644
index 0000000..3d670dd
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0034-From-Ken-Ma-make-marvell.com.patch
@@ -0,0 +1,54 @@
+From 866ee62843120457bd14a3a3ce534040ab75ef4b Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 14:31:41 +0200
+Subject: [PATCH 34/90] From: Ken Ma <make@marvell.com>
+
+pinctrl: armada-37xx: Correct mpp definitions
+
+This patch corrects below mpp definitions:
+ - The sdio_sb group is composed of 6 pins and not 5;
+ - The rgmii group contains pins mpp2[17:6] and not mpp2[19:6];
+ - Pin of group "pmic0" is mpp1[6] but not mpp1[16];
+ - Pin of group "pmic1" is mpp1[7] but not mpp1[17];
+ - A new group "smi" is added in A0 with 2 pins - mpp2[19:18], its
+   bitmask is bit4;
+ - Group "pcie1" has 3 pins in A0 - mpp2[5:3], its bit mask is
+   bit5 | bit9 | bit10 but not bit4;
+ - Group "ptp" has 3 pins in A0 as Z1, but its bitmask is changed to
+   bit11 | bit12 | bit13.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ drivers/pinctrl/mvebu/pinctrl-armada-37xx.c | 9 +++++----
+ 1 file changed, 5 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/pinctrl/mvebu/pinctrl-armada-37xx.c b/drivers/pinctrl/mvebu/pinctrl-armada-37xx.c
+index 44897d4b9d88..67a9a747a1b9 100644
+--- a/drivers/pinctrl/mvebu/pinctrl-armada-37xx.c
++++ b/drivers/pinctrl/mvebu/pinctrl-armada-37xx.c
+@@ -157,8 +157,8 @@ static struct armada_37xx_pin_group armada_37xx_nb_groups[] = {
+ 	PIN_GRP_GPIO("pwm1", 12, 1, BIT(4), "pwm"),
+ 	PIN_GRP_GPIO("pwm2", 13, 1, BIT(5), "pwm"),
+ 	PIN_GRP_GPIO("pwm3", 14, 1, BIT(6), "pwm"),
+-	PIN_GRP_GPIO("pmic1", 17, 1, BIT(7), "pmic"),
+-	PIN_GRP_GPIO("pmic0", 16, 1, BIT(8), "pmic"),
++	PIN_GRP_GPIO("pmic1", 7, 1, BIT(7), "pmic"),
++	PIN_GRP_GPIO("pmic0", 6, 1, BIT(8), "pmic"),
+ 	PIN_GRP_GPIO("i2c2", 2, 2, BIT(9), "i2c"),
+ 	PIN_GRP_GPIO("i2c1", 0, 2, BIT(10), "i2c"),
+ 	PIN_GRP_GPIO("spi_cs1", 17, 1, BIT(12), "spi"),
+@@ -182,8 +182,9 @@ static struct armada_37xx_pin_group armada_37xx_sb_groups[] = {
+ 	PIN_GRP_GPIO("usb2_drvvbus1", 1, 1, BIT(1), "drvbus"),
+ 	PIN_GRP_GPIO("sdio_sb", 24, 6, BIT(2), "sdio"),
+ 	PIN_GRP_GPIO("rgmii", 6, 12, BIT(3), "mii"),
+-	PIN_GRP_GPIO("pcie1", 3, 2, BIT(4), "pcie"),
+-	PIN_GRP_GPIO("ptp", 20, 3, BIT(5), "ptp"),
++	PIN_GRP_GPIO("smi", 18, 2, BIT(4), "smi"),
++	PIN_GRP_GPIO("pcie1", 3, 3, BIT(5) | BIT(9) | BIT(10), "pcie"),
++	PIN_GRP_GPIO("ptp", 20, 3, BIT(11) | BIT(12) | BIT(13), "ptp"),
+ 	PIN_GRP("ptp_clk", 21, 1, BIT(6), "ptp", "mii"),
+ 	PIN_GRP("ptp_trig", 22, 1, BIT(7), "ptp", "mii"),
+ 	PIN_GRP_GPIO_3("mii_col", 23, 1, BIT(8) | BIT(14), 0, BIT(8), BIT(14),
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0036-net-dsa-mv88e6xxx-88E6141-6341-SERDES-support.patch b/target/linux/mvebu/patches-4.14/0036-net-dsa-mv88e6xxx-88E6141-6341-SERDES-support.patch
new file mode 100644
index 0000000..18f6a74
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0036-net-dsa-mv88e6xxx-88E6141-6341-SERDES-support.patch
@@ -0,0 +1,87 @@
+From f36645628fa16eda119347d1619c97e4b4cdaf91 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 16:23:53 +0200
+Subject: [PATCH 36/90] net: dsa: mv88e6xxx: 88E6141/6341 SERDES support
+
+The 88E6141/6341 switches (also known as Topaz) have 1 SGMII lane,
+which can be configured the same way as the SERDES lane on 88E6390.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ drivers/net/dsa/mv88e6xxx/chip.c   |  2 ++
+ drivers/net/dsa/mv88e6xxx/serdes.c | 20 ++++++++++++++++++++
+ drivers/net/dsa/mv88e6xxx/serdes.h |  3 +++
+ 3 files changed, 25 insertions(+)
+
+diff --git a/drivers/net/dsa/mv88e6xxx/chip.c b/drivers/net/dsa/mv88e6xxx/chip.c
+index bd74c45f5495..fae362020305 100644
+--- a/drivers/net/dsa/mv88e6xxx/chip.c
++++ b/drivers/net/dsa/mv88e6xxx/chip.c
+@@ -2426,6 +2426,7 @@ static const struct mv88e6xxx_ops mv88e6141_ops = {
+ 	.reset = mv88e6352_g1_reset,
+ 	.vtu_getnext = mv88e6352_g1_vtu_getnext,
+ 	.vtu_loadpurge = mv88e6352_g1_vtu_loadpurge,
++	.serdes_power = mv88e6341_serdes_power,
+ };
+ 
+ static const struct mv88e6xxx_ops mv88e6161_ops = {
+@@ -2924,6 +2925,7 @@ static const struct mv88e6xxx_ops mv88e6341_ops = {
+ 	.reset = mv88e6352_g1_reset,
+ 	.vtu_getnext = mv88e6352_g1_vtu_getnext,
+ 	.vtu_loadpurge = mv88e6352_g1_vtu_loadpurge,
++	.serdes_power = mv88e6341_serdes_power,
+ };
+ 
+ static const struct mv88e6xxx_ops mv88e6350_ops = {
+diff --git a/drivers/net/dsa/mv88e6xxx/serdes.c b/drivers/net/dsa/mv88e6xxx/serdes.c
+index f3c01119b3d1..bfecbdf5f64d 100644
+--- a/drivers/net/dsa/mv88e6xxx/serdes.c
++++ b/drivers/net/dsa/mv88e6xxx/serdes.c
+@@ -227,3 +227,23 @@ int mv88e6390_serdes_power(struct mv88e6xxx_chip *chip, int port, bool on)
+ 
+ 	return 0;
+ }
++
++int mv88e6341_serdes_power(struct mv88e6xxx_chip *chip, int port, bool on)
++{
++	int err;
++	u8 cmode;
++
++	if (port != 5)
++		return 0;
++
++	err = mv88e6xxx_port_get_cmode(chip, port, &cmode);
++	if (err)
++		return err;
++
++	if ((cmode == MV88E6XXX_PORT_STS_CMODE_1000BASE_X) ||
++	     (cmode == MV88E6XXX_PORT_STS_CMODE_SGMII) ||
++	     (cmode == MV88E6XXX_PORT_STS_CMODE_2500BASEX))
++		return mv88e6390_serdes_sgmii(chip, MV88E6341_ADDR_SERDES, on);
++
++	return 0;
++}
+diff --git a/drivers/net/dsa/mv88e6xxx/serdes.h b/drivers/net/dsa/mv88e6xxx/serdes.h
+index 5c1cd6d8e9a5..87bfafc5fb29 100644
+--- a/drivers/net/dsa/mv88e6xxx/serdes.h
++++ b/drivers/net/dsa/mv88e6xxx/serdes.h
+@@ -19,6 +19,8 @@
+ #define MV88E6352_ADDR_SERDES		0x0f
+ #define MV88E6352_SERDES_PAGE_FIBER	0x01
+ 
++#define MV88E6341_ADDR_SERDES		0x15
++
+ #define MV88E6390_PORT9_LANE0		0x09
+ #define MV88E6390_PORT9_LANE1		0x12
+ #define MV88E6390_PORT9_LANE2		0x13
+@@ -42,6 +44,7 @@
+ #define MV88E6390_SGMII_CONTROL_LOOPBACK	BIT(14)
+ #define MV88E6390_SGMII_CONTROL_PDOWN		BIT(11)
+ 
++int mv88e6341_serdes_power(struct mv88e6xxx_chip *chip, int port, bool on);
+ int mv88e6352_serdes_power(struct mv88e6xxx_chip *chip, int port, bool on);
+ int mv88e6390_serdes_power(struct mv88e6xxx_chip *chip, int port, bool on);
+ 
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0037-drivers-gpio-Add-support-for-Turris-Mox-SFP-cage-mod.patch b/target/linux/mvebu/patches-4.14/0037-drivers-gpio-Add-support-for-Turris-Mox-SFP-cage-mod.patch
new file mode 100644
index 0000000..30d7a58
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0037-drivers-gpio-Add-support-for-Turris-Mox-SFP-cage-mod.patch
@@ -0,0 +1,316 @@
+From 005ed67f8eea65bc5f90b0b32fe232de20652f99 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 15:22:21 +0200
+Subject: [PATCH 37/90] drivers: gpio: Add support for Turris Mox SFP cage
+ module GPIOs
+
+The SFP GPIOs on the SFP cage module of Turris Mox can be configured
+via SPI bus (the same one which describes the topology of Mox modules).
+
+This driver finds out if the SFP cage module index (if it is present)
+and then exports GPIOs which can be used by phylink.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ .../bindings/gpio/gpio-turris-mox-sfp.txt     |  19 ++
+ drivers/gpio/Kconfig                          |   6 +
+ drivers/gpio/Makefile                         |   1 +
+ drivers/gpio/gpio-turris-mox-sfp.c            | 231 ++++++++++++++++++
+ 4 files changed, 257 insertions(+)
+ create mode 100644 Documentation/devicetree/bindings/gpio/gpio-turris-mox-sfp.txt
+ create mode 100644 drivers/gpio/gpio-turris-mox-sfp.c
+
+diff --git a/Documentation/devicetree/bindings/gpio/gpio-turris-mox-sfp.txt b/Documentation/devicetree/bindings/gpio/gpio-turris-mox-sfp.txt
+new file mode 100644
+index 000000000000..19cb4320faf4
+--- /dev/null
++++ b/Documentation/devicetree/bindings/gpio/gpio-turris-mox-sfp.txt
+@@ -0,0 +1,19 @@
++Turris Mox SFP module GPIO Driver (over SPI)
++
++Required properties:
++ - compatible		: Should be "cznic,turris-mox-sfp".
++ - gpio-controller	: Marks the device node as a GPIO controller.
++ - #gpio-cells		: Should be two. For consumer use see gpio.txt.
++
++For other required and optional properties of SPI slave
++nodes please refer to ../spi/spi-bus.txt.
++
++Example:
++
++	gpio_sfp: gpio_sfp@0 {
++		compatible = "cznic,turris-mox-sfp";
++		gpio-controller;
++		#gpio-cells = <2>;
++		reg = <1>;
++		spi-max-frequency = <50000000>;
++	};
+diff --git a/drivers/gpio/Kconfig b/drivers/gpio/Kconfig
+index 3f80f167ed56..697c14d07881 100644
+--- a/drivers/gpio/Kconfig
++++ b/drivers/gpio/Kconfig
+@@ -1268,6 +1268,12 @@ config GPIO_MC33880
+ 	  SPI driver for Freescale MC33880 high-side/low-side switch.
+ 	  This provides GPIO interface supporting inputs and outputs.
+ 
++config GPIO_MOX_SFP
++	tristate "Turris Mox SFP GPIO expander"
++	help
++	  This is the driver needed for configuring the SFP cage found
++	  on Turris Mox SFP module.
++
+ config GPIO_PISOSR
+ 	tristate "Generic parallel-in/serial-out shift register"
+ 	help
+diff --git a/drivers/gpio/Makefile b/drivers/gpio/Makefile
+index 8a2dfba3b231..20b24b8fd8b2 100644
+--- a/drivers/gpio/Makefile
++++ b/drivers/gpio/Makefile
+@@ -95,6 +95,7 @@ obj-$(CONFIG_GPIO_PCA953X)	+= gpio-pca953x.o
+ obj-$(CONFIG_GPIO_PCF857X)	+= gpio-pcf857x.o
+ obj-$(CONFIG_GPIO_PCH)		+= gpio-pch.o
+ obj-$(CONFIG_GPIO_PCI_IDIO_16)	+= gpio-pci-idio-16.o
++obj-$(CONFIG_GPIO_MOX_SFP)	+= gpio-turris-mox-sfp.o
+ obj-$(CONFIG_GPIO_PISOSR)	+= gpio-pisosr.o
+ obj-$(CONFIG_GPIO_PL061)	+= gpio-pl061.o
+ obj-$(CONFIG_GPIO_PXA)		+= gpio-pxa.o
+diff --git a/drivers/gpio/gpio-turris-mox-sfp.c b/drivers/gpio/gpio-turris-mox-sfp.c
+new file mode 100644
+index 000000000000..e09aefcb2cba
+--- /dev/null
++++ b/drivers/gpio/gpio-turris-mox-sfp.c
+@@ -0,0 +1,231 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ *  Turris Mox SFP - GPIOs on the SFP cage found on Turris Mox SFP module
++ *
++ *  Copyright (C) 2018 Marek Behun <marek.behun@nic.cz>
++ */
++
++#include <linux/gpio/consumer.h>
++#include <linux/init.h>
++#include <linux/mutex.h>
++#include <linux/spi/spi.h>
++#include <linux/gpio.h>
++#include <linux/of_gpio.h>
++#include <linux/slab.h>
++#include <linux/module.h>
++
++struct mox_sfp_chip {
++	struct gpio_chip	gpio_chip;
++	struct mutex		lock;
++	u8			state;
++	int			sfp_idx;
++	struct gpio_desc	*gpiod_oe;
++};
++
++static int mox_sfp_find(struct mox_sfp_chip *chip) {
++	struct spi_transfer t;
++	u8 rx[5], tx[5];
++	int i, res, idx = -1;
++
++	memset(rx, 0, 5);
++	memset(tx, 1, 5); /* DET has to be set to 1 for current boards */
++
++	memset(&t, 0, sizeof(t));
++
++	t.rx_buf = rx;
++	t.tx_buf = tx;
++	t.len = 5;
++
++	res = spi_sync_transfer(to_spi_device(chip->gpio_chip.parent), &t, 1);
++	if (res < 0)
++		return res;
++
++	if (rx[0] != 0xff && rx[0] != 0)
++		return -ENODEV;
++
++	for (i = 1; i < 5; ++i) {
++		if ((rx[i] & 0xf) == 0x1) {
++			idx = i;
++			break;
++		}
++	}
++
++	if (idx < 0)
++		return -ENODEV;
++
++	chip->sfp_idx = idx;
++
++	return 0;
++}
++
++static int mox_sfp_do_spi(struct mox_sfp_chip *chip) {
++	struct spi_transfer t;
++	u8 rx[5], tx[5], val;
++	int res;
++
++	val = (chip->state >> 2);
++	val |= 1; /* DET has to be set to 1 for current boards */
++
++	memset(tx, 0, 5);
++	tx[chip->sfp_idx] = val;
++
++	memset(&t, 0, sizeof(t));
++	t.rx_buf = rx;
++	t.tx_buf = tx;
++	t.len = 5;
++
++	res = spi_sync_transfer(to_spi_device(chip->gpio_chip.parent), &t, 1);
++	if (res < 0)
++		return res;
++
++	if ((rx[0] != 0xff && rx[0] != 0) || (rx[chip->sfp_idx] & 0xf) != 0x1)
++		return -ENODEV;
++
++	chip->state = (chip->state & ~3) | ((rx[1] >> 4) & 3);
++
++	return 0;
++}
++
++static int mox_sfp_get_value(struct gpio_chip *gc, unsigned offset)
++{
++	struct mox_sfp_chip *chip = gpiochip_get_data(gc);
++	int ret;
++
++	if (offset < 2) {
++		mutex_lock(&chip->lock);
++		ret = mox_sfp_do_spi(chip);
++		mutex_unlock(&chip->lock);
++
++		if (ret < 0)
++			return ret;
++	}
++
++	return (chip->state >> offset) & 1;
++}
++
++static void mox_sfp_set_value(struct gpio_chip *gc,
++		unsigned offset, int val)
++{
++	struct mox_sfp_chip *chip = gpiochip_get_data(gc);
++
++	mutex_lock(&chip->lock);
++	if (val)
++		chip->state |= (1 << offset);
++	else
++		chip->state &= ~(1 << offset);
++
++	if (offset > 2)
++		(void) mox_sfp_do_spi(chip);
++
++	mutex_unlock(&chip->lock);
++}
++
++static int mox_sfp_get_direction(struct gpio_chip *gc, unsigned offset)
++{
++	return offset < 3;
++}
++
++static int mox_sfp_direction_input(struct gpio_chip *gc, unsigned offset)
++{
++	if (offset > 2)
++		return -EINVAL;
++	return 0;
++}
++
++static int mox_sfp_direction_output(struct gpio_chip *gc,
++		unsigned offset, int val)
++{
++	if (offset < 3 || offset > 4)
++		return -EINVAL;
++	mox_sfp_set_value(gc, offset, val);
++	return 0;
++}
++
++static int mox_sfp_probe(struct spi_device *spi)
++{
++	struct mox_sfp_chip *chip;
++	int ret;
++
++	spi->max_speed_hz = 20000000;
++	spi->bits_per_word = 8;
++	spi->mode = SPI_CPHA;
++
++	ret = spi_setup(spi);
++	if (ret < 0)
++		return ret;
++
++	chip = devm_kzalloc(&spi->dev, sizeof(*chip), GFP_KERNEL);
++	if (!chip)
++		return -ENOMEM;
++
++	chip->gpiod_oe = devm_gpiod_get_optional(&spi->dev, "enable",
++						 GPIOD_OUT_LOW);
++	if (IS_ERR(chip->gpiod_oe))
++		return PTR_ERR(chip->gpiod_oe);
++
++	gpiod_set_value_cansleep(chip->gpiod_oe, 1);
++
++	spi_set_drvdata(spi, chip);
++
++	chip->gpio_chip.label = spi->modalias;
++	chip->gpio_chip.get_direction = mox_sfp_get_direction;
++	chip->gpio_chip.direction_input = mox_sfp_direction_input;
++	chip->gpio_chip.direction_output = mox_sfp_direction_output;
++	chip->gpio_chip.get = mox_sfp_get_value;
++	chip->gpio_chip.set = mox_sfp_set_value;
++	chip->gpio_chip.base = -1;
++
++	chip->gpio_chip.ngpio = 5;
++
++	chip->gpio_chip.can_sleep = true;
++	chip->gpio_chip.parent = &spi->dev;
++	chip->gpio_chip.owner = THIS_MODULE;
++
++	mutex_init(&chip->lock);
++
++	ret = mox_sfp_find(chip);
++	if (ret < 0) {
++		dev_err(&spi->dev, "Failed writing: %d\n", ret);
++		goto exit_destroy;
++	}
++
++	ret = gpiochip_add_data(&chip->gpio_chip, chip);
++	if (!ret)
++		return 0;
++
++exit_destroy:
++	mutex_destroy(&chip->lock);
++
++	return ret;
++}
++
++static int mox_sfp_remove(struct spi_device *spi)
++{
++	struct mox_sfp_chip *chip = spi_get_drvdata(spi);
++
++	gpiod_set_value_cansleep(chip->gpiod_oe, 0);
++	gpiochip_remove(&chip->gpio_chip);
++	mutex_destroy(&chip->lock);
++
++	return 0;
++}
++
++static const struct of_device_id mox_sfp_dt_ids[] = {
++	{ .compatible = "cznic,turris-mox-sfp" },
++	{},
++};
++MODULE_DEVICE_TABLE(of, mox_sfp_dt_ids);
++
++static struct spi_driver mox_sfp_driver = {
++	.driver = {
++		.name		= "mox-sfp",
++		.of_match_table	= mox_sfp_dt_ids,
++	},
++	.probe		= mox_sfp_probe,
++	.remove		= mox_sfp_remove,
++};
++module_spi_driver(mox_sfp_driver);
++
++MODULE_AUTHOR("Marek Behun <marek.behun@nic.cz>");
++MODULE_DESCRIPTION("GPIO configuration of the SFP cage found on Turris Mox");
++MODULE_LICENSE("GPL v2");
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0038-net-mvneta-Use-netif_receive_skb-instead-of-GRO-on-A.patch b/target/linux/mvebu/patches-4.14/0038-net-mvneta-Use-netif_receive_skb-instead-of-GRO-on-A.patch
new file mode 100644
index 0000000..e2b23f0
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0038-net-mvneta-Use-netif_receive_skb-instead-of-GRO-on-A.patch
@@ -0,0 +1,45 @@
+From 357370f5c7cf07a0dd42edbbe204eb6a898355af Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 15:28:00 +0200
+Subject: [PATCH 38/90] net: mvneta: Use netif_receive_skb instead of GRO on
+ Armada 3720
+
+This solves slow receive on Armada 3720 when receiving fragmented
+frames.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ drivers/net/ethernet/marvell/mvneta.c | 10 ++++++++--
+ 1 file changed, 8 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
+index dd5a627739c6..0d1351d57ef5 100644
+--- a/drivers/net/ethernet/marvell/mvneta.c
++++ b/drivers/net/ethernet/marvell/mvneta.c
+@@ -1962,7 +1962,10 @@ static int mvneta_rx_swbm(struct mvneta_port *pp, int rx_todo,
+ 
+ 			skb->protocol = eth_type_trans(skb, dev);
+ 			mvneta_rx_csum(pp, rx_status, skb);
+-			napi_gro_receive(&port->napi, skb);
++			if (pp->neta_armada3700)
++				netif_receive_skb(skb);
++			else
++				napi_gro_receive(&port->napi, skb);
+ 
+ 			rcvd_pkts++;
+ 			rcvd_bytes += rx_bytes;
+@@ -2004,7 +2007,10 @@ static int mvneta_rx_swbm(struct mvneta_port *pp, int rx_todo,
+ 
+ 		mvneta_rx_csum(pp, rx_status, skb);
+ 
+-		napi_gro_receive(&port->napi, skb);
++		if (pp->neta_armada3700)
++			netif_receive_skb(skb);
++		else
++			napi_gro_receive(&port->napi, skb);
+ 	}
+ 
+ 	if (rcvd_pkts) {
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0039-watchdog-Add-support-for-Armada-37xx-CPU-watchdog.patch b/target/linux/mvebu/patches-4.14/0039-watchdog-Add-support-for-Armada-37xx-CPU-watchdog.patch
new file mode 100644
index 0000000..f645155
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0039-watchdog-Add-support-for-Armada-37xx-CPU-watchdog.patch
@@ -0,0 +1,440 @@
+From 2d10a65b50ac73ce37316b9f3f2cb23d02cdbcef Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Fri, 23 Mar 2018 17:37:12 +0100
+Subject: [PATCH 39/90] watchdog: Add support for Armada 37xx CPU watchdog
+
+This adds support for the CPU watchdog found on Marvell Armada 37xx
+SoCs.
+
+There are 4 counters which can be set as CPU watchdog counters.
+This driver uses the second counter (ID 1, counting from 0)
+(Marvell's Linux also uses second counter by default).
+In the future it could be adapted to use other counters, with
+definition in the device tree.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ arch/arm64/boot/dts/marvell/armada-37xx.dtsi |   7 +
+ drivers/watchdog/Kconfig                     |  11 +
+ drivers/watchdog/Makefile                    |   1 +
+ drivers/watchdog/armada_37xx_wdt.c           | 356 +++++++++++++++++++
+ 4 files changed, 375 insertions(+)
+ create mode 100644 drivers/watchdog/armada_37xx_wdt.c
+
+diff --git a/arch/arm64/boot/dts/marvell/armada-37xx.dtsi b/arch/arm64/boot/dts/marvell/armada-37xx.dtsi
+index a5fe6c60cd0a..309da3501171 100644
+--- a/arch/arm64/boot/dts/marvell/armada-37xx.dtsi
++++ b/arch/arm64/boot/dts/marvell/armada-37xx.dtsi
+@@ -142,6 +142,13 @@
+ 				status = "disabled";
+ 			};
+ 
++			wdt: watchdog-timer@8300 {
++				compatible = "marvell,armada-3700-wdt";
++				reg = <0xd064 0x4>,
++				      <0x8300 0x40>;
++				clocks = <&xtalclk>;
++			};
++
+ 			nb_periph_clk: nb-periph-clk@13000 {
+ 				compatible = "marvell,armada-3700-periph-clock-nb";
+ 				reg = <0x13000 0x100>;
+diff --git a/drivers/watchdog/Kconfig b/drivers/watchdog/Kconfig
+index 3ece1335ba84..77c1b4b540e2 100644
+--- a/drivers/watchdog/Kconfig
++++ b/drivers/watchdog/Kconfig
+@@ -255,6 +255,17 @@ config ARM_SBSA_WATCHDOG
+ 	  To compile this driver as module, choose M here: The module
+ 	  will be called sbsa_gwdt.
+ 
++config ARMADA_37XX_WATCHDOG
++	tristate "Armada 37xx watchdog"
++	depends on ARCH_MVEBU || COMPILE_TEST
++	depends on ARM64
++	select WATCHDOG_CORE
++	help
++	  Say Y here to include support for the watchdog timer found on
++	  Marvell Armada 37xx SoCs.
++	  To compile this driver as a module, choose M here: the
++	  module will be called armada_37xx_wdt.
++
+ config ASM9260_WATCHDOG
+ 	tristate "Alphascale ASM9260 watchdog"
+ 	depends on MACH_ASM9260 || COMPILE_TEST
+diff --git a/drivers/watchdog/Makefile b/drivers/watchdog/Makefile
+index 715a21078e0c..eae72c5fa1e0 100644
+--- a/drivers/watchdog/Makefile
++++ b/drivers/watchdog/Makefile
+@@ -38,6 +38,7 @@ obj-$(CONFIG_USBPCWATCHDOG) += pcwd_usb.o
+ # ARM Architecture
+ obj-$(CONFIG_ARM_SP805_WATCHDOG) += sp805_wdt.o
+ obj-$(CONFIG_ARM_SBSA_WATCHDOG) += sbsa_gwdt.o
++obj-$(CONFIG_ARMADA_37XX_WATCHDOG) += armada_37xx_wdt.o
+ obj-$(CONFIG_ASM9260_WATCHDOG) += asm9260_wdt.o
+ obj-$(CONFIG_AT91RM9200_WATCHDOG) += at91rm9200_wdt.o
+ obj-$(CONFIG_AT91SAM9X_WATCHDOG) += at91sam9_wdt.o
+diff --git a/drivers/watchdog/armada_37xx_wdt.c b/drivers/watchdog/armada_37xx_wdt.c
+new file mode 100644
+index 000000000000..99145168e93e
+--- /dev/null
++++ b/drivers/watchdog/armada_37xx_wdt.c
+@@ -0,0 +1,356 @@
++/*
++ * drivers/watchdog/armada_37xx_wdt.c
++ *
++ * Watchdog driver for Marvell Armada 37xx SoCs
++ *
++ * Author: Marek Behun <marek.behun@nic.cz>
++ *
++ * This file is licensed under  the terms of the GNU General Public
++ * License version 2. This program is licensed "as is" without any
++ * warranty of any kind, whether express or implied.
++ */
++
++#include <linux/module.h>
++#include <linux/moduleparam.h>
++#include <linux/types.h>
++#include <linux/kernel.h>
++#include <linux/platform_device.h>
++#include <linux/watchdog.h>
++#include <linux/interrupt.h>
++#include <linux/io.h>
++#include <linux/clk.h>
++#include <linux/err.h>
++#include <linux/of.h>
++#include <linux/of_device.h>
++#include <asm/io.h>
++
++/*
++ * There are four counters that can be used for watchdog on Armada 37xx.
++ * The adresses for counter control registers are register base plus ID*0x10,
++ * where ID is 0, 1, 2 or 3.
++ * In this driver we use ID 1. Marvell's Linux also uses this ID by default,
++ * and the U-Boot driver written simultaneosly by the same author as this
++ * driver also uses ID 1.
++ * Maybe in the future we could change this driver to support other counters,
++ * depending on the device tree, but I don't think this is necessary.
++ *
++ * Note that CNTR_ID cannot be 3, because the third counter is an increment
++ * counter, and this driver is written to support decrementing counters only.
++ */
++
++#define CNTR_ID				1
++
++#define CNTR_CTRL			(CNTR_ID * 0x10)
++#define CNTR_CTRL_ENABLE		0x0001
++#define CNTR_CTRL_ACTIVE		0x0002
++#define CNTR_CTRL_MODE_MASK		0x000c
++#define CNTR_CTRL_MODE_ONESHOT		0x0000
++#define CNTR_CTRL_PRESCALE_MASK		0xff00
++#define CNTR_CTRL_PRESCALE_MIN		2
++#define CNTR_CTRL_PRESCALE_SHIFT	8
++
++#define CNTR_COUNT_LOW			(CNTR_CTRL + 0x4)
++#define CNTR_COUNT_HIGH			(CNTR_CTRL + 0x8)
++
++#define WDT_TIMER_SELECT_MASK		0xf
++#define WDT_TIMER_SELECT		(1 << CNTR_ID)
++
++#define WATCHDOG_TIMEOUT		120
++
++static unsigned int timeout = WATCHDOG_TIMEOUT;
++module_param(timeout, int, 0);
++MODULE_PARM_DESC(timeout, "Watchdog timeout in seconds (default="
++			  __MODULE_STRING(WATCHDOG_TIMEOUT) ")");
++
++static bool nowayout = WATCHDOG_NOWAYOUT;
++module_param(nowayout, bool, 0);
++MODULE_PARM_DESC(nowayout, "Watchdog cannot be stopped once started (default="
++			   __MODULE_STRING(WATCHDOG_NOWAYOUT) ")");
++
++struct armada_37xx_watchdog {
++	struct watchdog_device wdt;
++	void __iomem *sel_reg;
++	void __iomem *reg;
++	u64 timeout; /* in clock ticks */
++	unsigned long clk_rate;
++	struct clk *clk;
++};
++
++static u64 get_counter_value(struct armada_37xx_watchdog *dev)
++{
++	u64 val;
++
++	val = readl(dev->reg + CNTR_COUNT_HIGH);
++	val = (val << 32) | readl(dev->reg + CNTR_COUNT_LOW);
++
++	return val;
++}
++
++static void set_counter_value(struct armada_37xx_watchdog *dev)
++{
++	writel(dev->timeout & 0xffffffff, dev->reg + CNTR_COUNT_LOW);
++	writel(dev->timeout >> 32, dev->reg + CNTR_COUNT_HIGH);
++}
++
++static void armada_37xx_wdt_counter_enable(struct armada_37xx_watchdog *dev)
++{
++	u32 reg;
++
++	reg = readl(dev->reg + CNTR_CTRL);
++	reg |= CNTR_CTRL_ENABLE;
++	writel(reg, dev->reg + CNTR_CTRL);
++}
++
++static void armada_37xx_wdt_counter_disable(struct armada_37xx_watchdog *dev)
++{
++	u32 reg;
++
++	reg = readl(dev->reg + CNTR_CTRL);
++	reg &= ~CNTR_CTRL_ENABLE;
++	writel(reg, dev->reg + CNTR_CTRL);
++}
++
++static int armada_37xx_wdt_ping(struct watchdog_device *wdt)
++{
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++
++	armada_37xx_wdt_counter_disable(dev);
++	set_counter_value(dev);
++	armada_37xx_wdt_counter_enable(dev);
++
++	return 0;
++}
++
++static unsigned int armada_37xx_wdt_get_timeleft(struct watchdog_device *wdt)
++{
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++	unsigned int res;
++
++	res = get_counter_value(dev) * CNTR_CTRL_PRESCALE_MIN / dev->clk_rate;
++
++	return res;
++}
++
++static int armada_37xx_wdt_set_timeout(struct watchdog_device *wdt,
++				       unsigned int timeout)
++{
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++
++	wdt->timeout = timeout;
++
++	/*
++	 * Compute the timeout in clock rate. We use smallest possible prescaler,
++	 * which divides the clock rate by 2 (CNTR_CTRL_PRESCALE_MIN).
++	 */
++	dev->timeout = (u64)dev->clk_rate * timeout / CNTR_CTRL_PRESCALE_MIN;
++
++	return 0;
++}
++
++static bool armada_37xx_wdt_is_running(struct armada_37xx_watchdog *dev)
++{
++	u32 reg;
++
++	reg = readl(dev->sel_reg);
++	if ((reg & WDT_TIMER_SELECT_MASK) != WDT_TIMER_SELECT)
++		return false;
++
++	reg = readl(dev->reg + CNTR_CTRL);
++	return !!(reg & CNTR_CTRL_ACTIVE);
++}
++
++static int armada_37xx_wdt_start(struct watchdog_device *wdt)
++{
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++	u32 reg;
++
++	reg = readl(dev->reg + CNTR_CTRL);
++
++	if (reg & CNTR_CTRL_ACTIVE)
++		return -EBUSY;
++
++	/* set mode */
++	reg = (reg & ~CNTR_CTRL_MODE_MASK) | CNTR_CTRL_MODE_ONESHOT;
++
++	/* set prescaler to the min value of 2 */
++	reg &= ~CNTR_CTRL_PRESCALE_MASK;
++	reg |= CNTR_CTRL_PRESCALE_MIN << CNTR_CTRL_PRESCALE_SHIFT;
++
++	writel(reg, dev->reg + CNTR_CTRL);
++
++	set_counter_value(dev);
++
++	writel(WDT_TIMER_SELECT, dev->sel_reg);
++	armada_37xx_wdt_counter_enable(dev);
++
++	return 0;
++}
++
++static int armada_37xx_wdt_stop(struct watchdog_device *wdt)
++{
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++
++	armada_37xx_wdt_counter_disable(dev);
++	writel(0, dev->sel_reg);
++
++	return 0;
++}
++
++static const struct watchdog_info armada_37xx_wdt_info = {
++	.options = WDIOF_SETTIMEOUT | WDIOF_KEEPALIVEPING | WDIOF_MAGICCLOSE,
++	.identity = "Armada 37xx Watchdog",
++};
++
++static const struct watchdog_ops armada_37xx_wdt_ops = {
++	.owner = THIS_MODULE,
++	.start = armada_37xx_wdt_start,
++	.stop = armada_37xx_wdt_stop,
++	.ping = armada_37xx_wdt_ping,
++	.set_timeout = armada_37xx_wdt_set_timeout,
++	.get_timeleft = armada_37xx_wdt_get_timeleft,
++};
++
++static int armada_37xx_wdt_probe(struct platform_device *pdev)
++{
++	struct armada_37xx_watchdog *dev;
++	struct resource *res;
++	int ret;
++
++	dev = devm_kzalloc(&pdev->dev, sizeof(struct armada_37xx_watchdog),
++			   GFP_KERNEL);
++	if (!dev)
++		return -ENOMEM;
++
++	dev->wdt.info = &armada_37xx_wdt_info;
++	dev->wdt.ops = &armada_37xx_wdt_ops;
++	dev->wdt.min_timeout = 1;
++
++	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
++	if (!res)
++		return -ENODEV;
++	dev->sel_reg = devm_ioremap(&pdev->dev, res->start,
++				    resource_size(res));
++
++	res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
++	if (!res)
++		return -ENODEV;
++	dev->reg = devm_ioremap(&pdev->dev, res->start, resource_size(res));
++
++	/* init clock */
++	dev->clk = clk_get(&pdev->dev, NULL);
++	if (IS_ERR(dev->clk))
++		return PTR_ERR(dev->clk);
++
++	ret = clk_prepare_enable(dev->clk);
++	if (ret) {
++		clk_put(dev->clk);
++		return ret;
++	}
++
++	dev->clk_rate = clk_get_rate(dev->clk);
++
++	/*
++	 * Since the timeout in seconds is given as 32 bit unsigned int, and
++	 * the counters hold 64 bit values, even after multiplication by clock
++	 * rate the counter can hold timeout of UINT_MAX seconds.
++	 */
++	dev->wdt.min_timeout = 0;
++	dev->wdt.max_timeout = UINT_MAX;
++	dev->wdt.parent = &pdev->dev;
++
++	/* default value, possibly override by module parameter or dtb */
++	dev->wdt.timeout = WATCHDOG_TIMEOUT;
++	watchdog_init_timeout(&dev->wdt, timeout, &pdev->dev);
++
++	platform_set_drvdata(pdev, &dev->wdt);
++	watchdog_set_drvdata(&dev->wdt, dev);
++
++	armada_37xx_wdt_set_timeout(&dev->wdt, dev->wdt.timeout);
++
++	if (armada_37xx_wdt_is_running(dev))
++		set_bit(WDOG_HW_RUNNING, &dev->wdt.status);
++	else
++		armada_37xx_wdt_stop(&dev->wdt);
++
++	watchdog_set_nowayout(&dev->wdt, nowayout);
++	ret = watchdog_register_device(&dev->wdt);
++	if (ret)
++		goto disable_clk;
++
++	dev_info(&pdev->dev, "Initial timeout %d sec%s\n",
++		 dev->wdt.timeout, nowayout ? ", nowayout" : "");
++
++	return 0;
++
++disable_clk:
++	clk_disable_unprepare(dev->clk);
++	clk_put(dev->clk);
++	return ret;
++}
++
++static int armada_37xx_wdt_remove(struct platform_device *pdev)
++{
++	struct watchdog_device *wdt = platform_get_drvdata(pdev);
++	struct armada_37xx_watchdog *dev = watchdog_get_drvdata(wdt);
++
++	watchdog_unregister_device(wdt);
++	clk_disable_unprepare(dev->clk);
++	clk_put(dev->clk);
++	return 0;
++}
++
++static void armada_37xx_wdt_shutdown(struct platform_device *pdev)
++{
++	struct watchdog_device *wdt = platform_get_drvdata(pdev);
++
++	armada_37xx_wdt_stop(wdt);
++}
++
++static int __maybe_unused armada_37xx_wdt_suspend(struct device *dev)
++{
++	struct watchdog_device *wdt = dev_get_drvdata(dev);
++
++	return armada_37xx_wdt_stop(wdt);
++}
++
++static int __maybe_unused armada_37xx_wdt_resume(struct device *dev)
++{
++	struct watchdog_device *wdt = dev_get_drvdata(dev);
++
++	if (watchdog_active(wdt))
++		return armada_37xx_wdt_start(wdt);
++
++	return 0;
++}
++
++static const struct dev_pm_ops armada_37xx_wdt_dev_pm_ops = {
++	SET_SYSTEM_SLEEP_PM_OPS(armada_37xx_wdt_suspend,
++				armada_37xx_wdt_resume)
++};
++
++#ifdef CONFIG_OF
++static const struct of_device_id armada_37xx_wdt_match[] = {
++	{ .compatible = "marvell,armada-3700-wdt", },
++	{},
++};
++MODULE_DEVICE_TABLE(of, armada_37xx_wdt_match);
++#endif
++
++static struct platform_driver armada_37xx_wdt_driver = {
++	.probe		= armada_37xx_wdt_probe,
++	.remove		= armada_37xx_wdt_remove,
++	.shutdown	= armada_37xx_wdt_shutdown,
++	.driver		= {
++		.name	= "armada_37xx_wdt",
++		.of_match_table = of_match_ptr(armada_37xx_wdt_match),
++		.pm = &armada_37xx_wdt_dev_pm_ops,
++	},
++};
++
++module_platform_driver(armada_37xx_wdt_driver);
++
++MODULE_AUTHOR("Marek Behun <marek.behun@nic.cz>");
++MODULE_DESCRIPTION("Armada 37xx CPU Watchdog");
++
++MODULE_LICENSE("GPL v2");
++MODULE_ALIAS("platform:armada_37xx_wdt");
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0040-drivers-mmc-Add-no-uhs-voltage-cap-to-MMC-host-struc.patch b/target/linux/mvebu/patches-4.14/0040-drivers-mmc-Add-no-uhs-voltage-cap-to-MMC-host-struc.patch
new file mode 100644
index 0000000..64b572e
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0040-drivers-mmc-Add-no-uhs-voltage-cap-to-MMC-host-struc.patch
@@ -0,0 +1,57 @@
+From 004ccecb55538e776bc9364492718bc5cf667b1e Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 15:55:35 +0200
+Subject: [PATCH 40/90] drivers: mmc: Add no-uhs-voltage cap to MMC host struct
+
+This is needed for Turris Mox, where calling mmc_set_uhs_voltage
+causes failure on the SD card.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ drivers/mmc/core/core.c  | 3 ++-
+ drivers/mmc/core/host.c  | 2 ++
+ include/linux/mmc/host.h | 1 +
+ 3 files changed, 5 insertions(+), 1 deletion(-)
+
+diff --git a/drivers/mmc/core/core.c b/drivers/mmc/core/core.c
+index 29bfff2ed4d3..bc3e038bfeae 100644
+--- a/drivers/mmc/core/core.c
++++ b/drivers/mmc/core/core.c
+@@ -1494,7 +1494,8 @@ int mmc_set_uhs_voltage(struct mmc_host *host, u32 ocr)
+ 	 * If we cannot switch voltages, return failure so the caller
+ 	 * can continue without UHS mode
+ 	 */
+-	if (!host->ops->start_signal_voltage_switch)
++	if (!host->ops->start_signal_voltage_switch ||
++	    host->caps2 & MMC_CAP2_NO_UHS_VOLTAGE)
+ 		return -EPERM;
+ 	if (!host->ops->card_busy)
+ 		pr_warn("%s: cannot verify signal voltage switch\n",
+diff --git a/drivers/mmc/core/host.c b/drivers/mmc/core/host.c
+index ad88deb2e8f3..b51db1d968c3 100644
+--- a/drivers/mmc/core/host.c
++++ b/drivers/mmc/core/host.c
+@@ -325,6 +325,8 @@ int mmc_of_parse(struct mmc_host *host)
+ 		host->caps2 |= MMC_CAP2_NO_SD;
+ 	if (device_property_read_bool(dev, "no-mmc"))
+ 		host->caps2 |= MMC_CAP2_NO_MMC;
++	if (device_property_read_bool(dev, "no-uhs-voltage"))
++		host->caps2 |= MMC_CAP2_NO_UHS_VOLTAGE;
+ 
+ 	host->dsr_req = !device_property_read_u32(dev, "dsr", &host->dsr);
+ 	if (host->dsr_req && (host->dsr & ~0xffff)) {
+diff --git a/include/linux/mmc/host.h b/include/linux/mmc/host.h
+index 9a43763a68ad..336893a60e04 100644
+--- a/include/linux/mmc/host.h
++++ b/include/linux/mmc/host.h
+@@ -349,6 +349,7 @@ struct mmc_host {
+ #define MMC_CAP2_NO_MMC		(1 << 22)	/* Do not send (e)MMC commands during initialization */
+ #define MMC_CAP2_CQE		(1 << 23)	/* Has eMMC command queue engine */
+ #define MMC_CAP2_CQE_DCMD	(1 << 24)	/* CQE can issue a direct command */
++#define MMC_CAP2_NO_UHS_VOLTAGE	(1 << 25)	/* Do not switch to UHS voltage during initialization */
+ 
+ 	mmc_pm_flag_t		pm_caps;	/* supported pm features */
+ 
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0041-ARM64-dts-marvell-Add-DTS-files-for-Turris-Mox-and-i.patch b/target/linux/mvebu/patches-4.14/0041-ARM64-dts-marvell-Add-DTS-files-for-Turris-Mox-and-i.patch
new file mode 100644
index 0000000..131d732
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0041-ARM64-dts-marvell-Add-DTS-files-for-Turris-Mox-and-i.patch
@@ -0,0 +1,463 @@
+From 7a94abe0d3460ce95928596ec9f2782abb4b6350 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Thu, 3 May 2018 15:59:46 +0200
+Subject: [PATCH 41/90] ARM64: dts: marvell: Add DTS files for Turris Mox and
+ its modules
+
+This adds basic support for the Turris Mox board from CZ.NIC, which is
+currently being crowdfunded on Indiegogo.
+
+Turris Mox is as modular router based on the Armada 3720 SOC (same as
+EspressoBin).
+
+The basic module can be extended by different modules. The device tree
+binary for the kernel can be dependent on which modules are connected,
+and in what order. Because of this, the board specific code creates
+in U-Boot a variable called module_topology, which carries this
+information.
+
+Signed-off-by: Marek Behun <marek.behun@nic.cz>
+---
+ arch/arm64/boot/dts/marvell/Makefile          |   6 +
+ .../armada-3720-turris-mox-emmc-sfp.dts       |   8 +
+ .../armada-3720-turris-mox-emmc-topaz.dts     |   8 +
+ .../marvell/armada-3720-turris-mox-emmc.dts   |  14 ++
+ .../marvell/armada-3720-turris-mox-sd-sfp.dts |   8 +
+ .../armada-3720-turris-mox-sd-topaz.dts       |   8 +
+ .../dts/marvell/armada-3720-turris-mox-sd.dts |  16 ++
+ .../marvell/armada-3720-turris-mox-sfp.dtsi   |  34 ++++
+ .../marvell/armada-3720-turris-mox-topaz.dtsi |  75 ++++++++
+ .../dts/marvell/armada-3720-turris-mox.dtsi   | 177 ++++++++++++++++++
+ 10 files changed, 354 insertions(+)
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-sfp.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-topaz.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-sfp.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-topaz.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox-topaz.dtsi
+ create mode 100644 arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
+
+diff --git a/arch/arm64/boot/dts/marvell/Makefile b/arch/arm64/boot/dts/marvell/Makefile
+index 5633676fa9d0..b1e6a803b2a6 100644
+--- a/arch/arm64/boot/dts/marvell/Makefile
++++ b/arch/arm64/boot/dts/marvell/Makefile
+@@ -6,6 +6,12 @@ dtb-$(CONFIG_ARCH_BERLIN) += berlin4ct-stb.dtb
+ # Mvebu SoC Family
+ dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-db.dtb
+ dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-espressobin.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-emmc-sfp.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-emmc-topaz.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-emmc.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-sd-sfp.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-sd-topaz.dtb
++dtb-$(CONFIG_ARCH_MVEBU) += armada-3720-turris-mox-sd.dtb
+ dtb-$(CONFIG_ARCH_MVEBU) += armada-7040-db.dtb
+ dtb-$(CONFIG_ARCH_MVEBU) += armada-8040-db.dtb
+ dtb-$(CONFIG_ARCH_MVEBU) += armada-8040-mcbin.dtb
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-sfp.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-sfp.dts
+new file mode 100644
+index 000000000000..71de376e1394
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-sfp.dts
+@@ -0,0 +1,8 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board 
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox-emmc.dts"
++#include "armada-3720-turris-mox-sfp.dtsi"
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-topaz.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-topaz.dts
+new file mode 100644
+index 000000000000..53e73552c081
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc-topaz.dts
+@@ -0,0 +1,8 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board 
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox-emmc.dts"
++#include "armada-3720-turris-mox-topaz.dtsi"
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc.dts
+new file mode 100644
+index 000000000000..b995f25f5d58
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-emmc.dts
+@@ -0,0 +1,14 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board with eMMC card
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox.dtsi"
++
++&sdhci0 {
++	non-removable;
++	bus-width = <4>;
++	marvell,pad-type = "fixed-1-8v";
++	status = "okay";
++};
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-sfp.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-sfp.dts
+new file mode 100644
+index 000000000000..ea7069571498
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-sfp.dts
+@@ -0,0 +1,8 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board 
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox-sd.dts"
++#include "armada-3720-turris-mox-sfp.dtsi"
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-topaz.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-topaz.dts
+new file mode 100644
+index 000000000000..ecfdfe51fd12
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd-topaz.dts
+@@ -0,0 +1,8 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox-sd.dts"
++#include "armada-3720-turris-mox-topaz.dtsi"
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
+new file mode 100644
+index 000000000000..6e98d82ad654
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
+@@ -0,0 +1,16 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board with SD card slot
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++#include "armada-3720-turris-mox.dtsi"
++
++&sdhci0 {
++	wp-inverted;
++	bus-width = <4>;
++	cd-gpios = <&gpionb 10 GPIO_ACTIVE_HIGH>;
++	no-uhs-voltage;
++	marvell,pad-type = "fixed-1-8v";
++	status = "okay";
++};
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
+new file mode 100644
+index 000000000000..1769750a6e57
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
+@@ -0,0 +1,34 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board SFP cage module
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++/ {
++	sfp: sfp {
++		compatible = "sff,sfp+";
++		i2c-bus = <&i2c0>;
++		mod-def0-gpio = <&gpio_spi 2 GPIO_ACTIVE_LOW>;
++		los-gpio = <&gpio_spi 0 GPIO_ACTIVE_HIGH>;
++		tx-fault-gpio = <&gpio_spi 1 GPIO_ACTIVE_HIGH>;
++		tx-disable-gpio = <&gpio_spi 3 GPIO_ACTIVE_HIGH>;
++		rate-select0-gpio = <&gpio_spi 4 GPIO_ACTIVE_HIGH>;
++	};
++};
++
++&eth1 {
++	status = "okay";
++	phy-mode = "sgmii";
++	sfp = <&sfp>;
++	managed = "in-band-status";
++};
++
++&spi0 {
++	gpio_spi: gpio_spi@0 {
++		compatible = "cznic,turris-mox-sfp";
++		gpio-controller;
++		#gpio-cells = <2>;
++		reg = <1>;
++		spi-max-frequency = <50000000>;
++	};
++};
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-topaz.dtsi b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-topaz.dtsi
+new file mode 100644
+index 000000000000..2dcae0deef7f
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-topaz.dtsi
+@@ -0,0 +1,75 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board Topaz switch module
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++&eth1 {
++	status = "okay";
++	phy-mode = "1000base-x";
++	managed = "in-band-status";
++};
++
++&mdio {
++	switch0: switch0@0 {
++		compatible = "marvell,mv88e6085";
++		#address-cells = <1>;
++		#size-cells = <0>;
++		reg = <2 0>;
++
++		dsa,member = <0 0>;
++
++		ports {
++			#address-cells = <1>;
++			#size-cells = <0>;
++
++			port@1 {
++				reg = <1>;
++				label = "lan1";
++				phy-handle = <&switch0phy1>;
++			};
++
++			port@2 {
++				reg = <2>;
++				label = "lan2";
++				phy-handle = <&switch0phy2>;
++			};
++
++			port@3 {
++				reg = <3>;
++				label = "lan3";
++				phy-handle = <&switch0phy3>;
++			};
++
++			port@4 {
++				reg = <4>;
++				label = "lan4";
++				phy-handle = <&switch0phy4>;
++			};
++
++			port@5 {
++				reg = <5>;
++				label = "cpu";
++				ethernet = <&eth1>;
++			};
++		};
++
++		mdio {
++			#address-cells = <1>;
++			#size-cells = <0>;
++
++			switch0phy1: switch0phy1@11 {
++				reg = <0x11>;
++			};
++			switch0phy2: switch0phy2@12 {
++				reg = <0x12>;
++			};
++			switch0phy3: switch0phy3@13 {
++				reg = <0x13>;
++			};
++			switch0phy4: switch0phy4@14 {
++				reg = <0x14>;
++			};
++		};
++	};
++};
+diff --git a/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
+new file mode 100644
+index 000000000000..f17d9737e1ba
+--- /dev/null
++++ b/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
+@@ -0,0 +1,177 @@
++// SPDX-License-Identifier: GPL-2.0+ or X11
++/*
++ * Device Tree file for CZ.NIC Turris Mox Board
++ * 2018 by Marek Behun <marek.behun@nic.cz>
++ */
++
++/dts-v1/;
++
++#include <dt-bindings/gpio/gpio.h>
++#include <dt-bindings/input/input.h>
++#include "armada-372x.dtsi"
++
++/ {
++	model = "CZ.NIC Turris Mox Board";
++	compatible = "cznic,turris-mox", "marvell,armada3720", "marvell,armada3710";
++
++	aliases {
++		spi0 = &spi0;
++	};
++
++	chosen {
++		stdout-path = "serial0:115200n8";
++	};
++
++	memory@0 {
++		device_type = "memory";
++		reg = <0x00000000 0x00000000 0x00000000 0x20000000>;
++	};
++
++	leds {
++		compatible = "gpio-leds";
++		red {
++			gpios = <&gpionb 11 GPIO_ACTIVE_LOW>;
++			linux,default-trigger = "heartbeat";
++		};
++	};
++
++	gpio_keys {
++		compatible = "gpio-keys";
++		#address-cells = <1>;
++		#size-cells = <0>;
++
++		button@0 {
++			label = "reset";
++			linux,code = <BTN_MISC>;
++			gpios = <&gpiosb 20 GPIO_ACTIVE_LOW>;
++			debounce-interval = <60>;
++		};
++	};
++
++	exp_usb3_vbus: usb3-vbus {
++		compatible = "regulator-fixed";
++		regulator-name = "usb3-vbus";
++		regulator-min-microvolt = <5000000>;
++		regulator-max-microvolt = <5000000>;
++		enable-active-high;
++		regulator-always-on;
++		gpio = <&gpiosb 0 GPIO_ACTIVE_HIGH>;
++	};
++
++	usb3_phy: usb3-phy {
++		compatible = "usb-nop-xceiv";
++		vcc-supply = <&exp_usb3_vbus>;
++	};
++};
++
++&pinctrl_nb {
++	spi_cs1_pins: spi-cs1-pins {
++		groups = "spi_cs1";
++		function = "spi";
++	};
++
++	sdio0_pins: sdio0-pins {
++		groups = "sdio0";
++		function = "sdio";
++	};
++};
++
++&pinctrl_sb {
++	sdio_sb_pins: sdio-sb-pins {
++		groups = "sdio_sb";
++		function = "sdio";
++	};
++
++	smi_pins: smi-pins {
++		groups = "smi";
++		function = "smi";
++	};
++
++	pcie1_pins: pcie1-pins {
++		groups = "pcie1";
++		function = "pcie";
++	};
++};
++
++&i2c0 {
++	pinctrl-names = "default";
++	pinctrl-0 = <&i2c1_pins>;
++	status = "okay";
++
++	rtc@51 {
++		compatible = "nxp,pcf85063";
++		reg = <0x51>;
++	};
++};
++
++&pcie0 {
++	pinctrl-names = "default";
++	pinctrl-0 = <&pcie1_pins>;
++	status = "okay";
++};
++
++&uart0 {
++	status = "okay";
++};
++
++&eth0 {
++	pinctrl-names = "default";
++	pinctrl-0 = <&rgmii_pins>;
++	phy-mode = "rgmii-id";
++	phy = <&phy1>;
++
++	status = "okay";
++};
++
++&mdio {
++	pinctrl-names = "default";
++	pinctrl-0 = <&smi_pins>;
++	status = "okay";
++
++	phy1: ethernet-phy@1 {
++		reg = <1>;
++	};
++};
++
++&sdhci1 {
++	pinctrl-names = "default";
++	pinctrl-0 = <&sdio0_pins &sdio_sb_pins>;
++	non-removable;
++	bus-width = <1>;
++	marvell,pad-type = "fixed-1-8v";
++	status = "okay";
++};
++
++&spi0 {
++	status = "okay";
++	pinctrl-names = "default";
++	pinctrl-0 = <&spi_quad_pins &spi_cs1_pins>;
++
++	spi-flash@0 {
++		#address-cells = <1>;
++		#size-cells = <1>;
++		compatible = "jedec,spi-nor";
++		reg = <0>;
++		spi-max-frequency = <50000000>;
++
++		partition@0 {
++			label = "u-boot";
++			reg = <0x0 0x3f0000>;
++		};
++
++		partition@3f0000 {
++			label = "u-boot-env";
++			reg = <0x180000 0x10000>;
++		};
++
++		partition@400000 {
++			label = "Rescue system";
++			reg = <0x190000 0x670000>;
++		};
++	};
++};
++
++&usb3 {
++	status = "okay";
++	usb-phy = <&usb3_phy>;
++};
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/0043-crypto-inside-secure-remove-null-check-before-kfree.patch b/target/linux/mvebu/patches-4.14/0043-crypto-inside-secure-remove-null-check-before-kfree.patch
new file mode 100644
index 0000000..1dcb91a
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0043-crypto-inside-secure-remove-null-check-before-kfree.patch
@@ -0,0 +1,33 @@
+From ca191187d2de37857276f7a03baa9ffa1069872e Mon Sep 17 00:00:00 2001
+From: Himanshu Jha <himanshujha199640@gmail.com>
+Date: Sun, 27 Aug 2017 02:45:30 +0530
+Subject: [PATCH 43/88] crypto: inside-secure - remove null check before kfree
+
+Kfree on NULL pointer is a no-op and therefore checking is redundant.
+
+Signed-off-by: Himanshu Jha <himanshujha199640@gmail.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 6 ++----
+ 1 file changed, 2 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index d626aa485a76..f2bca572b1db 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -326,10 +326,8 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 		ctx->base.cache_sz = 0;
+ 	}
+ free_cache:
+-	if (ctx->base.cache) {
+-		kfree(ctx->base.cache);
+-		ctx->base.cache = NULL;
+-	}
++	kfree(ctx->base.cache);
++	ctx->base.cache = NULL;
+ 
+ unlock:
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0044-crypto-inside-secure-do-not-use-areq-result-for-part.patch b/target/linux/mvebu/patches-4.14/0044-crypto-inside-secure-do-not-use-areq-result-for-part.patch
new file mode 100644
index 0000000..e4ca471
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0044-crypto-inside-secure-do-not-use-areq-result-for-part.patch
@@ -0,0 +1,63 @@
+From 8af70f1e3625f4d728e8883057abfb5064c0efdc Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Mon, 11 Dec 2017 12:10:58 +0100
+Subject: [PATCH 44/88] crypto: inside-secure - do not use areq->result for
+ partial results
+
+This patches update the SafeXcel driver to stop using the crypto
+ahash_request result field for partial results (i.e. on updates).
+Instead the driver local safexcel_ahash_req state field is used, and
+only on final operations the ahash_request result buffer is updated.
+
+Fixes: 1b44c5a60c13 ("crypto: inside-secure - add SafeXcel EIP197 crypto engine driver")
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 10 +++++-----
+ 1 file changed, 5 insertions(+), 5 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index f2bca572b1db..da9d040bccc2 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -37,7 +37,7 @@ struct safexcel_ahash_req {
+ 	int nents;
+ 
+ 	u8 state_sz;    /* expected sate size, only set once */
+-	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)];
++	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)] __aligned(sizeof(u32));
+ 
+ 	u64 len;
+ 	u64 processed;
+@@ -130,7 +130,7 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 	struct ahash_request *areq = ahash_request_cast(async);
+ 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_req *sreq = ahash_request_ctx(areq);
+-	int cache_len, result_sz = sreq->state_sz;
++	int cache_len;
+ 
+ 	*ret = 0;
+ 
+@@ -151,8 +151,8 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 
+ 	if (sreq->finish)
+-		result_sz = crypto_ahash_digestsize(ahash);
+-	memcpy(sreq->state, areq->result, result_sz);
++		memcpy(areq->result, sreq->state,
++		       crypto_ahash_digestsize(ahash));
+ 
+ 	if (sreq->nents) {
+ 		dma_unmap_sg(priv->dev, areq->src, sreq->nents, DMA_TO_DEVICE);
+@@ -292,7 +292,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	/* Add the token */
+ 	safexcel_hash_token(first_cdesc, len, req->state_sz);
+ 
+-	ctx->base.result_dma = dma_map_single(priv->dev, areq->result,
++	ctx->base.result_dma = dma_map_single(priv->dev, req->state,
+ 					      req->state_sz, DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(priv->dev, ctx->base.result_dma)) {
+ 		ret = -EINVAL;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0045-crypto-inside-secure-remove-extra-empty-line.patch b/target/linux/mvebu/patches-4.14/0045-crypto-inside-secure-remove-extra-empty-line.patch
new file mode 100644
index 0000000..a8f5453
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0045-crypto-inside-secure-remove-extra-empty-line.patch
@@ -0,0 +1,28 @@
+From 62db066bcd803d30c362f785a1457bfc8a238350 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:43 +0100
+Subject: [PATCH 45/88] crypto: inside-secure - remove extra empty line
+
+Cosmetic patch removing an extra empty line between header inclusions.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 1 -
+ 1 file changed, 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index da9d040bccc2..0c7a7d63c666 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -14,7 +14,6 @@
+ #include <linux/dma-mapping.h>
+ #include <linux/dmapool.h>
+ 
+-
+ #include "safexcel.h"
+ 
+ struct safexcel_ahash_ctx {
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0046-crypto-inside-secure-fix-typo-in-a-comment.patch b/target/linux/mvebu/patches-4.14/0046-crypto-inside-secure-fix-typo-in-a-comment.patch
new file mode 100644
index 0000000..5a8a10a
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0046-crypto-inside-secure-fix-typo-in-a-comment.patch
@@ -0,0 +1,29 @@
+From 9a6fff45b2fb341893013a54177fa5dfb8b3b037 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:44 +0100
+Subject: [PATCH 46/88] crypto: inside-secure - fix typo in a comment
+
+Cosmetic patch fixing one typo in one of the driver's comments.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 0c7a7d63c666..57e7bb27ca6a 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -522,7 +522,7 @@ static int safexcel_ahash_cache(struct ahash_request *areq)
+ 		return areq->nbytes;
+ 	}
+ 
+-	/* We could'nt cache all the data */
++	/* We couldn't cache all the data */
+ 	return -E2BIG;
+ }
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0047-crypto-inside-secure-remove-useless-memset.patch b/target/linux/mvebu/patches-4.14/0047-crypto-inside-secure-remove-useless-memset.patch
new file mode 100644
index 0000000..9525e3b
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0047-crypto-inside-secure-remove-useless-memset.patch
@@ -0,0 +1,30 @@
+From e9cb1fc250888813abc5aa9af104354fbfdf8520 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:45 +0100
+Subject: [PATCH 47/88] crypto: inside-secure - remove useless memset
+
+This patch removes an useless memset in the ahash_export function, as
+the zeroed buffer will be entirely overridden the next line.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 1 -
+ 1 file changed, 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 57e7bb27ca6a..3b72cb95ed4f 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -642,7 +642,6 @@ static int safexcel_ahash_export(struct ahash_request *areq, void *out)
+ 	export->processed = req->processed;
+ 
+ 	memcpy(export->state, req->state, req->state_sz);
+-	memset(export->cache, 0, crypto_ahash_blocksize(ahash));
+ 	memcpy(export->cache, req->cache, crypto_ahash_blocksize(ahash));
+ 
+ 	return 0;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0048-crypto-inside-secure-refrain-from-unneeded-invalidat.patch b/target/linux/mvebu/patches-4.14/0048-crypto-inside-secure-refrain-from-unneeded-invalidat.patch
new file mode 100644
index 0000000..a15fdcf
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0048-crypto-inside-secure-refrain-from-unneeded-invalidat.patch
@@ -0,0 +1,91 @@
+From 244916e6e5eb0df2bfb3e0a8e81f38f811a1039b Mon Sep 17 00:00:00 2001
+From: Ofer Heifetz <oferh@marvell.com>
+Date: Thu, 14 Dec 2017 15:26:47 +0100
+Subject: [PATCH 48/88] crypto: inside-secure - refrain from unneeded
+ invalidations
+
+The check to know if an invalidation is needed (i.e. when the context
+changes) is done even if the context does not exist yet. This happens
+when first setting a key for ciphers and/or hmac operations.
+
+This commits adds a check in the _setkey functions to only check if an
+invalidation is needed when a context exists, as there is no need to
+perform this check otherwise.
+
+Signed-off-by: Ofer Heifetz <oferh@marvell.com>
+[Antoine: commit message and added a comment and reworked one of the
+checks]
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ .../crypto/inside-secure/safexcel_cipher.c    | 10 ++++----
+ drivers/crypto/inside-secure/safexcel_hash.c  | 24 ++++++++++++-------
+ 2 files changed, 22 insertions(+), 12 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index fcc0a606d748..794849b859a7 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -78,10 +78,12 @@ static int safexcel_aes_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ 		return ret;
+ 	}
+ 
+-	for (i = 0; i < len / sizeof(u32); i++) {
+-		if (ctx->key[i] != cpu_to_le32(aes.key_enc[i])) {
+-			ctx->base.needs_inv = true;
+-			break;
++	if (ctx->base.ctxr_dma) {
++		for (i = 0; i < len / sizeof(u32); i++) {
++			if (ctx->key[i] != cpu_to_le32(aes.key_enc[i])) {
++				ctx->base.needs_inv = true;
++				break;
++			}
+ 		}
+ 	}
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 3b72cb95ed4f..8ecc386e78f2 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -535,10 +535,16 @@ static int safexcel_ahash_enqueue(struct ahash_request *areq)
+ 
+ 	req->needs_inv = false;
+ 
+-	if (req->processed && ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
+-		ctx->base.needs_inv = safexcel_ahash_needs_inv_get(areq);
+-
+ 	if (ctx->base.ctxr) {
++		if (!ctx->base.needs_inv && req->processed &&
++		    ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
++			/* We're still setting needs_inv here, even though it is
++			 * cleared right away, because the needs_inv flag can be
++			 * set in other functions and we want to keep the same
++			 * logic.
++			 */
++			ctx->base.needs_inv = safexcel_ahash_needs_inv_get(areq);
++
+ 		if (ctx->base.needs_inv) {
+ 			ctx->base.needs_inv = false;
+ 			req->needs_inv = true;
+@@ -936,11 +942,13 @@ static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 	if (ret)
+ 		return ret;
+ 
+-	for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {
+-		if (ctx->ipad[i] != le32_to_cpu(istate.state[i]) ||
+-		    ctx->opad[i] != le32_to_cpu(ostate.state[i])) {
+-			ctx->base.needs_inv = true;
+-			break;
++	if (ctx->base.ctxr) {
++		for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {
++			if (ctx->ipad[i] != le32_to_cpu(istate.state[i]) ||
++			    ctx->opad[i] != le32_to_cpu(ostate.state[i])) {
++				ctx->base.needs_inv = true;
++				break;
++			}
+ 		}
+ 	}
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0049-crypto-inside-secure-EBUSY-is-not-an-error-on-async-.patch b/target/linux/mvebu/patches-4.14/0049-crypto-inside-secure-EBUSY-is-not-an-error-on-async-.patch
new file mode 100644
index 0000000..bc885cb
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0049-crypto-inside-secure-EBUSY-is-not-an-error-on-async-.patch
@@ -0,0 +1,35 @@
+From 3da198498b275cf54041839e9a01292b4f4a8f40 Mon Sep 17 00:00:00 2001
+From: Ofer Heifetz <oferh@marvell.com>
+Date: Thu, 14 Dec 2017 15:26:48 +0100
+Subject: [PATCH 49/88] crypto: inside-secure - EBUSY is not an error on async
+ request
+
+When initializing the IVs crypto_ahash_update() is called, which at some
+point will call crypto_enqueue_request(). This function can return
+-EBUSY when no resource is available and the request is queued. Since
+this is a valid case, -EBUSY shouldn't be treated as an error.
+
+Signed-off-by: Ofer Heifetz <oferh@marvell.com>
+[Antoine: commit message]
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 8ecc386e78f2..89ab80a20a2f 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -870,7 +870,7 @@ static int safexcel_hmac_init_iv(struct ahash_request *areq,
+ 	req->last_req = true;
+ 
+ 	ret = crypto_ahash_update(areq);
+-	if (ret && ret != -EINPROGRESS)
++	if (ret && ret != -EINPROGRESS && ret != -EBUSY)
+ 		return ret;
+ 
+ 	wait_for_completion_interruptible(&result.completion);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0050-crypto-inside-secure-move-cipher-crypto-mode-to-requ.patch b/target/linux/mvebu/patches-4.14/0050-crypto-inside-secure-move-cipher-crypto-mode-to-requ.patch
new file mode 100644
index 0000000..a3aeb71
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0050-crypto-inside-secure-move-cipher-crypto-mode-to-requ.patch
@@ -0,0 +1,76 @@
+From cb627d61729d22832f1875180081b4d9c05f5057 Mon Sep 17 00:00:00 2001
+From: Ofer Heifetz <oferh@marvell.com>
+Date: Thu, 14 Dec 2017 15:26:49 +0100
+Subject: [PATCH 50/88] crypto: inside-secure - move cipher crypto mode to
+ request context
+
+The cipher direction can be different for requests within the same
+transformation context. This patch moves the direction flag from the
+context to the request scope.
+
+Signed-off-by: Ofer Heifetz <oferh@marvell.com>
+[Antoine: commit message]
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_cipher.c | 11 +++++++----
+ 1 file changed, 7 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index 794849b859a7..62382a7c4cbe 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -27,7 +27,6 @@ struct safexcel_cipher_ctx {
+ 	struct safexcel_context base;
+ 	struct safexcel_crypto_priv *priv;
+ 
+-	enum safexcel_cipher_direction direction;
+ 	u32 mode;
+ 
+ 	__le32 key[8];
+@@ -35,6 +34,7 @@ struct safexcel_cipher_ctx {
+ };
+ 
+ struct safexcel_cipher_req {
++	enum safexcel_cipher_direction direction;
+ 	bool needs_inv;
+ };
+ 
+@@ -97,12 +97,15 @@ static int safexcel_aes_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ }
+ 
+ static int safexcel_context_control(struct safexcel_cipher_ctx *ctx,
++				    struct crypto_async_request *async,
+ 				    struct safexcel_command_desc *cdesc)
+ {
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
++	struct skcipher_request *req = skcipher_request_cast(async);
++	struct safexcel_cipher_req *sreq = skcipher_request_ctx(req);
+ 	int ctrl_size;
+ 
+-	if (ctx->direction == SAFEXCEL_ENCRYPT)
++	if (sreq->direction == SAFEXCEL_ENCRYPT)
+ 		cdesc->control_data.control0 |= CONTEXT_CONTROL_TYPE_CRYPTO_OUT;
+ 	else
+ 		cdesc->control_data.control0 |= CONTEXT_CONTROL_TYPE_CRYPTO_IN;
+@@ -245,7 +248,7 @@ static int safexcel_aes_send(struct crypto_async_request *async,
+ 		n_cdesc++;
+ 
+ 		if (n_cdesc == 1) {
+-			safexcel_context_control(ctx, cdesc);
++			safexcel_context_control(ctx, async, cdesc);
+ 			safexcel_cipher_token(ctx, async, cdesc, req->cryptlen);
+ 		}
+ 
+@@ -469,7 +472,7 @@ static int safexcel_aes(struct skcipher_request *req,
+ 	int ret, ring;
+ 
+ 	sreq->needs_inv = false;
+-	ctx->direction = dir;
++	sreq->direction = dir;
+ 	ctx->mode = mode;
+ 
+ 	if (ctx->base.ctxr) {
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0051-crypto-inside-secure-remove-unused-parameter-in-inva.patch b/target/linux/mvebu/patches-4.14/0051-crypto-inside-secure-remove-unused-parameter-in-inva.patch
new file mode 100644
index 0000000..4d308a7
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0051-crypto-inside-secure-remove-unused-parameter-in-inva.patch
@@ -0,0 +1,74 @@
+From f5a7bdfa43b5fa2b36b54aab91ad4ece3d4cfde5 Mon Sep 17 00:00:00 2001
+From: Ofer Heifetz <oferh@marvell.com>
+Date: Thu, 14 Dec 2017 15:26:50 +0100
+Subject: [PATCH 51/88] crypto: inside-secure - remove unused parameter in
+ invalidate_cache
+
+The SafeXcel context isn't used in the cache invalidation function. This
+cosmetic patch removes it (as well as from the function prototype in the
+header file and when the function is called).
+
+Signed-off-by: Ofer Heifetz <oferh@marvell.com>
+[Antoine: commit message]
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c        | 1 -
+ drivers/crypto/inside-secure/safexcel.h        | 1 -
+ drivers/crypto/inside-secure/safexcel_cipher.c | 2 +-
+ drivers/crypto/inside-secure/safexcel_hash.c   | 2 +-
+ 4 files changed, 2 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index d4c81cb73bee..3652aa1ec370 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -540,7 +540,6 @@ void safexcel_inv_complete(struct crypto_async_request *req, int error)
+ }
+ 
+ int safexcel_invalidate_cache(struct crypto_async_request *async,
+-			      struct safexcel_context *ctx,
+ 			      struct safexcel_crypto_priv *priv,
+ 			      dma_addr_t ctxr_dma, int ring,
+ 			      struct safexcel_request *request)
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 304c5838c11a..d12c2b479a5e 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -539,7 +539,6 @@ void safexcel_free_context(struct safexcel_crypto_priv *priv,
+ 				  struct crypto_async_request *req,
+ 				  int result_sz);
+ int safexcel_invalidate_cache(struct crypto_async_request *async,
+-			      struct safexcel_context *ctx,
+ 			      struct safexcel_crypto_priv *priv,
+ 			      dma_addr_t ctxr_dma, int ring,
+ 			      struct safexcel_request *request);
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index 62382a7c4cbe..fe1d588d6a25 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -395,7 +395,7 @@ static int safexcel_cipher_send_inv(struct crypto_async_request *async,
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int ret;
+ 
+-	ret = safexcel_invalidate_cache(async, &ctx->base, priv,
++	ret = safexcel_invalidate_cache(async, priv,
+ 					ctx->base.ctxr_dma, ring, request);
+ 	if (unlikely(ret))
+ 		return ret;
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 89ab80a20a2f..d17031724792 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -435,7 +435,7 @@ static int safexcel_ahash_send_inv(struct crypto_async_request *async,
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	int ret;
+ 
+-	ret = safexcel_invalidate_cache(async, &ctx->base, ctx->priv,
++	ret = safexcel_invalidate_cache(async, ctx->priv,
+ 					ctx->base.ctxr_dma, ring, request);
+ 	if (unlikely(ret))
+ 		return ret;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0052-crypto-inside-secure-move-request-dequeueing-into-a-.patch b/target/linux/mvebu/patches-4.14/0052-crypto-inside-secure-move-request-dequeueing-into-a-.patch
new file mode 100644
index 0000000..f45db55
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0052-crypto-inside-secure-move-request-dequeueing-into-a-.patch
@@ -0,0 +1,204 @@
+From db490acfb08309470c737aab9b4051b7b7b6978d Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:51 +0100
+Subject: [PATCH 52/88] crypto: inside-secure - move request dequeueing into a
+ workqueue
+
+This patch moves the request dequeueing into a workqueue to improve the
+coalescing of interrupts when sending requests to the engine; as the
+engine is capable of having one single interrupt for n requests sent.
+Using a workqueue allows to send more request at once.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c       | 29 ++++++++++---------
+ drivers/crypto/inside-secure/safexcel.h       |  2 +-
+ .../crypto/inside-secure/safexcel_cipher.c    | 12 ++++----
+ drivers/crypto/inside-secure/safexcel_hash.c  | 12 ++++----
+ 4 files changed, 29 insertions(+), 26 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 3652aa1ec370..817c55015fba 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -429,8 +429,6 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	struct safexcel_request *request;
+ 	int ret, nreq = 0, cdesc = 0, rdesc = 0, commands, results;
+ 
+-	priv->ring[ring].need_dequeue = false;
+-
+ 	do {
+ 		spin_lock_bh(&priv->ring[ring].queue_lock);
+ 		backlog = crypto_get_backlog(&priv->ring[ring].queue);
+@@ -445,8 +443,6 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 			spin_lock_bh(&priv->ring[ring].queue_lock);
+ 			crypto_enqueue_request(&priv->ring[ring].queue, req);
+ 			spin_unlock_bh(&priv->ring[ring].queue_lock);
+-
+-			priv->ring[ring].need_dequeue = true;
+ 			goto finalize;
+ 		}
+ 
+@@ -455,7 +451,6 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 		if (ret) {
+ 			kfree(request);
+ 			req->complete(req, ret);
+-			priv->ring[ring].need_dequeue = true;
+ 			goto finalize;
+ 		}
+ 
+@@ -471,9 +466,7 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	} while (nreq++ < EIP197_MAX_BATCH_SZ);
+ 
+ finalize:
+-	if (nreq == EIP197_MAX_BATCH_SZ)
+-		priv->ring[ring].need_dequeue = true;
+-	else if (!nreq)
++	if (!nreq)
+ 		return;
+ 
+ 	spin_lock_bh(&priv->ring[ring].lock);
+@@ -628,13 +621,18 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ static void safexcel_handle_result_work(struct work_struct *work)
+ {
+ 	struct safexcel_work_data *data =
+-			container_of(work, struct safexcel_work_data, work);
++			container_of(work, struct safexcel_work_data, result_work);
+ 	struct safexcel_crypto_priv *priv = data->priv;
+ 
+ 	safexcel_handle_result_descriptor(priv, data->ring);
++}
++
++static void safexcel_dequeue_work(struct work_struct *work)
++{
++	struct safexcel_work_data *data =
++			container_of(work, struct safexcel_work_data, work);
+ 
+-	if (priv->ring[data->ring].need_dequeue)
+-		safexcel_dequeue(data->priv, data->ring);
++	safexcel_dequeue(data->priv, data->ring);
+ }
+ 
+ struct safexcel_ring_irq_data {
+@@ -665,7 +663,10 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ 			 */
+ 			dev_err(priv->dev, "RDR: fatal error.");
+ 		} else if (likely(stat & EIP197_xDR_THRESH)) {
+-			queue_work(priv->ring[ring].workqueue, &priv->ring[ring].work_data.work);
++			queue_work(priv->ring[ring].workqueue,
++				   &priv->ring[ring].work_data.result_work);
++			queue_work(priv->ring[ring].workqueue,
++				   &priv->ring[ring].work_data.work);
+ 		}
+ 
+ 		/* ACK the interrupts */
+@@ -846,7 +847,9 @@ static int safexcel_probe(struct platform_device *pdev)
+ 
+ 		priv->ring[i].work_data.priv = priv;
+ 		priv->ring[i].work_data.ring = i;
+-		INIT_WORK(&priv->ring[i].work_data.work, safexcel_handle_result_work);
++		INIT_WORK(&priv->ring[i].work_data.result_work,
++			  safexcel_handle_result_work);
++		INIT_WORK(&priv->ring[i].work_data.work, safexcel_dequeue_work);
+ 
+ 		snprintf(wq_name, 9, "wq_ring%d", i);
+ 		priv->ring[i].workqueue = create_singlethread_workqueue(wq_name);
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index d12c2b479a5e..8e9c65183439 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -459,6 +459,7 @@ struct safexcel_config {
+ 
+ struct safexcel_work_data {
+ 	struct work_struct work;
++	struct work_struct result_work;
+ 	struct safexcel_crypto_priv *priv;
+ 	int ring;
+ };
+@@ -489,7 +490,6 @@ struct safexcel_crypto_priv {
+ 		/* queue */
+ 		struct crypto_queue queue;
+ 		spinlock_t queue_lock;
+-		bool need_dequeue;
+ 	} ring[EIP197_MAX_RINGS];
+ };
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index fe1d588d6a25..0e5cc230e49a 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -358,8 +358,8 @@ static int safexcel_handle_inv_result(struct safexcel_crypto_priv *priv,
+ 	if (enq_ret != -EINPROGRESS)
+ 		*ret = enq_ret;
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	*should_complete = false;
+ 
+@@ -448,8 +448,8 @@ static int safexcel_cipher_exit_inv(struct crypto_tfm *tfm)
+ 	crypto_enqueue_request(&priv->ring[ring].queue, &req->base);
+ 	spin_unlock_bh(&priv->ring[ring].queue_lock);
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	wait_for_completion_interruptible(&result.completion);
+ 
+@@ -495,8 +495,8 @@ static int safexcel_aes(struct skcipher_request *req,
+ 	ret = crypto_enqueue_request(&priv->ring[ring].queue, &req->base);
+ 	spin_unlock_bh(&priv->ring[ring].queue_lock);
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	return ret;
+ }
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index d17031724792..9c0cfdaf87f8 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -399,8 +399,8 @@ static int safexcel_handle_inv_result(struct safexcel_crypto_priv *priv,
+ 	if (enq_ret != -EINPROGRESS)
+ 		*ret = enq_ret;
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	*should_complete = false;
+ 
+@@ -488,8 +488,8 @@ static int safexcel_ahash_exit_inv(struct crypto_tfm *tfm)
+ 	crypto_enqueue_request(&priv->ring[ring].queue, &req->base);
+ 	spin_unlock_bh(&priv->ring[ring].queue_lock);
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	wait_for_completion_interruptible(&result.completion);
+ 
+@@ -564,8 +564,8 @@ static int safexcel_ahash_enqueue(struct ahash_request *areq)
+ 	ret = crypto_enqueue_request(&priv->ring[ring].queue, &areq->base);
+ 	spin_unlock_bh(&priv->ring[ring].queue_lock);
+ 
+-	if (!priv->ring[ring].need_dequeue)
+-		safexcel_dequeue(priv, ring);
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
+ 
+ 	return ret;
+ }
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0053-crypto-inside-secure-use-threaded-IRQs-for-result-ha.patch b/target/linux/mvebu/patches-4.14/0053-crypto-inside-secure-use-threaded-IRQs-for-result-ha.patch
new file mode 100644
index 0000000..bea96fd
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0053-crypto-inside-secure-use-threaded-IRQs-for-result-ha.patch
@@ -0,0 +1,136 @@
+From 657d4405dc469be3e32335a71149721028fdd74d Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:52 +0100
+Subject: [PATCH 53/88] crypto: inside-secure - use threaded IRQs for result
+ handling
+
+This patch moves the result handling from an IRQ handler to a threaded
+IRQ handler, to improve the number of complete requests being handled at
+once.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 41 +++++++++++++------------
+ drivers/crypto/inside-secure/safexcel.h |  1 -
+ 2 files changed, 22 insertions(+), 20 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 817c55015fba..d2979a50c0b4 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -618,15 +618,6 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 	}
+ }
+ 
+-static void safexcel_handle_result_work(struct work_struct *work)
+-{
+-	struct safexcel_work_data *data =
+-			container_of(work, struct safexcel_work_data, result_work);
+-	struct safexcel_crypto_priv *priv = data->priv;
+-
+-	safexcel_handle_result_descriptor(priv, data->ring);
+-}
+-
+ static void safexcel_dequeue_work(struct work_struct *work)
+ {
+ 	struct safexcel_work_data *data =
+@@ -644,12 +635,12 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ {
+ 	struct safexcel_ring_irq_data *irq_data = data;
+ 	struct safexcel_crypto_priv *priv = irq_data->priv;
+-	int ring = irq_data->ring;
++	int ring = irq_data->ring, rc = IRQ_NONE;
+ 	u32 status, stat;
+ 
+ 	status = readl(priv->base + EIP197_HIA_AIC_R_ENABLED_STAT(ring));
+ 	if (!status)
+-		return IRQ_NONE;
++		return rc;
+ 
+ 	/* RDR interrupts */
+ 	if (status & EIP197_RDR_IRQ(ring)) {
+@@ -663,10 +654,7 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ 			 */
+ 			dev_err(priv->dev, "RDR: fatal error.");
+ 		} else if (likely(stat & EIP197_xDR_THRESH)) {
+-			queue_work(priv->ring[ring].workqueue,
+-				   &priv->ring[ring].work_data.result_work);
+-			queue_work(priv->ring[ring].workqueue,
+-				   &priv->ring[ring].work_data.work);
++			rc = IRQ_WAKE_THREAD;
+ 		}
+ 
+ 		/* ACK the interrupts */
+@@ -677,11 +665,26 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ 	/* ACK the interrupts */
+ 	writel(status, priv->base + EIP197_HIA_AIC_R_ACK(ring));
+ 
++	return rc;
++}
++
++static irqreturn_t safexcel_irq_ring_thread(int irq, void *data)
++{
++	struct safexcel_ring_irq_data *irq_data = data;
++	struct safexcel_crypto_priv *priv = irq_data->priv;
++	int ring = irq_data->ring;
++
++	safexcel_handle_result_descriptor(priv, ring);
++
++	queue_work(priv->ring[ring].workqueue,
++		   &priv->ring[ring].work_data.work);
++
+ 	return IRQ_HANDLED;
+ }
+ 
+ static int safexcel_request_ring_irq(struct platform_device *pdev, const char *name,
+ 				     irq_handler_t handler,
++				     irq_handler_t threaded_handler,
+ 				     struct safexcel_ring_irq_data *ring_irq_priv)
+ {
+ 	int ret, irq = platform_get_irq_byname(pdev, name);
+@@ -691,8 +694,9 @@ static int safexcel_request_ring_irq(struct platform_device *pdev, const char *n
+ 		return irq;
+ 	}
+ 
+-	ret = devm_request_irq(&pdev->dev, irq, handler, 0,
+-			       dev_name(&pdev->dev), ring_irq_priv);
++	ret = devm_request_threaded_irq(&pdev->dev, irq, handler,
++					threaded_handler, IRQF_ONESHOT,
++					dev_name(&pdev->dev), ring_irq_priv);
+ 	if (ret) {
+ 		dev_err(&pdev->dev, "unable to request IRQ %d\n", irq);
+ 		return ret;
+@@ -839,6 +843,7 @@ static int safexcel_probe(struct platform_device *pdev)
+ 
+ 		snprintf(irq_name, 6, "ring%d", i);
+ 		irq = safexcel_request_ring_irq(pdev, irq_name, safexcel_irq_ring,
++						safexcel_irq_ring_thread,
+ 						ring_irq);
+ 		if (irq < 0) {
+ 			ret = irq;
+@@ -847,8 +852,6 @@ static int safexcel_probe(struct platform_device *pdev)
+ 
+ 		priv->ring[i].work_data.priv = priv;
+ 		priv->ring[i].work_data.ring = i;
+-		INIT_WORK(&priv->ring[i].work_data.result_work,
+-			  safexcel_handle_result_work);
+ 		INIT_WORK(&priv->ring[i].work_data.work, safexcel_dequeue_work);
+ 
+ 		snprintf(wq_name, 9, "wq_ring%d", i);
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 8e9c65183439..fffddefb0d9b 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -459,7 +459,6 @@ struct safexcel_config {
+ 
+ struct safexcel_work_data {
+ 	struct work_struct work;
+-	struct work_struct result_work;
+ 	struct safexcel_crypto_priv *priv;
+ 	int ring;
+ };
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0054-crypto-inside-secure-dequeue-all-requests-at-once.patch b/target/linux/mvebu/patches-4.14/0054-crypto-inside-secure-dequeue-all-requests-at-once.patch
new file mode 100644
index 0000000..8a1a12b
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0054-crypto-inside-secure-dequeue-all-requests-at-once.patch
@@ -0,0 +1,179 @@
+From 8f08941187135b1f32519dad50b03abe2d65baf7 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:53 +0100
+Subject: [PATCH 54/88] crypto: inside-secure - dequeue all requests at once
+
+This patch updates the dequeueing logic to dequeue all requests at once.
+Since we can have many requests in the queue, the interrupt coalescing
+is kept so that the ring interrupt fires every EIP197_MAX_BATCH_SZ at
+most.
+
+To allow dequeueing all requests at once while still using reasonable
+settings for the interrupt coalescing, the result handling function was
+updated to setup the threshold interrupt when needed (i.e. when more
+requests than EIP197_MAX_BATCH_SZ are in the queue). When using this
+capability the ring is marked as busy so that the dequeue function
+enqueue new requests without setting the threshold interrupt.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 60 ++++++++++++++++++++-----
+ drivers/crypto/inside-secure/safexcel.h |  8 ++++
+ 2 files changed, 56 insertions(+), 12 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index d2979a50c0b4..265a540211ad 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -422,6 +422,23 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	return 0;
+ }
+ 
++/* Called with ring's lock taken */
++int safexcel_try_push_requests(struct safexcel_crypto_priv *priv, int ring,
++			       int reqs)
++{
++	int coal = min_t(int, reqs, EIP197_MAX_BATCH_SZ);
++
++	if (!coal)
++		return 0;
++
++	/* Configure when we want an interrupt */
++	writel(EIP197_HIA_RDR_THRESH_PKT_MODE |
++	       EIP197_HIA_RDR_THRESH_PROC_PKT(coal),
++	       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_THRESH);
++
++	return coal;
++}
++
+ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ {
+ 	struct crypto_async_request *req, *backlog;
+@@ -429,7 +446,7 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	struct safexcel_request *request;
+ 	int ret, nreq = 0, cdesc = 0, rdesc = 0, commands, results;
+ 
+-	do {
++	while (true) {
+ 		spin_lock_bh(&priv->ring[ring].queue_lock);
+ 		backlog = crypto_get_backlog(&priv->ring[ring].queue);
+ 		req = crypto_dequeue_request(&priv->ring[ring].queue);
+@@ -463,18 +480,24 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 
+ 		cdesc += commands;
+ 		rdesc += results;
+-	} while (nreq++ < EIP197_MAX_BATCH_SZ);
++		nreq++;
++	}
+ 
+ finalize:
+ 	if (!nreq)
+ 		return;
+ 
+-	spin_lock_bh(&priv->ring[ring].lock);
++	spin_lock_bh(&priv->ring[ring].egress_lock);
+ 
+-	/* Configure when we want an interrupt */
+-	writel(EIP197_HIA_RDR_THRESH_PKT_MODE |
+-	       EIP197_HIA_RDR_THRESH_PROC_PKT(nreq),
+-	       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_THRESH);
++	if (!priv->ring[ring].busy) {
++		nreq -= safexcel_try_push_requests(priv, ring, nreq);
++		if (nreq)
++			priv->ring[ring].busy = true;
++	}
++
++	priv->ring[ring].requests_left += nreq;
++
++	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 
+ 	/* let the RDR know we have pending descriptors */
+ 	writel((rdesc * priv->config.rd_offset) << 2,
+@@ -483,8 +506,6 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	/* let the CDR know we have pending descriptors */
+ 	writel((cdesc * priv->config.cd_offset) << 2,
+ 	       priv->base + EIP197_HIA_CDR(ring) + EIP197_HIA_xDR_PREP_COUNT);
+-
+-	spin_unlock_bh(&priv->ring[ring].lock);
+ }
+ 
+ void safexcel_free_context(struct safexcel_crypto_priv *priv,
+@@ -579,14 +600,14 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ {
+ 	struct safexcel_request *sreq;
+ 	struct safexcel_context *ctx;
+-	int ret, i, nreq, ndesc = 0;
++	int ret, i, nreq, ndesc = 0, done;
+ 	bool should_complete;
+ 
+ 	nreq = readl(priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+ 	nreq >>= 24;
+ 	nreq &= GENMASK(6, 0);
+ 	if (!nreq)
+-		return;
++		goto requests_left;
+ 
+ 	for (i = 0; i < nreq; i++) {
+ 		spin_lock_bh(&priv->ring[ring].egress_lock);
+@@ -601,7 +622,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 		if (ndesc < 0) {
+ 			kfree(sreq);
+ 			dev_err(priv->dev, "failed to handle result (%d)", ndesc);
+-			return;
++			goto requests_left;
+ 		}
+ 
+ 		writel(EIP197_xDR_PROC_xD_PKT(1) |
+@@ -616,6 +637,18 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 
+ 		kfree(sreq);
+ 	}
++
++requests_left:
++	spin_lock_bh(&priv->ring[ring].egress_lock);
++
++	done = safexcel_try_push_requests(priv, ring,
++					  priv->ring[ring].requests_left);
++
++	priv->ring[ring].requests_left -= done;
++	if (!done && !priv->ring[ring].requests_left)
++		priv->ring[ring].busy = false;
++
++	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ }
+ 
+ static void safexcel_dequeue_work(struct work_struct *work)
+@@ -861,6 +894,9 @@ static int safexcel_probe(struct platform_device *pdev)
+ 			goto err_clk;
+ 		}
+ 
++		priv->ring[i].requests_left = 0;
++		priv->ring[i].busy = false;
++
+ 		crypto_init_queue(&priv->ring[i].queue,
+ 				  EIP197_DEFAULT_RING_SIZE);
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index fffddefb0d9b..531e3e9d8384 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -489,6 +489,14 @@ struct safexcel_crypto_priv {
+ 		/* queue */
+ 		struct crypto_queue queue;
+ 		spinlock_t queue_lock;
++
++		/* Number of requests in the engine that needs the threshold
++		 * interrupt to be set up.
++		 */
++		int requests_left;
++
++		/* The ring is currently handling at least one request */
++		bool busy;
+ 	} ring[EIP197_MAX_RINGS];
+ };
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0055-crypto-inside-secure-increase-the-ring-size.patch b/target/linux/mvebu/patches-4.14/0055-crypto-inside-secure-increase-the-ring-size.patch
new file mode 100644
index 0000000..9c0a301
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0055-crypto-inside-secure-increase-the-ring-size.patch
@@ -0,0 +1,37 @@
+From 22c1850714fa6b80bcc4d4d71560858084ed470d Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:54 +0100
+Subject: [PATCH 55/88] crypto: inside-secure - increase the ring size
+
+Increase the ring size to handle more requests in parallel, while
+keeping the batch size (for interrupt coalescing) to its previous value.
+The ring size and batch size are now unlinked.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.h | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 531e3e9d8384..2a0ab6ce716a 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -19,11 +19,11 @@
+ #define EIP197_HIA_VERSION_BE			0x35ca
+ 
+ /* Static configuration */
+-#define EIP197_DEFAULT_RING_SIZE		64
++#define EIP197_DEFAULT_RING_SIZE		400
+ #define EIP197_MAX_TOKENS			5
+ #define EIP197_MAX_RINGS			4
+ #define EIP197_FETCH_COUNT			1
+-#define EIP197_MAX_BATCH_SZ			EIP197_DEFAULT_RING_SIZE
++#define EIP197_MAX_BATCH_SZ			64
+ 
+ #define EIP197_GFP_FLAGS(base)	((base).flags & CRYPTO_TFM_REQ_MAY_SLEEP ? \
+ 				 GFP_KERNEL : GFP_ATOMIC)
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0056-crypto-inside-secure-acknowledge-the-result-requests.patch b/target/linux/mvebu/patches-4.14/0056-crypto-inside-secure-acknowledge-the-result-requests.patch
new file mode 100644
index 0000000..6b47f7e
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0056-crypto-inside-secure-acknowledge-the-result-requests.patch
@@ -0,0 +1,62 @@
+From 0cf912fa3fc6ea2ffe7cad6b8aae9c86d4a731ed Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:55 +0100
+Subject: [PATCH 56/88] crypto: inside-secure - acknowledge the result requests
+ all at once
+
+This patches moves the result request acknowledgment from a per request
+process to acknowledging all the result requests handled at once.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 16 ++++++++++------
+ 1 file changed, 10 insertions(+), 6 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 265a540211ad..f51344d07d7c 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -600,7 +600,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ {
+ 	struct safexcel_request *sreq;
+ 	struct safexcel_context *ctx;
+-	int ret, i, nreq, ndesc = 0, done;
++	int ret, i, nreq, ndesc = 0, tot_descs = 0, done;
+ 	bool should_complete;
+ 
+ 	nreq = readl(priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+@@ -622,13 +622,9 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 		if (ndesc < 0) {
+ 			kfree(sreq);
+ 			dev_err(priv->dev, "failed to handle result (%d)", ndesc);
+-			goto requests_left;
++			goto acknowledge;
+ 		}
+ 
+-		writel(EIP197_xDR_PROC_xD_PKT(1) |
+-		       EIP197_xDR_PROC_xD_COUNT(ndesc * priv->config.rd_offset),
+-		       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+-
+ 		if (should_complete) {
+ 			local_bh_disable();
+ 			sreq->req->complete(sreq->req, ret);
+@@ -636,6 +632,14 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 		}
+ 
+ 		kfree(sreq);
++		tot_descs += ndesc;
++	}
++
++acknowledge:
++	if (i) {
++		writel(EIP197_xDR_PROC_xD_PKT(i) |
++		       EIP197_xDR_PROC_xD_COUNT(tot_descs * priv->config.rd_offset),
++		       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+ 	}
+ 
+ requests_left:
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0057-crypto-inside-secure-handle-more-result-requests-whe.patch b/target/linux/mvebu/patches-4.14/0057-crypto-inside-secure-handle-more-result-requests-whe.patch
new file mode 100644
index 0000000..9c60559
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0057-crypto-inside-secure-handle-more-result-requests-whe.patch
@@ -0,0 +1,70 @@
+From a8fbf74452ba32a8f1c3f6fde142d72cf91d9e3e Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:56 +0100
+Subject: [PATCH 57/88] crypto: inside-secure - handle more result requests
+ when counter is full
+
+This patch modifies the result handling logic to continue handling
+results when the completed requests counter is full and not showing the
+actual number of requests to handle.
+
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 15 ++++++++++++---
+ drivers/crypto/inside-secure/safexcel.h |  2 ++
+ 2 files changed, 14 insertions(+), 3 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index f51344d07d7c..311fb2378069 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -600,12 +600,15 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ {
+ 	struct safexcel_request *sreq;
+ 	struct safexcel_context *ctx;
+-	int ret, i, nreq, ndesc = 0, tot_descs = 0, done;
++	int ret, i, nreq, ndesc, tot_descs, done;
+ 	bool should_complete;
+ 
++handle_results:
++	tot_descs = 0;
++
+ 	nreq = readl(priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+-	nreq >>= 24;
+-	nreq &= GENMASK(6, 0);
++	nreq >>= EIP197_xDR_PROC_xD_PKT_OFFSET;
++	nreq &= EIP197_xDR_PROC_xD_PKT_MASK;
+ 	if (!nreq)
+ 		goto requests_left;
+ 
+@@ -642,6 +645,12 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 		       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+ 	}
+ 
++	/* If the number of requests overflowed the counter, try to proceed more
++	 * requests.
++	 */
++	if (nreq == EIP197_xDR_PROC_xD_PKT_MASK)
++		goto handle_results;
++
+ requests_left:
+ 	spin_lock_bh(&priv->ring[ring].egress_lock);
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 2a0ab6ce716a..0c47e792192d 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -117,6 +117,8 @@
+ #define EIP197_xDR_PREP_CLR_COUNT		BIT(31)
+ 
+ /* EIP197_HIA_xDR_PROC_COUNT */
++#define EIP197_xDR_PROC_xD_PKT_OFFSET		24
++#define EIP197_xDR_PROC_xD_PKT_MASK		GENMASK(6, 0)
+ #define EIP197_xDR_PROC_xD_COUNT(n)		((n) << 2)
+ #define EIP197_xDR_PROC_xD_PKT(n)		((n) << 24)
+ #define EIP197_xDR_PROC_CLR_COUNT		BIT(31)
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0058-crypto-inside-secure-retry-to-proceed-the-request-la.patch b/target/linux/mvebu/patches-4.14/0058-crypto-inside-secure-retry-to-proceed-the-request-la.patch
new file mode 100644
index 0000000..7a2ea12
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0058-crypto-inside-secure-retry-to-proceed-the-request-la.patch
@@ -0,0 +1,103 @@
+From 9162fd5eedef07113d67097f8ce2e0f4270dcbc0 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:57 +0100
+Subject: [PATCH 58/88] crypto: inside-secure - retry to proceed the request
+ later on fail
+
+The dequeueing function was putting back a request in the crypto queue
+on failure (when not enough resources are available) which is not
+perfect as the request will be handled much later. This patch updates
+this logic by keeping a reference on the failed request to try
+proceeding it later when enough resources are available.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 32 ++++++++++++++++++-------
+ drivers/crypto/inside-secure/safexcel.h |  6 +++++
+ 2 files changed, 29 insertions(+), 9 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 311fb2378069..7af8923103e7 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -446,29 +446,36 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	struct safexcel_request *request;
+ 	int ret, nreq = 0, cdesc = 0, rdesc = 0, commands, results;
+ 
++	/* If a request wasn't properly dequeued because of a lack of resources,
++	 * proceeded it first,
++	 */
++	req = priv->ring[ring].req;
++	backlog = priv->ring[ring].backlog;
++	if (req)
++		goto handle_req;
++
+ 	while (true) {
+ 		spin_lock_bh(&priv->ring[ring].queue_lock);
+ 		backlog = crypto_get_backlog(&priv->ring[ring].queue);
+ 		req = crypto_dequeue_request(&priv->ring[ring].queue);
+ 		spin_unlock_bh(&priv->ring[ring].queue_lock);
+ 
+-		if (!req)
++		if (!req) {
++			priv->ring[ring].req = NULL;
++			priv->ring[ring].backlog = NULL;
+ 			goto finalize;
++		}
+ 
++handle_req:
+ 		request = kzalloc(sizeof(*request), EIP197_GFP_FLAGS(*req));
+-		if (!request) {
+-			spin_lock_bh(&priv->ring[ring].queue_lock);
+-			crypto_enqueue_request(&priv->ring[ring].queue, req);
+-			spin_unlock_bh(&priv->ring[ring].queue_lock);
+-			goto finalize;
+-		}
++		if (!request)
++			goto request_failed;
+ 
+ 		ctx = crypto_tfm_ctx(req->tfm);
+ 		ret = ctx->send(req, ring, request, &commands, &results);
+ 		if (ret) {
+ 			kfree(request);
+-			req->complete(req, ret);
+-			goto finalize;
++			goto request_failed;
+ 		}
+ 
+ 		if (backlog)
+@@ -483,6 +490,13 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 		nreq++;
+ 	}
+ 
++request_failed:
++	/* Not enough resources to handle all the requests. Bail out and save
++	 * the request and the backlog for the next dequeue call (per-ring).
++	 */
++	priv->ring[ring].req = req;
++	priv->ring[ring].backlog = backlog;
++
+ finalize:
+ 	if (!nreq)
+ 		return;
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 0c47e792192d..d4955abf873b 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -499,6 +499,12 @@ struct safexcel_crypto_priv {
+ 
+ 		/* The ring is currently handling at least one request */
+ 		bool busy;
++
++		/* Store for current requests when bailing out of the dequeueing
++		 * function when no enough resources are available.
++		 */
++		struct crypto_async_request *req;
++		struct crypto_async_request *backlog;
+ 	} ring[EIP197_MAX_RINGS];
+ };
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0059-crypto-inside-secure-EIP97-support.patch b/target/linux/mvebu/patches-4.14/0059-crypto-inside-secure-EIP97-support.patch
new file mode 100644
index 0000000..8f65d89
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0059-crypto-inside-secure-EIP97-support.patch
@@ -0,0 +1,841 @@
+From 6759a2f3aeb5734aa55bd60c1fca20bcef9a060e Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Antoine=20T=C3=A9nart?= <antoine.tenart@free-electrons.com>
+Date: Thu, 14 Dec 2017 15:26:58 +0100
+Subject: [PATCH 59/88] crypto: inside-secure - EIP97 support
+
+The Inside Secure SafeXcel driver was firstly designed to support the
+EIP197 cryptographic engine which is an evolution (with much more
+feature, better performances) of the EIP97 cryptographic engine. This
+patch convert the Inside Secure SafeXcel driver to support both engines
+(EIP97 + EIP197).
+
+The main differences are the register offsets and the context
+invalidation process which is EIP197 specific. This patch adds an
+indirection on the register offsets and adds checks not to send any
+invalidation request when driving the EIP97. A new compatible is added
+as well to bind the driver from device trees.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c       | 212 +++++++++++-------
+ drivers/crypto/inside-secure/safexcel.h       | 151 +++++++++----
+ .../crypto/inside-secure/safexcel_cipher.c    |  20 +-
+ drivers/crypto/inside-secure/safexcel_hash.c  |  19 +-
+ 4 files changed, 264 insertions(+), 138 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 7af8923103e7..729b26b50e4f 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -108,10 +108,10 @@ static void eip197_write_firmware(struct safexcel_crypto_priv *priv,
+ 	writel(EIP197_PE_ICE_x_CTRL_SW_RESET |
+ 	       EIP197_PE_ICE_x_CTRL_CLR_ECC_CORR |
+ 	       EIP197_PE_ICE_x_CTRL_CLR_ECC_NON_CORR,
+-	       priv->base + ctrl);
++	       EIP197_PE(priv) + ctrl);
+ 
+ 	/* Enable access to the program memory */
+-	writel(prog_en, priv->base + EIP197_PE_ICE_RAM_CTRL);
++	writel(prog_en, EIP197_PE(priv) + EIP197_PE_ICE_RAM_CTRL);
+ 
+ 	/* Write the firmware */
+ 	for (i = 0; i < fw->size / sizeof(u32); i++)
+@@ -119,12 +119,12 @@ static void eip197_write_firmware(struct safexcel_crypto_priv *priv,
+ 		       priv->base + EIP197_CLASSIFICATION_RAMS + i * sizeof(u32));
+ 
+ 	/* Disable access to the program memory */
+-	writel(0, priv->base + EIP197_PE_ICE_RAM_CTRL);
++	writel(0, EIP197_PE(priv) + EIP197_PE_ICE_RAM_CTRL);
+ 
+ 	/* Release engine from reset */
+-	val = readl(priv->base + ctrl);
++	val = readl(EIP197_PE(priv) + ctrl);
+ 	val &= ~EIP197_PE_ICE_x_CTRL_SW_RESET;
+-	writel(val, priv->base + ctrl);
++	writel(val, EIP197_PE(priv) + ctrl);
+ }
+ 
+ static int eip197_load_firmwares(struct safexcel_crypto_priv *priv)
+@@ -145,14 +145,14 @@ static int eip197_load_firmwares(struct safexcel_crypto_priv *priv)
+ 	 }
+ 
+ 	/* Clear the scratchpad memory */
+-	val = readl(priv->base + EIP197_PE_ICE_SCRATCH_CTRL);
++	val = readl(EIP197_PE(priv) + EIP197_PE_ICE_SCRATCH_CTRL);
+ 	val |= EIP197_PE_ICE_SCRATCH_CTRL_CHANGE_TIMER |
+ 	       EIP197_PE_ICE_SCRATCH_CTRL_TIMER_EN |
+ 	       EIP197_PE_ICE_SCRATCH_CTRL_SCRATCH_ACCESS |
+ 	       EIP197_PE_ICE_SCRATCH_CTRL_CHANGE_ACCESS;
+-	writel(val, priv->base + EIP197_PE_ICE_SCRATCH_CTRL);
++	writel(val, EIP197_PE(priv) + EIP197_PE_ICE_SCRATCH_CTRL);
+ 
+-	memset(priv->base + EIP197_PE_ICE_SCRATCH_RAM, 0,
++	memset(EIP197_PE(priv) + EIP197_PE_ICE_SCRATCH_RAM, 0,
+ 	       EIP197_NUM_OF_SCRATCH_BLOCKS * sizeof(u32));
+ 
+ 	eip197_write_firmware(priv, fw[FW_IFPP], EIP197_PE_ICE_FPP_CTRL,
+@@ -173,7 +173,7 @@ static int safexcel_hw_setup_cdesc_rings(struct safexcel_crypto_priv *priv)
+ 	u32 hdw, cd_size_rnd, val;
+ 	int i;
+ 
+-	hdw = readl(priv->base + EIP197_HIA_OPTIONS);
++	hdw = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+ 	hdw &= GENMASK(27, 25);
+ 	hdw >>= 25;
+ 
+@@ -182,26 +182,25 @@ static int safexcel_hw_setup_cdesc_rings(struct safexcel_crypto_priv *priv)
+ 	for (i = 0; i < priv->config.rings; i++) {
+ 		/* ring base address */
+ 		writel(lower_32_bits(priv->ring[i].cdr.base_dma),
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
+ 		writel(upper_32_bits(priv->ring[i].cdr.base_dma),
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
+ 
+ 		writel(EIP197_xDR_DESC_MODE_64BIT | (priv->config.cd_offset << 16) |
+ 		       priv->config.cd_size,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_DESC_SIZE);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_DESC_SIZE);
+ 		writel(((EIP197_FETCH_COUNT * (cd_size_rnd << hdw)) << 16) |
+ 		       (EIP197_FETCH_COUNT * priv->config.cd_offset),
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_CFG);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_CFG);
+ 
+ 		/* Configure DMA tx control */
+ 		val = EIP197_HIA_xDR_CFG_WR_CACHE(WR_CACHE_3BITS);
+ 		val |= EIP197_HIA_xDR_CFG_RD_CACHE(RD_CACHE_3BITS);
+-		writel(val,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_DMA_CFG);
++		writel(val, EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_DMA_CFG);
+ 
+ 		/* clear any pending interrupt */
+ 		writel(GENMASK(5, 0),
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_STAT);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_STAT);
+ 	}
+ 
+ 	return 0;
+@@ -212,7 +211,7 @@ static int safexcel_hw_setup_rdesc_rings(struct safexcel_crypto_priv *priv)
+ 	u32 hdw, rd_size_rnd, val;
+ 	int i;
+ 
+-	hdw = readl(priv->base + EIP197_HIA_OPTIONS);
++	hdw = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+ 	hdw &= GENMASK(27, 25);
+ 	hdw >>= 25;
+ 
+@@ -221,33 +220,33 @@ static int safexcel_hw_setup_rdesc_rings(struct safexcel_crypto_priv *priv)
+ 	for (i = 0; i < priv->config.rings; i++) {
+ 		/* ring base address */
+ 		writel(lower_32_bits(priv->ring[i].rdr.base_dma),
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
+ 		writel(upper_32_bits(priv->ring[i].rdr.base_dma),
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
+ 
+ 		writel(EIP197_xDR_DESC_MODE_64BIT | (priv->config.rd_offset << 16) |
+ 		       priv->config.rd_size,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_DESC_SIZE);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_DESC_SIZE);
+ 
+ 		writel(((EIP197_FETCH_COUNT * (rd_size_rnd << hdw)) << 16) |
+ 		       (EIP197_FETCH_COUNT * priv->config.rd_offset),
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_CFG);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_CFG);
+ 
+ 		/* Configure DMA tx control */
+ 		val = EIP197_HIA_xDR_CFG_WR_CACHE(WR_CACHE_3BITS);
+ 		val |= EIP197_HIA_xDR_CFG_RD_CACHE(RD_CACHE_3BITS);
+ 		val |= EIP197_HIA_xDR_WR_RES_BUF | EIP197_HIA_xDR_WR_CTRL_BUG;
+ 		writel(val,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_DMA_CFG);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_DMA_CFG);
+ 
+ 		/* clear any pending interrupt */
+ 		writel(GENMASK(7, 0),
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_STAT);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_STAT);
+ 
+ 		/* enable ring interrupt */
+-		val = readl(priv->base + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
++		val = readl(EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
+ 		val |= EIP197_RDR_IRQ(i);
+-		writel(val, priv->base + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
++		writel(val, EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
+ 	}
+ 
+ 	return 0;
+@@ -259,39 +258,40 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	int i, ret;
+ 
+ 	/* Determine endianess and configure byte swap */
+-	version = readl(priv->base + EIP197_HIA_VERSION);
+-	val = readl(priv->base + EIP197_HIA_MST_CTRL);
++	version = readl(EIP197_HIA_AIC(priv) + EIP197_HIA_VERSION);
++	val = readl(EIP197_HIA_AIC(priv) + EIP197_HIA_MST_CTRL);
+ 
+ 	if ((version & 0xffff) == EIP197_HIA_VERSION_BE)
+ 		val |= EIP197_MST_CTRL_BYTE_SWAP;
+ 	else if (((version >> 16) & 0xffff) == EIP197_HIA_VERSION_LE)
+ 		val |= (EIP197_MST_CTRL_NO_BYTE_SWAP >> 24);
+ 
+-	writel(val, priv->base + EIP197_HIA_MST_CTRL);
+-
++	writel(val, EIP197_HIA_AIC(priv) + EIP197_HIA_MST_CTRL);
+ 
+ 	/* Configure wr/rd cache values */
+ 	writel(EIP197_MST_CTRL_RD_CACHE(RD_CACHE_4BITS) |
+ 	       EIP197_MST_CTRL_WD_CACHE(WR_CACHE_4BITS),
+-	       priv->base + EIP197_MST_CTRL);
++	       EIP197_HIA_GEN_CFG(priv) + EIP197_MST_CTRL);
+ 
+ 	/* Interrupts reset */
+ 
+ 	/* Disable all global interrupts */
+-	writel(0, priv->base + EIP197_HIA_AIC_G_ENABLE_CTRL);
++	writel(0, EIP197_HIA_AIC_G(priv) + EIP197_HIA_AIC_G_ENABLE_CTRL);
+ 
+ 	/* Clear any pending interrupt */
+-	writel(GENMASK(31, 0), priv->base + EIP197_HIA_AIC_G_ACK);
++	writel(GENMASK(31, 0), EIP197_HIA_AIC_G(priv) + EIP197_HIA_AIC_G_ACK);
+ 
+ 	/* Data Fetch Engine configuration */
+ 
+ 	/* Reset all DFE threads */
+ 	writel(EIP197_DxE_THR_CTRL_RESET_PE,
+-	       priv->base + EIP197_HIA_DFE_THR_CTRL);
++	       EIP197_HIA_DFE_THR(priv) + EIP197_HIA_DFE_THR_CTRL);
+ 
+-	/* Reset HIA input interface arbiter */
+-	writel(EIP197_HIA_RA_PE_CTRL_RESET,
+-	       priv->base + EIP197_HIA_RA_PE_CTRL);
++	if (priv->version == EIP197) {
++		/* Reset HIA input interface arbiter */
++		writel(EIP197_HIA_RA_PE_CTRL_RESET,
++		       EIP197_HIA_AIC(priv) + EIP197_HIA_RA_PE_CTRL);
++	}
+ 
+ 	/* DMA transfer size to use */
+ 	val = EIP197_HIA_DFE_CFG_DIS_DEBUG;
+@@ -299,29 +299,32 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	val |= EIP197_HIA_DxE_CFG_MIN_CTRL_SIZE(5) | EIP197_HIA_DxE_CFG_MAX_CTRL_SIZE(7);
+ 	val |= EIP197_HIA_DxE_CFG_DATA_CACHE_CTRL(RD_CACHE_3BITS);
+ 	val |= EIP197_HIA_DxE_CFG_CTRL_CACHE_CTRL(RD_CACHE_3BITS);
+-	writel(val, priv->base + EIP197_HIA_DFE_CFG);
++	writel(val, EIP197_HIA_DFE(priv) + EIP197_HIA_DFE_CFG);
+ 
+ 	/* Leave the DFE threads reset state */
+-	writel(0, priv->base + EIP197_HIA_DFE_THR_CTRL);
++	writel(0, EIP197_HIA_DFE_THR(priv) + EIP197_HIA_DFE_THR_CTRL);
+ 
+ 	/* Configure the procesing engine thresholds */
+ 	writel(EIP197_PE_IN_xBUF_THRES_MIN(5) | EIP197_PE_IN_xBUF_THRES_MAX(9),
+-	      priv->base + EIP197_PE_IN_DBUF_THRES);
++	       EIP197_PE(priv) + EIP197_PE_IN_DBUF_THRES);
+ 	writel(EIP197_PE_IN_xBUF_THRES_MIN(5) | EIP197_PE_IN_xBUF_THRES_MAX(7),
+-	      priv->base + EIP197_PE_IN_TBUF_THRES);
++	       EIP197_PE(priv) + EIP197_PE_IN_TBUF_THRES);
+ 
+-	/* enable HIA input interface arbiter and rings */
+-	writel(EIP197_HIA_RA_PE_CTRL_EN | GENMASK(priv->config.rings - 1, 0),
+-	       priv->base + EIP197_HIA_RA_PE_CTRL);
++	if (priv->version == EIP197) {
++		/* enable HIA input interface arbiter and rings */
++		writel(EIP197_HIA_RA_PE_CTRL_EN |
++		       GENMASK(priv->config.rings - 1, 0),
++		       EIP197_HIA_AIC(priv) + EIP197_HIA_RA_PE_CTRL);
++	}
+ 
+ 	/* Data Store Engine configuration */
+ 
+ 	/* Reset all DSE threads */
+ 	writel(EIP197_DxE_THR_CTRL_RESET_PE,
+-	       priv->base + EIP197_HIA_DSE_THR_CTRL);
++	       EIP197_HIA_DSE_THR(priv) + EIP197_HIA_DSE_THR_CTRL);
+ 
+ 	/* Wait for all DSE threads to complete */
+-	while ((readl(priv->base + EIP197_HIA_DSE_THR_STAT) &
++	while ((readl(EIP197_HIA_DSE_THR(priv) + EIP197_HIA_DSE_THR_STAT) &
+ 		GENMASK(15, 12)) != GENMASK(15, 12))
+ 		;
+ 
+@@ -330,15 +333,19 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	val |= EIP197_HIA_DxE_CFG_MIN_DATA_SIZE(7) | EIP197_HIA_DxE_CFG_MAX_DATA_SIZE(8);
+ 	val |= EIP197_HIA_DxE_CFG_DATA_CACHE_CTRL(WR_CACHE_3BITS);
+ 	val |= EIP197_HIA_DSE_CFG_ALLWAYS_BUFFERABLE;
+-	val |= EIP197_HIA_DSE_CFG_EN_SINGLE_WR;
+-	writel(val, priv->base + EIP197_HIA_DSE_CFG);
++	/* FIXME: instability issues can occur for EIP97 but disabling it impact
++	 * performances.
++	 */
++	if (priv->version == EIP197)
++		val |= EIP197_HIA_DSE_CFG_EN_SINGLE_WR;
++	writel(val, EIP197_HIA_DSE(priv) + EIP197_HIA_DSE_CFG);
+ 
+ 	/* Leave the DSE threads reset state */
+-	writel(0, priv->base + EIP197_HIA_DSE_THR_CTRL);
++	writel(0, EIP197_HIA_DSE_THR(priv) + EIP197_HIA_DSE_THR_CTRL);
+ 
+ 	/* Configure the procesing engine thresholds */
+ 	writel(EIP197_PE_OUT_DBUF_THRES_MIN(7) | EIP197_PE_OUT_DBUF_THRES_MAX(8),
+-	       priv->base + EIP197_PE_OUT_DBUF_THRES);
++	       EIP197_PE(priv) + EIP197_PE_OUT_DBUF_THRES);
+ 
+ 	/* Processing Engine configuration */
+ 
+@@ -348,73 +355,75 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	val |= EIP197_ALG_AES_ECB | EIP197_ALG_AES_CBC;
+ 	val |= EIP197_ALG_SHA1 | EIP197_ALG_HMAC_SHA1;
+ 	val |= EIP197_ALG_SHA2;
+-	writel(val, priv->base + EIP197_PE_EIP96_FUNCTION_EN);
++	writel(val, EIP197_PE(priv) + EIP197_PE_EIP96_FUNCTION_EN);
+ 
+ 	/* Command Descriptor Rings prepare */
+ 	for (i = 0; i < priv->config.rings; i++) {
+ 		/* Clear interrupts for this ring */
+ 		writel(GENMASK(31, 0),
+-		       priv->base + EIP197_HIA_AIC_R_ENABLE_CLR(i));
++		       EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLE_CLR(i));
+ 
+ 		/* Disable external triggering */
+-		writel(0, priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_CFG);
++		writel(0, EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_CFG);
+ 
+ 		/* Clear the pending prepared counter */
+ 		writel(EIP197_xDR_PREP_CLR_COUNT,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_PREP_COUNT);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_PREP_COUNT);
+ 
+ 		/* Clear the pending processed counter */
+ 		writel(EIP197_xDR_PROC_CLR_COUNT,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_PROC_COUNT);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_PROC_COUNT);
+ 
+ 		writel(0,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_PREP_PNTR);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_PREP_PNTR);
+ 		writel(0,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_PROC_PNTR);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_PROC_PNTR);
+ 
+ 		writel((EIP197_DEFAULT_RING_SIZE * priv->config.cd_offset) << 2,
+-		       priv->base + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_RING_SIZE);
++		       EIP197_HIA_CDR(priv, i) + EIP197_HIA_xDR_RING_SIZE);
+ 	}
+ 
+ 	/* Result Descriptor Ring prepare */
+ 	for (i = 0; i < priv->config.rings; i++) {
+ 		/* Disable external triggering*/
+-		writel(0, priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_CFG);
++		writel(0, EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_CFG);
+ 
+ 		/* Clear the pending prepared counter */
+ 		writel(EIP197_xDR_PREP_CLR_COUNT,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_PREP_COUNT);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_PREP_COUNT);
+ 
+ 		/* Clear the pending processed counter */
+ 		writel(EIP197_xDR_PROC_CLR_COUNT,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_PROC_COUNT);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_PROC_COUNT);
+ 
+ 		writel(0,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_PREP_PNTR);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_PREP_PNTR);
+ 		writel(0,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_PROC_PNTR);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_PROC_PNTR);
+ 
+ 		/* Ring size */
+ 		writel((EIP197_DEFAULT_RING_SIZE * priv->config.rd_offset) << 2,
+-		       priv->base + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_RING_SIZE);
++		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_RING_SIZE);
+ 	}
+ 
+ 	/* Enable command descriptor rings */
+ 	writel(EIP197_DxE_THR_CTRL_EN | GENMASK(priv->config.rings - 1, 0),
+-	       priv->base + EIP197_HIA_DFE_THR_CTRL);
++	       EIP197_HIA_DFE_THR(priv) + EIP197_HIA_DFE_THR_CTRL);
+ 
+ 	/* Enable result descriptor rings */
+ 	writel(EIP197_DxE_THR_CTRL_EN | GENMASK(priv->config.rings - 1, 0),
+-	       priv->base + EIP197_HIA_DSE_THR_CTRL);
++	       EIP197_HIA_DSE_THR(priv) + EIP197_HIA_DSE_THR_CTRL);
+ 
+ 	/* Clear any HIA interrupt */
+-	writel(GENMASK(30, 20), priv->base + EIP197_HIA_AIC_G_ACK);
++	writel(GENMASK(30, 20), EIP197_HIA_AIC_G(priv) + EIP197_HIA_AIC_G_ACK);
+ 
+-	eip197_trc_cache_init(priv);
++	if (priv->version == EIP197) {
++		eip197_trc_cache_init(priv);
+ 
+-	ret = eip197_load_firmwares(priv);
+-	if (ret)
+-		return ret;
++		ret = eip197_load_firmwares(priv);
++		if (ret)
++			return ret;
++	}
+ 
+ 	safexcel_hw_setup_cdesc_rings(priv);
+ 	safexcel_hw_setup_rdesc_rings(priv);
+@@ -434,7 +443,7 @@ int safexcel_try_push_requests(struct safexcel_crypto_priv *priv, int ring,
+ 	/* Configure when we want an interrupt */
+ 	writel(EIP197_HIA_RDR_THRESH_PKT_MODE |
+ 	       EIP197_HIA_RDR_THRESH_PROC_PKT(coal),
+-	       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_THRESH);
++	       EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_THRESH);
+ 
+ 	return coal;
+ }
+@@ -515,11 +524,11 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 
+ 	/* let the RDR know we have pending descriptors */
+ 	writel((rdesc * priv->config.rd_offset) << 2,
+-	       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PREP_COUNT);
++	       EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_PREP_COUNT);
+ 
+ 	/* let the CDR know we have pending descriptors */
+ 	writel((cdesc * priv->config.cd_offset) << 2,
+-	       priv->base + EIP197_HIA_CDR(ring) + EIP197_HIA_xDR_PREP_COUNT);
++	       EIP197_HIA_CDR(priv, ring) + EIP197_HIA_xDR_PREP_COUNT);
+ }
+ 
+ void safexcel_free_context(struct safexcel_crypto_priv *priv,
+@@ -620,7 +629,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ handle_results:
+ 	tot_descs = 0;
+ 
+-	nreq = readl(priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
++	nreq = readl(EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_PROC_COUNT);
+ 	nreq >>= EIP197_xDR_PROC_xD_PKT_OFFSET;
+ 	nreq &= EIP197_xDR_PROC_xD_PKT_MASK;
+ 	if (!nreq)
+@@ -656,7 +665,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 	if (i) {
+ 		writel(EIP197_xDR_PROC_xD_PKT(i) |
+ 		       EIP197_xDR_PROC_xD_COUNT(tot_descs * priv->config.rd_offset),
+-		       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
++		       EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_PROC_COUNT);
+ 	}
+ 
+ 	/* If the number of requests overflowed the counter, try to proceed more
+@@ -698,13 +707,13 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ 	int ring = irq_data->ring, rc = IRQ_NONE;
+ 	u32 status, stat;
+ 
+-	status = readl(priv->base + EIP197_HIA_AIC_R_ENABLED_STAT(ring));
++	status = readl(EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLED_STAT(ring));
+ 	if (!status)
+ 		return rc;
+ 
+ 	/* RDR interrupts */
+ 	if (status & EIP197_RDR_IRQ(ring)) {
+-		stat = readl(priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_STAT);
++		stat = readl(EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_STAT);
+ 
+ 		if (unlikely(stat & EIP197_xDR_ERR)) {
+ 			/*
+@@ -719,11 +728,11 @@ static irqreturn_t safexcel_irq_ring(int irq, void *data)
+ 
+ 		/* ACK the interrupts */
+ 		writel(stat & 0xff,
+-		       priv->base + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_STAT);
++		       EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_STAT);
+ 	}
+ 
+ 	/* ACK the interrupts */
+-	writel(status, priv->base + EIP197_HIA_AIC_R_ACK(ring));
++	writel(status, EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ACK(ring));
+ 
+ 	return rc;
+ }
+@@ -819,11 +828,11 @@ static void safexcel_configure(struct safexcel_crypto_priv *priv)
+ {
+ 	u32 val, mask;
+ 
+-	val = readl(priv->base + EIP197_HIA_OPTIONS);
++	val = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+ 	val = (val & GENMASK(27, 25)) >> 25;
+ 	mask = BIT(val) - 1;
+ 
+-	val = readl(priv->base + EIP197_HIA_OPTIONS);
++	val = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+ 	priv->config.rings = min_t(u32, val & GENMASK(3, 0), max_rings);
+ 
+ 	priv->config.cd_size = (sizeof(struct safexcel_command_desc) / sizeof(u32));
+@@ -833,6 +842,35 @@ static void safexcel_configure(struct safexcel_crypto_priv *priv)
+ 	priv->config.rd_offset = (priv->config.rd_size + mask) & ~mask;
+ }
+ 
++static void safexcel_init_register_offsets(struct safexcel_crypto_priv *priv)
++{
++	struct safexcel_register_offsets *offsets = &priv->offsets;
++
++	if (priv->version == EIP197) {
++		offsets->hia_aic	= EIP197_HIA_AIC_BASE;
++		offsets->hia_aic_g	= EIP197_HIA_AIC_G_BASE;
++		offsets->hia_aic_r	= EIP197_HIA_AIC_R_BASE;
++		offsets->hia_aic_xdr	= EIP197_HIA_AIC_xDR_BASE;
++		offsets->hia_dfe	= EIP197_HIA_DFE_BASE;
++		offsets->hia_dfe_thr	= EIP197_HIA_DFE_THR_BASE;
++		offsets->hia_dse	= EIP197_HIA_DSE_BASE;
++		offsets->hia_dse_thr	= EIP197_HIA_DSE_THR_BASE;
++		offsets->hia_gen_cfg	= EIP197_HIA_GEN_CFG_BASE;
++		offsets->pe		= EIP197_PE_BASE;
++	} else {
++		offsets->hia_aic	= EIP97_HIA_AIC_BASE;
++		offsets->hia_aic_g	= EIP97_HIA_AIC_G_BASE;
++		offsets->hia_aic_r	= EIP97_HIA_AIC_R_BASE;
++		offsets->hia_aic_xdr	= EIP97_HIA_AIC_xDR_BASE;
++		offsets->hia_dfe	= EIP97_HIA_DFE_BASE;
++		offsets->hia_dfe_thr	= EIP97_HIA_DFE_THR_BASE;
++		offsets->hia_dse	= EIP97_HIA_DSE_BASE;
++		offsets->hia_dse_thr	= EIP97_HIA_DSE_THR_BASE;
++		offsets->hia_gen_cfg	= EIP97_HIA_GEN_CFG_BASE;
++		offsets->pe		= EIP97_PE_BASE;
++	}
++}
++
+ static int safexcel_probe(struct platform_device *pdev)
+ {
+ 	struct device *dev = &pdev->dev;
+@@ -845,6 +883,9 @@ static int safexcel_probe(struct platform_device *pdev)
+ 		return -ENOMEM;
+ 
+ 	priv->dev = dev;
++	priv->version = (enum safexcel_eip_version)of_device_get_match_data(dev);
++
++	safexcel_init_register_offsets(priv);
+ 
+ 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+ 	priv->base = devm_ioremap_resource(dev, res);
+@@ -971,7 +1012,14 @@ static int safexcel_remove(struct platform_device *pdev)
+ }
+ 
+ static const struct of_device_id safexcel_of_match_table[] = {
+-	{ .compatible = "inside-secure,safexcel-eip197" },
++	{
++		.compatible = "inside-secure,safexcel-eip97",
++		.data = (void *)EIP97,
++	},
++	{
++		.compatible = "inside-secure,safexcel-eip197",
++		.data = (void *)EIP197,
++	},
+ 	{},
+ };
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index d4955abf873b..4e219c21608b 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -28,55 +28,94 @@
+ #define EIP197_GFP_FLAGS(base)	((base).flags & CRYPTO_TFM_REQ_MAY_SLEEP ? \
+ 				 GFP_KERNEL : GFP_ATOMIC)
+ 
++/* Register base offsets */
++#define EIP197_HIA_AIC(priv)		((priv)->base + (priv)->offsets.hia_aic)
++#define EIP197_HIA_AIC_G(priv)		((priv)->base + (priv)->offsets.hia_aic_g)
++#define EIP197_HIA_AIC_R(priv)		((priv)->base + (priv)->offsets.hia_aic_r)
++#define EIP197_HIA_AIC_xDR(priv)	((priv)->base + (priv)->offsets.hia_aic_xdr)
++#define EIP197_HIA_DFE(priv)		((priv)->base + (priv)->offsets.hia_dfe)
++#define EIP197_HIA_DFE_THR(priv)	((priv)->base + (priv)->offsets.hia_dfe_thr)
++#define EIP197_HIA_DSE(priv)		((priv)->base + (priv)->offsets.hia_dse)
++#define EIP197_HIA_DSE_THR(priv)	((priv)->base + (priv)->offsets.hia_dse_thr)
++#define EIP197_HIA_GEN_CFG(priv)	((priv)->base + (priv)->offsets.hia_gen_cfg)
++#define EIP197_PE(priv)			((priv)->base + (priv)->offsets.pe)
++
++/* EIP197 base offsets */
++#define EIP197_HIA_AIC_BASE		0x90000
++#define EIP197_HIA_AIC_G_BASE		0x90000
++#define EIP197_HIA_AIC_R_BASE		0x90800
++#define EIP197_HIA_AIC_xDR_BASE		0x80000
++#define EIP197_HIA_DFE_BASE		0x8c000
++#define EIP197_HIA_DFE_THR_BASE		0x8c040
++#define EIP197_HIA_DSE_BASE		0x8d000
++#define EIP197_HIA_DSE_THR_BASE		0x8d040
++#define EIP197_HIA_GEN_CFG_BASE		0xf0000
++#define EIP197_PE_BASE			0xa0000
++
++/* EIP97 base offsets */
++#define EIP97_HIA_AIC_BASE		0x0
++#define EIP97_HIA_AIC_G_BASE		0x0
++#define EIP97_HIA_AIC_R_BASE		0x0
++#define EIP97_HIA_AIC_xDR_BASE		0x0
++#define EIP97_HIA_DFE_BASE		0xf000
++#define EIP97_HIA_DFE_THR_BASE		0xf200
++#define EIP97_HIA_DSE_BASE		0xf400
++#define EIP97_HIA_DSE_THR_BASE		0xf600
++#define EIP97_HIA_GEN_CFG_BASE		0x10000
++#define EIP97_PE_BASE			0x10000
++
+ /* CDR/RDR register offsets */
+-#define EIP197_HIA_xDR_OFF(r)			(0x80000 + (r) * 0x1000)
+-#define EIP197_HIA_CDR(r)			(EIP197_HIA_xDR_OFF(r))
+-#define EIP197_HIA_RDR(r)			(EIP197_HIA_xDR_OFF(r) + 0x800)
+-#define EIP197_HIA_xDR_RING_BASE_ADDR_LO	0x0
+-#define EIP197_HIA_xDR_RING_BASE_ADDR_HI	0x4
+-#define EIP197_HIA_xDR_RING_SIZE		0x18
+-#define EIP197_HIA_xDR_DESC_SIZE		0x1c
+-#define EIP197_HIA_xDR_CFG			0x20
+-#define EIP197_HIA_xDR_DMA_CFG			0x24
+-#define EIP197_HIA_xDR_THRESH			0x28
+-#define EIP197_HIA_xDR_PREP_COUNT		0x2c
+-#define EIP197_HIA_xDR_PROC_COUNT		0x30
+-#define EIP197_HIA_xDR_PREP_PNTR		0x34
+-#define EIP197_HIA_xDR_PROC_PNTR		0x38
+-#define EIP197_HIA_xDR_STAT			0x3c
++#define EIP197_HIA_xDR_OFF(priv, r)		(EIP197_HIA_AIC_xDR(priv) + (r) * 0x1000)
++#define EIP197_HIA_CDR(priv, r)			(EIP197_HIA_xDR_OFF(priv, r))
++#define EIP197_HIA_RDR(priv, r)			(EIP197_HIA_xDR_OFF(priv, r) + 0x800)
++#define EIP197_HIA_xDR_RING_BASE_ADDR_LO	0x0000
++#define EIP197_HIA_xDR_RING_BASE_ADDR_HI	0x0004
++#define EIP197_HIA_xDR_RING_SIZE		0x0018
++#define EIP197_HIA_xDR_DESC_SIZE		0x001c
++#define EIP197_HIA_xDR_CFG			0x0020
++#define EIP197_HIA_xDR_DMA_CFG			0x0024
++#define EIP197_HIA_xDR_THRESH			0x0028
++#define EIP197_HIA_xDR_PREP_COUNT		0x002c
++#define EIP197_HIA_xDR_PROC_COUNT		0x0030
++#define EIP197_HIA_xDR_PREP_PNTR		0x0034
++#define EIP197_HIA_xDR_PROC_PNTR		0x0038
++#define EIP197_HIA_xDR_STAT			0x003c
+ 
+ /* register offsets */
+-#define EIP197_HIA_DFE_CFG			0x8c000
+-#define EIP197_HIA_DFE_THR_CTRL			0x8c040
+-#define EIP197_HIA_DFE_THR_STAT			0x8c044
+-#define EIP197_HIA_DSE_CFG			0x8d000
+-#define EIP197_HIA_DSE_THR_CTRL			0x8d040
+-#define EIP197_HIA_DSE_THR_STAT			0x8d044
+-#define EIP197_HIA_RA_PE_CTRL			0x90010
+-#define EIP197_HIA_RA_PE_STAT			0x90014
++#define EIP197_HIA_DFE_CFG			0x0000
++#define EIP197_HIA_DFE_THR_CTRL			0x0000
++#define EIP197_HIA_DFE_THR_STAT			0x0004
++#define EIP197_HIA_DSE_CFG			0x0000
++#define EIP197_HIA_DSE_THR_CTRL			0x0000
++#define EIP197_HIA_DSE_THR_STAT			0x0004
++#define EIP197_HIA_RA_PE_CTRL			0x0010
++#define EIP197_HIA_RA_PE_STAT			0x0014
+ #define EIP197_HIA_AIC_R_OFF(r)			((r) * 0x1000)
+-#define EIP197_HIA_AIC_R_ENABLE_CTRL(r)		(0x9e808 - EIP197_HIA_AIC_R_OFF(r))
+-#define EIP197_HIA_AIC_R_ENABLED_STAT(r)	(0x9e810 - EIP197_HIA_AIC_R_OFF(r))
+-#define EIP197_HIA_AIC_R_ACK(r)			(0x9e810 - EIP197_HIA_AIC_R_OFF(r))
+-#define EIP197_HIA_AIC_R_ENABLE_CLR(r)		(0x9e814 - EIP197_HIA_AIC_R_OFF(r))
+-#define EIP197_HIA_AIC_G_ENABLE_CTRL		0x9f808
+-#define EIP197_HIA_AIC_G_ENABLED_STAT		0x9f810
+-#define EIP197_HIA_AIC_G_ACK			0x9f810
+-#define EIP197_HIA_MST_CTRL			0x9fff4
+-#define EIP197_HIA_OPTIONS			0x9fff8
+-#define EIP197_HIA_VERSION			0x9fffc
+-#define EIP197_PE_IN_DBUF_THRES			0xa0000
+-#define EIP197_PE_IN_TBUF_THRES			0xa0100
+-#define EIP197_PE_ICE_SCRATCH_RAM		0xa0800
+-#define EIP197_PE_ICE_PUE_CTRL			0xa0c80
+-#define EIP197_PE_ICE_SCRATCH_CTRL		0xa0d04
+-#define EIP197_PE_ICE_FPP_CTRL			0xa0d80
+-#define EIP197_PE_ICE_RAM_CTRL			0xa0ff0
+-#define EIP197_PE_EIP96_FUNCTION_EN		0xa1004
+-#define EIP197_PE_EIP96_CONTEXT_CTRL		0xa1008
+-#define EIP197_PE_EIP96_CONTEXT_STAT		0xa100c
+-#define EIP197_PE_OUT_DBUF_THRES		0xa1c00
+-#define EIP197_PE_OUT_TBUF_THRES		0xa1d00
++#define EIP197_HIA_AIC_R_ENABLE_CTRL(r)		(0xe008 - EIP197_HIA_AIC_R_OFF(r))
++#define EIP197_HIA_AIC_R_ENABLED_STAT(r)	(0xe010 - EIP197_HIA_AIC_R_OFF(r))
++#define EIP197_HIA_AIC_R_ACK(r)			(0xe010 - EIP197_HIA_AIC_R_OFF(r))
++#define EIP197_HIA_AIC_R_ENABLE_CLR(r)		(0xe014 - EIP197_HIA_AIC_R_OFF(r))
++#define EIP197_HIA_AIC_G_ENABLE_CTRL		0xf808
++#define EIP197_HIA_AIC_G_ENABLED_STAT		0xf810
++#define EIP197_HIA_AIC_G_ACK			0xf810
++#define EIP197_HIA_MST_CTRL			0xfff4
++#define EIP197_HIA_OPTIONS			0xfff8
++#define EIP197_HIA_VERSION			0xfffc
++#define EIP197_PE_IN_DBUF_THRES			0x0000
++#define EIP197_PE_IN_TBUF_THRES			0x0100
++#define EIP197_PE_ICE_SCRATCH_RAM		0x0800
++#define EIP197_PE_ICE_PUE_CTRL			0x0c80
++#define EIP197_PE_ICE_SCRATCH_CTRL		0x0d04
++#define EIP197_PE_ICE_FPP_CTRL			0x0d80
++#define EIP197_PE_ICE_RAM_CTRL			0x0ff0
++#define EIP197_PE_EIP96_FUNCTION_EN		0x1004
++#define EIP197_PE_EIP96_CONTEXT_CTRL		0x1008
++#define EIP197_PE_EIP96_CONTEXT_STAT		0x100c
++#define EIP197_PE_OUT_DBUF_THRES		0x1c00
++#define EIP197_PE_OUT_TBUF_THRES		0x1d00
++#define EIP197_MST_CTRL				0xfff4
++
++/* EIP197-specific registers, no indirection */
+ #define EIP197_CLASSIFICATION_RAMS		0xe0000
+ #define EIP197_TRC_CTRL				0xf0800
+ #define EIP197_TRC_LASTRES			0xf0804
+@@ -90,7 +129,6 @@
+ #define EIP197_TRC_ECCDATASTAT			0xf083c
+ #define EIP197_TRC_ECCDATA			0xf0840
+ #define EIP197_CS_RAM_CTRL			0xf7ff0
+-#define EIP197_MST_CTRL				0xffff4
+ 
+ /* EIP197_HIA_xDR_DESC_SIZE */
+ #define EIP197_xDR_DESC_MODE_64BIT		BIT(31)
+@@ -465,12 +503,33 @@ struct safexcel_work_data {
+ 	int ring;
+ };
+ 
++enum safexcel_eip_version {
++	EIP97,
++	EIP197,
++};
++
++struct safexcel_register_offsets {
++	u32 hia_aic;
++	u32 hia_aic_g;
++	u32 hia_aic_r;
++	u32 hia_aic_xdr;
++	u32 hia_dfe;
++	u32 hia_dfe_thr;
++	u32 hia_dse;
++	u32 hia_dse_thr;
++	u32 hia_gen_cfg;
++	u32 pe;
++};
++
+ struct safexcel_crypto_priv {
+ 	void __iomem *base;
+ 	struct device *dev;
+ 	struct clk *clk;
+ 	struct safexcel_config config;
+ 
++	enum safexcel_eip_version version;
++	struct safexcel_register_offsets offsets;
++
+ 	/* context DMA pool */
+ 	struct dma_pool *context_pool;
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index 0e5cc230e49a..63a8768ed2ae 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -69,6 +69,7 @@ static int safexcel_aes_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
++	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 
+@@ -78,7 +79,7 @@ static int safexcel_aes_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ 		return ret;
+ 	}
+ 
+-	if (ctx->base.ctxr_dma) {
++	if (priv->version == EIP197 && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < len / sizeof(u32); i++) {
+ 			if (ctx->key[i] != cpu_to_le32(aes.key_enc[i])) {
+ 				ctx->base.needs_inv = true;
+@@ -411,9 +412,13 @@ static int safexcel_send(struct crypto_async_request *async,
+ 			 int *commands, int *results)
+ {
+ 	struct skcipher_request *req = skcipher_request_cast(async);
++	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(req->base.tfm);
+ 	struct safexcel_cipher_req *sreq = skcipher_request_ctx(req);
++	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int ret;
+ 
++	BUG_ON(priv->version == EIP97 && sreq->needs_inv);
++
+ 	if (sreq->needs_inv)
+ 		ret = safexcel_cipher_send_inv(async, ring, request,
+ 					       commands, results);
+@@ -476,7 +481,7 @@ static int safexcel_aes(struct skcipher_request *req,
+ 	ctx->mode = mode;
+ 
+ 	if (ctx->base.ctxr) {
+-		if (ctx->base.needs_inv) {
++		if (priv->version == EIP197 && ctx->base.needs_inv) {
+ 			sreq->needs_inv = true;
+ 			ctx->base.needs_inv = false;
+ 		}
+@@ -544,9 +549,14 @@ static void safexcel_skcipher_cra_exit(struct crypto_tfm *tfm)
+ 
+ 	memzero_explicit(ctx->base.ctxr->data, 8 * sizeof(u32));
+ 
+-	ret = safexcel_cipher_exit_inv(tfm);
+-	if (ret)
+-		dev_warn(priv->dev, "cipher: invalidation error %d\n", ret);
++	if (priv->version == EIP197) {
++		ret = safexcel_cipher_exit_inv(tfm);
++		if (ret)
++			dev_warn(priv->dev, "cipher: invalidation error %d\n", ret);
++	} else {
++		dma_pool_free(priv->context_pool, ctx->base.ctxr,
++			      ctx->base.ctxr_dma);
++	}
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_aes = {
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 9c0cfdaf87f8..33462daa6e1c 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -415,6 +415,8 @@ static int safexcel_handle_result(struct safexcel_crypto_priv *priv, int ring,
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 	int err;
+ 
++	BUG_ON(priv->version == EIP97 && req->needs_inv);
++
+ 	if (req->needs_inv) {
+ 		req->needs_inv = false;
+ 		err = safexcel_handle_inv_result(priv, ring, async,
+@@ -536,7 +538,8 @@ static int safexcel_ahash_enqueue(struct ahash_request *areq)
+ 	req->needs_inv = false;
+ 
+ 	if (ctx->base.ctxr) {
+-		if (!ctx->base.needs_inv && req->processed &&
++		if (priv->version == EIP197 &&
++		    !ctx->base.needs_inv && req->processed &&
+ 		    ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
+ 			/* We're still setting needs_inv here, even though it is
+ 			 * cleared right away, because the needs_inv flag can be
+@@ -729,9 +732,14 @@ static void safexcel_ahash_cra_exit(struct crypto_tfm *tfm)
+ 	if (!ctx->base.ctxr)
+ 		return;
+ 
+-	ret = safexcel_ahash_exit_inv(tfm);
+-	if (ret)
+-		dev_warn(priv->dev, "hash: invalidation error %d\n", ret);
++	if (priv->version == EIP197) {
++		ret = safexcel_ahash_exit_inv(tfm);
++		if (ret)
++			dev_warn(priv->dev, "hash: invalidation error %d\n", ret);
++	} else {
++		dma_pool_free(priv->context_pool, ctx->base.ctxr,
++			      ctx->base.ctxr_dma);
++	}
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha1 = {
+@@ -935,6 +943,7 @@ static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
++	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct safexcel_ahash_export_state istate, ostate;
+ 	int ret, i;
+ 
+@@ -942,7 +951,7 @@ static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 	if (ret)
+ 		return ret;
+ 
+-	if (ctx->base.ctxr) {
++	if (priv->version == EIP197 && ctx->base.ctxr) {
+ 		for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {
+ 			if (ctx->ipad[i] != le32_to_cpu(istate.state[i]) ||
+ 			    ctx->opad[i] != le32_to_cpu(ostate.state[i])) {
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0060-crypto-inside-secure-make-function-safexcel_try_push.patch b/target/linux/mvebu/patches-4.14/0060-crypto-inside-secure-make-function-safexcel_try_push.patch
new file mode 100644
index 0000000..607b82f
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0060-crypto-inside-secure-make-function-safexcel_try_push.patch
@@ -0,0 +1,38 @@
+From 0597efbf3911e2cfce66159872fe91f86416567e Mon Sep 17 00:00:00 2001
+From: Colin Ian King <colin.king@canonical.com>
+Date: Tue, 16 Jan 2018 08:41:58 +0100
+Subject: [PATCH 60/88] crypto: inside-secure - make function
+ safexcel_try_push_requests static
+
+The function safexcel_try_push_requests  is local to the source and does
+not need to be in global scope, so make it static.
+
+Cleans up sparse warning:
+symbol 'safexcel_try_push_requests' was not declared. Should it be static?
+
+Signed-off-by: Colin Ian King <colin.king@canonical.com>
+[Antoine: fixed alignment]
+Signed-off-by: Antoine Tenart <antoine.tenart@free-electrons.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 729b26b50e4f..0dd3a7ac1dd1 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -432,8 +432,8 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ }
+ 
+ /* Called with ring's lock taken */
+-int safexcel_try_push_requests(struct safexcel_crypto_priv *priv, int ring,
+-			       int reqs)
++static int safexcel_try_push_requests(struct safexcel_crypto_priv *priv,
++				      int ring, int reqs)
+ {
+ 	int coal = min_t(int, reqs, EIP197_MAX_BATCH_SZ);
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0061-crypto-inside-secure-do-not-overwrite-the-threshold-.patch b/target/linux/mvebu/patches-4.14/0061-crypto-inside-secure-do-not-overwrite-the-threshold-.patch
new file mode 100644
index 0000000..9a84901
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0061-crypto-inside-secure-do-not-overwrite-the-threshold-.patch
@@ -0,0 +1,40 @@
+From f1698e8577cee8e1fb34a3024d0020c7250285b7 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Tue, 13 Feb 2018 09:26:51 +0100
+Subject: [PATCH 61/88] crypto: inside-secure - do not overwrite the threshold
+ value
+
+This patch fixes the Inside Secure SafeXcel driver not to overwrite the
+interrupt threshold value. In certain cases the value of this register,
+which controls when to fire an interrupt, was overwritten. This lead to
+packet not being processed or acked as the driver never was aware of
+their completion.
+
+This patch fixes this behaviour by not setting the threshold when
+requests are being processed by the engine.
+
+Fixes: dc7e28a3286e ("crypto: inside-secure - dequeue all requests at once")
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 3 +--
+ 1 file changed, 1 insertion(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 0dd3a7ac1dd1..ee96814d8d76 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -514,8 +514,7 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 
+ 	if (!priv->ring[ring].busy) {
+ 		nreq -= safexcel_try_push_requests(priv, ring, nreq);
+-		if (nreq)
+-			priv->ring[ring].busy = true;
++		priv->ring[ring].busy = true;
+ 	}
+ 
+ 	priv->ring[ring].requests_left += nreq;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0064-crypto-inside-secure-do-not-process-request-if-no-co.patch b/target/linux/mvebu/patches-4.14/0064-crypto-inside-secure-do-not-process-request-if-no-co.patch
new file mode 100644
index 0000000..1a05ef2
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0064-crypto-inside-secure-do-not-process-request-if-no-co.patch
@@ -0,0 +1,41 @@
+From 14c6232e1e7e09f15cd95dac18caeb906f0fad1c Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Tue, 13 Feb 2018 09:26:54 +0100
+Subject: [PATCH 64/88] crypto: inside-secure - do not process request if no
+ command was issued
+
+This patch adds a check in the SafeXcel dequeue function, to avoid
+processing request further if no hardware command was issued. This can
+happen in certain cases where the ->send() function caches all the data
+that would have been send.
+
+Fixes: 809778e02cd4 ("crypto: inside-secure - fix hash when length is a multiple of a block")
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 9 +++++++++
+ 1 file changed, 9 insertions(+)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index ee96814d8d76..f4a76971b4ac 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -490,6 +490,15 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 		if (backlog)
+ 			backlog->complete(backlog, -EINPROGRESS);
+ 
++		/* In case the send() helper did not issue any command to push
++		 * to the engine because the input data was cached, continue to
++		 * dequeue other requests as this is valid and not an error.
++		 */
++		if (!commands && !results) {
++			kfree(request);
++			continue;
++		}
++
+ 		spin_lock_bh(&priv->ring[ring].egress_lock);
+ 		list_add_tail(&request->list, &priv->ring[ring].list);
+ 		spin_unlock_bh(&priv->ring[ring].egress_lock);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0066-crypto-inside-secure-keep-the-requests-push-pop-sync.patch b/target/linux/mvebu/patches-4.14/0066-crypto-inside-secure-keep-the-requests-push-pop-sync.patch
new file mode 100644
index 0000000..ceb3419
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0066-crypto-inside-secure-keep-the-requests-push-pop-sync.patch
@@ -0,0 +1,136 @@
+From f504b5ddf871eaeecf69ace3369dc5fd4e8a6a11 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Tue, 13 Feb 2018 09:26:56 +0100
+Subject: [PATCH 66/88] crypto: inside-secure - keep the requests push/pop
+ synced
+
+This patch updates the Inside Secure SafeXcel driver to avoid being
+out-of-sync between the number of requests sent and the one being
+completed.
+
+The number of requests acknowledged by the driver can be different than
+the threshold that was configured if new requests were being pushed to
+the h/w in the meantime. The driver wasn't taking those into account,
+and the number of remaining requests to handled (to reconfigure the
+interrupt threshold) could be out-of sync.
+
+This patch fixes it by not taking in account the number of requests
+left, but by taking in account the total number of requests being sent
+to the hardware, so that new requests are being taken into account.
+
+Fixes: dc7e28a3286e ("crypto: inside-secure - dequeue all requests at once")
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 28 ++++++++++++-------------
+ drivers/crypto/inside-secure/safexcel.h |  6 ++----
+ 2 files changed, 15 insertions(+), 19 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index f4a76971b4ac..fe1f55c3e501 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -432,20 +432,18 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ }
+ 
+ /* Called with ring's lock taken */
+-static int safexcel_try_push_requests(struct safexcel_crypto_priv *priv,
+-				      int ring, int reqs)
++static void safexcel_try_push_requests(struct safexcel_crypto_priv *priv,
++				       int ring)
+ {
+-	int coal = min_t(int, reqs, EIP197_MAX_BATCH_SZ);
++	int coal = min_t(int, priv->ring[ring].requests, EIP197_MAX_BATCH_SZ);
+ 
+ 	if (!coal)
+-		return 0;
++		return;
+ 
+ 	/* Configure when we want an interrupt */
+ 	writel(EIP197_HIA_RDR_THRESH_PKT_MODE |
+ 	       EIP197_HIA_RDR_THRESH_PROC_PKT(coal),
+ 	       EIP197_HIA_RDR(priv, ring) + EIP197_HIA_xDR_THRESH);
+-
+-	return coal;
+ }
+ 
+ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+@@ -521,13 +519,13 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 
+ 	spin_lock_bh(&priv->ring[ring].egress_lock);
+ 
++	priv->ring[ring].requests += nreq;
++
+ 	if (!priv->ring[ring].busy) {
+-		nreq -= safexcel_try_push_requests(priv, ring, nreq);
++		safexcel_try_push_requests(priv, ring);
+ 		priv->ring[ring].busy = true;
+ 	}
+ 
+-	priv->ring[ring].requests_left += nreq;
+-
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 
+ 	/* let the RDR know we have pending descriptors */
+@@ -631,7 +629,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ {
+ 	struct safexcel_request *sreq;
+ 	struct safexcel_context *ctx;
+-	int ret, i, nreq, ndesc, tot_descs, done;
++	int ret, i, nreq, ndesc, tot_descs, handled = 0;
+ 	bool should_complete;
+ 
+ handle_results:
+@@ -667,6 +665,7 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ 
+ 		kfree(sreq);
+ 		tot_descs += ndesc;
++		handled++;
+ 	}
+ 
+ acknowledge:
+@@ -685,11 +684,10 @@ static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv
+ requests_left:
+ 	spin_lock_bh(&priv->ring[ring].egress_lock);
+ 
+-	done = safexcel_try_push_requests(priv, ring,
+-					  priv->ring[ring].requests_left);
++	priv->ring[ring].requests -= handled;
++	safexcel_try_push_requests(priv, ring);
+ 
+-	priv->ring[ring].requests_left -= done;
+-	if (!done && !priv->ring[ring].requests_left)
++	if (!priv->ring[ring].requests)
+ 		priv->ring[ring].busy = false;
+ 
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+@@ -970,7 +968,7 @@ static int safexcel_probe(struct platform_device *pdev)
+ 			goto err_clk;
+ 		}
+ 
+-		priv->ring[i].requests_left = 0;
++		priv->ring[i].requests = 0;
+ 		priv->ring[i].busy = false;
+ 
+ 		crypto_init_queue(&priv->ring[i].queue,
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 4e219c21608b..caaf6a81b162 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -551,10 +551,8 @@ struct safexcel_crypto_priv {
+ 		struct crypto_queue queue;
+ 		spinlock_t queue_lock;
+ 
+-		/* Number of requests in the engine that needs the threshold
+-		 * interrupt to be set up.
+-		 */
+-		int requests_left;
++		/* Number of requests in the engine. */
++		int requests;
+ 
+ 		/* The ring is currently handling at least one request */
+ 		bool busy;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0067-crypto-inside-secure-unmap-the-result-in-the-hash-se.patch b/target/linux/mvebu/patches-4.14/0067-crypto-inside-secure-unmap-the-result-in-the-hash-se.patch
new file mode 100644
index 0000000..1a2a835
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0067-crypto-inside-secure-unmap-the-result-in-the-hash-se.patch
@@ -0,0 +1,42 @@
+From 6470154d81481868a8cb6e5397a34bd479c36978 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Tue, 13 Feb 2018 09:26:57 +0100
+Subject: [PATCH 67/88] crypto: inside-secure - unmap the result in the hash
+ send error path
+
+This patch adds a label to unmap the result buffer in the hash send
+function error path.
+
+Fixes: 1b44c5a60c13 ("crypto: inside-secure - add SafeXcel EIP197 crypto engine driver")
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 4 +++-
+ 1 file changed, 3 insertions(+), 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 791b0931ec6b..51a7c3023bf5 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -303,7 +303,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 				   req->state_sz);
+ 	if (IS_ERR(rdesc)) {
+ 		ret = PTR_ERR(rdesc);
+-		goto cdesc_rollback;
++		goto unmap_result;
+ 	}
+ 
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+@@ -315,6 +315,8 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	*results = 1;
+ 	return 0;
+ 
++unmap_result:
++	dma_unmap_sg(priv->dev, areq->src, req->nents, DMA_TO_DEVICE);
+ cdesc_rollback:
+ 	for (i = 0; i < n_cdesc; i++)
+ 		safexcel_ring_rollback_wptr(priv, &priv->ring[ring].cdr);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0068-crypto-inside-secure-move-hash-result-dma-mapping-to.patch b/target/linux/mvebu/patches-4.14/0068-crypto-inside-secure-move-hash-result-dma-mapping-to.patch
new file mode 100644
index 0000000..0335865
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0068-crypto-inside-secure-move-hash-result-dma-mapping-to.patch
@@ -0,0 +1,115 @@
+From 88820be1c71dc7a5013999e6abca172ea8a486fd Mon Sep 17 00:00:00 2001
+From: Ofer Heifetz <oferh@marvell.com>
+Date: Mon, 26 Feb 2018 14:45:10 +0100
+Subject: [PATCH 68/88] crypto: inside-secure - move hash result dma mapping to
+ request
+
+In heavy traffic the DMA mapping is overwritten by multiple requests as
+the DMA address is stored in a global context. This patch moves this
+information to the per-hash request context so that it can't be
+overwritten.
+
+Fixes: 1b44c5a60c13 ("crypto: inside-secure - add SafeXcel EIP197 crypto engine driver")
+Signed-off-by: Ofer Heifetz <oferh@marvell.com>
+[Antoine: rebased the patch, small fixes, commit message.]
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c      |  7 +------
+ drivers/crypto/inside-secure/safexcel.h      |  4 +---
+ drivers/crypto/inside-secure/safexcel_hash.c | 17 ++++++++++++-----
+ 3 files changed, 14 insertions(+), 14 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index fe1f55c3e501..97cd64041189 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -538,15 +538,10 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ }
+ 
+ void safexcel_free_context(struct safexcel_crypto_priv *priv,
+-			   struct crypto_async_request *req,
+-			   int result_sz)
++			   struct crypto_async_request *req)
+ {
+ 	struct safexcel_context *ctx = crypto_tfm_ctx(req->tfm);
+ 
+-	if (ctx->result_dma)
+-		dma_unmap_single(priv->dev, ctx->result_dma, result_sz,
+-				 DMA_FROM_DEVICE);
+-
+ 	if (ctx->cache) {
+ 		dma_unmap_single(priv->dev, ctx->cache_dma, ctx->cache_sz,
+ 				 DMA_TO_DEVICE);
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index caaf6a81b162..4e14c7e730c4 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -580,7 +580,6 @@ struct safexcel_context {
+ 	bool exit_inv;
+ 
+ 	/* Used for ahash requests */
+-	dma_addr_t result_dma;
+ 	void *cache;
+ 	dma_addr_t cache_dma;
+ 	unsigned int cache_sz;
+@@ -608,8 +607,7 @@ struct safexcel_inv_result {
+ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring);
+ void safexcel_complete(struct safexcel_crypto_priv *priv, int ring);
+ void safexcel_free_context(struct safexcel_crypto_priv *priv,
+-				  struct crypto_async_request *req,
+-				  int result_sz);
++				  struct crypto_async_request *req);
+ int safexcel_invalidate_cache(struct crypto_async_request *async,
+ 			      struct safexcel_crypto_priv *priv,
+ 			      dma_addr_t ctxr_dma, int ring,
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 51a7c3023bf5..abbd8d2b33cb 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -34,6 +34,7 @@ struct safexcel_ahash_req {
+ 	bool needs_inv;
+ 
+ 	int nents;
++	dma_addr_t result_dma;
+ 
+ 	u8 state_sz;    /* expected sate size, only set once */
+ 	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)] __aligned(sizeof(u32));
+@@ -158,7 +159,13 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 		sreq->nents = 0;
+ 	}
+ 
+-	safexcel_free_context(priv, async, sreq->state_sz);
++	if (sreq->result_dma) {
++		dma_unmap_single(priv->dev, sreq->result_dma, sreq->state_sz,
++				 DMA_FROM_DEVICE);
++		sreq->result_dma = 0;
++	}
++
++	safexcel_free_context(priv, async);
+ 
+ 	cache_len = sreq->len - sreq->processed;
+ 	if (cache_len)
+@@ -291,15 +298,15 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	/* Add the token */
+ 	safexcel_hash_token(first_cdesc, len, req->state_sz);
+ 
+-	ctx->base.result_dma = dma_map_single(priv->dev, req->state,
+-					      req->state_sz, DMA_FROM_DEVICE);
+-	if (dma_mapping_error(priv->dev, ctx->base.result_dma)) {
++	req->result_dma = dma_map_single(priv->dev, req->state, req->state_sz,
++					 DMA_FROM_DEVICE);
++	if (dma_mapping_error(priv->dev, req->result_dma)) {
+ 		ret = -EINVAL;
+ 		goto cdesc_rollback;
+ 	}
+ 
+ 	/* Add a result descriptor */
+-	rdesc = safexcel_add_rdesc(priv, ring, 1, 1, ctx->base.result_dma,
++	rdesc = safexcel_add_rdesc(priv, ring, 1, 1, req->result_dma,
+ 				   req->state_sz);
+ 	if (IS_ERR(rdesc)) {
+ 		ret = PTR_ERR(rdesc);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0069-crypto-inside-secure-move-cache-result-dma-mapping-t.patch b/target/linux/mvebu/patches-4.14/0069-crypto-inside-secure-move-cache-result-dma-mapping-t.patch
new file mode 100644
index 0000000..87507dd
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0069-crypto-inside-secure-move-cache-result-dma-mapping-t.patch
@@ -0,0 +1,152 @@
+From 82ab47e5abc47286cc46f24433c4a93770402031 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 26 Feb 2018 14:45:11 +0100
+Subject: [PATCH 69/88] crypto: inside-secure - move cache result dma mapping
+ to request
+
+In heavy traffic the DMA mapping is overwritten by multiple requests as
+the DMA address is stored in a global context. This patch moves this
+information to the per-hash request context so that it can't be
+overwritten.
+
+Fixes: 1b44c5a60c13 ("crypto: inside-secure - add SafeXcel EIP197 crypto engine driver")
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c      | 14 -------
+ drivers/crypto/inside-secure/safexcel.h      |  7 ----
+ drivers/crypto/inside-secure/safexcel_hash.c | 42 +++++++++-----------
+ 3 files changed, 18 insertions(+), 45 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 97cd64041189..09adeaa0da6b 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -537,20 +537,6 @@ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
+ 	       EIP197_HIA_CDR(priv, ring) + EIP197_HIA_xDR_PREP_COUNT);
+ }
+ 
+-void safexcel_free_context(struct safexcel_crypto_priv *priv,
+-			   struct crypto_async_request *req)
+-{
+-	struct safexcel_context *ctx = crypto_tfm_ctx(req->tfm);
+-
+-	if (ctx->cache) {
+-		dma_unmap_single(priv->dev, ctx->cache_dma, ctx->cache_sz,
+-				 DMA_TO_DEVICE);
+-		kfree(ctx->cache);
+-		ctx->cache = NULL;
+-		ctx->cache_sz = 0;
+-	}
+-}
+-
+ void safexcel_complete(struct safexcel_crypto_priv *priv, int ring)
+ {
+ 	struct safexcel_command_desc *cdesc;
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 4e14c7e730c4..d8dff65fc311 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -578,11 +578,6 @@ struct safexcel_context {
+ 	int ring;
+ 	bool needs_inv;
+ 	bool exit_inv;
+-
+-	/* Used for ahash requests */
+-	void *cache;
+-	dma_addr_t cache_dma;
+-	unsigned int cache_sz;
+ };
+ 
+ /*
+@@ -606,8 +601,6 @@ struct safexcel_inv_result {
+ 
+ void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring);
+ void safexcel_complete(struct safexcel_crypto_priv *priv, int ring);
+-void safexcel_free_context(struct safexcel_crypto_priv *priv,
+-				  struct crypto_async_request *req);
+ int safexcel_invalidate_cache(struct crypto_async_request *async,
+ 			      struct safexcel_crypto_priv *priv,
+ 			      dma_addr_t ctxr_dma, int ring,
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index abbd8d2b33cb..cd4596d6f692 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -43,6 +43,9 @@ struct safexcel_ahash_req {
+ 	u64 processed;
+ 
+ 	u8 cache[SHA256_BLOCK_SIZE] __aligned(sizeof(u32));
++	dma_addr_t cache_dma;
++	unsigned int cache_sz;
++
+ 	u8 cache_next[SHA256_BLOCK_SIZE] __aligned(sizeof(u32));
+ };
+ 
+@@ -165,7 +168,11 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 		sreq->result_dma = 0;
+ 	}
+ 
+-	safexcel_free_context(priv, async);
++	if (sreq->cache_dma) {
++		dma_unmap_single(priv->dev, sreq->cache_dma, sreq->cache_sz,
++				 DMA_TO_DEVICE);
++		sreq->cache_dma = 0;
++	}
+ 
+ 	cache_len = sreq->len - sreq->processed;
+ 	if (cache_len)
+@@ -227,24 +234,15 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 
+ 	/* Add a command descriptor for the cached data, if any */
+ 	if (cache_len) {
+-		ctx->base.cache = kzalloc(cache_len, EIP197_GFP_FLAGS(*async));
+-		if (!ctx->base.cache) {
+-			ret = -ENOMEM;
+-			goto unlock;
+-		}
+-		memcpy(ctx->base.cache, req->cache, cache_len);
+-		ctx->base.cache_dma = dma_map_single(priv->dev, ctx->base.cache,
+-						     cache_len, DMA_TO_DEVICE);
+-		if (dma_mapping_error(priv->dev, ctx->base.cache_dma)) {
+-			ret = -EINVAL;
+-			goto free_cache;
+-		}
++		req->cache_dma = dma_map_single(priv->dev, req->cache,
++						cache_len, DMA_TO_DEVICE);
++		if (dma_mapping_error(priv->dev, req->cache_dma))
++			return -EINVAL;
+ 
+-		ctx->base.cache_sz = cache_len;
++		req->cache_sz = cache_len;
+ 		first_cdesc = safexcel_add_cdesc(priv, ring, 1,
+ 						 (cache_len == len),
+-						 ctx->base.cache_dma,
+-						 cache_len, len,
++						 req->cache_dma, cache_len, len,
+ 						 ctx->base.ctxr_dma);
+ 		if (IS_ERR(first_cdesc)) {
+ 			ret = PTR_ERR(first_cdesc);
+@@ -328,16 +326,12 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	for (i = 0; i < n_cdesc; i++)
+ 		safexcel_ring_rollback_wptr(priv, &priv->ring[ring].cdr);
+ unmap_cache:
+-	if (ctx->base.cache_dma) {
+-		dma_unmap_single(priv->dev, ctx->base.cache_dma,
+-				 ctx->base.cache_sz, DMA_TO_DEVICE);
+-		ctx->base.cache_sz = 0;
++	if (req->cache_dma) {
++		dma_unmap_single(priv->dev, req->cache_dma, req->cache_sz,
++				 DMA_TO_DEVICE);
++		req->cache_sz = 0;
+ 	}
+-free_cache:
+-	kfree(ctx->base.cache);
+-	ctx->base.cache = NULL;
+ 
+-unlock:
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 	return ret;
+ }
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0071-crypto-inside-secure-fix-missing-unlock-on-error-in-.patch b/target/linux/mvebu/patches-4.14/0071-crypto-inside-secure-fix-missing-unlock-on-error-in-.patch
new file mode 100644
index 0000000..cae6eb3
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0071-crypto-inside-secure-fix-missing-unlock-on-error-in-.patch
@@ -0,0 +1,36 @@
+From 4790830eabca40c75ea20c6c1a39aa16b348e933 Mon Sep 17 00:00:00 2001
+From: "weiyongjun \\(A\\)" <weiyongjun1@huawei.com>
+Date: Tue, 13 Mar 2018 14:54:03 +0000
+Subject: [PATCH 71/88] crypto: inside-secure - fix missing unlock on error in
+ safexcel_ahash_send_req()
+
+Add the missing unlock before return from function
+safexcel_ahash_send_req() in the error handling case.
+
+Fixes: cff9a17545a3 ("crypto: inside-secure - move cache result dma mapping to request")
+Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
+Acked-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 4 +++-
+ 1 file changed, 3 insertions(+), 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 9703a4063cfc..a7702da92b02 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -236,8 +236,10 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	if (cache_len) {
+ 		req->cache_dma = dma_map_single(priv->dev, req->cache,
+ 						cache_len, DMA_TO_DEVICE);
+-		if (dma_mapping_error(priv->dev, req->cache_dma))
++		if (dma_mapping_error(priv->dev, req->cache_dma)) {
++			spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 			return -EINVAL;
++		}
+ 
+ 		req->cache_sz = cache_len;
+ 		first_cdesc = safexcel_add_cdesc(priv, ring, 1,
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0072-crypto-inside-secure-improve-clock-initialization.patch b/target/linux/mvebu/patches-4.14/0072-crypto-inside-secure-improve-clock-initialization.patch
new file mode 100644
index 0000000..f5e48ec
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0072-crypto-inside-secure-improve-clock-initialization.patch
@@ -0,0 +1,48 @@
+From 878948e37c288f2f32033c01447b79a477694bf2 Mon Sep 17 00:00:00 2001
+From: Gregory CLEMENT <gregory.clement@bootlin.com>
+Date: Tue, 13 Mar 2018 17:48:41 +0100
+Subject: [PATCH 72/88] crypto: inside-secure - improve clock initialization
+
+The clock is optional, but if it is present we should managed it. If
+there is an error while trying getting it, we should exit and report this
+error.
+
+So instead of returning an error only in the -EPROBE case, turn it in an
+other way and ignore the clock only if it is not present (-ENOENT case).
+
+Signed-off-by: Gregory CLEMENT <gregory.clement@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 11 ++++++-----
+ 1 file changed, 6 insertions(+), 5 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 09adeaa0da6b..cbcb5d9f17bd 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -882,16 +882,17 @@ static int safexcel_probe(struct platform_device *pdev)
+ 	}
+ 
+ 	priv->clk = devm_clk_get(&pdev->dev, NULL);
+-	if (!IS_ERR(priv->clk)) {
++	ret = PTR_ERR_OR_ZERO(priv->clk);
++	/* The clock isn't mandatory */
++	if  (ret != -ENOENT) {
++		if (ret)
++			return ret;
++
+ 		ret = clk_prepare_enable(priv->clk);
+ 		if (ret) {
+ 			dev_err(dev, "unable to enable clk (%d)\n", ret);
+ 			return ret;
+ 		}
+-	} else {
+-		/* The clock isn't mandatory */
+-		if (PTR_ERR(priv->clk) == -EPROBE_DEFER)
+-			return -EPROBE_DEFER;
+ 	}
+ 
+ 	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0073-crypto-inside-secure-fix-clock-resource-by-adding-a-.patch b/target/linux/mvebu/patches-4.14/0073-crypto-inside-secure-fix-clock-resource-by-adding-a-.patch
new file mode 100644
index 0000000..277163a
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0073-crypto-inside-secure-fix-clock-resource-by-adding-a-.patch
@@ -0,0 +1,146 @@
+From f59eb3a0f934940e48823ae7edf2823ee1820829 Mon Sep 17 00:00:00 2001
+From: Gregory CLEMENT <gregory.clement@bootlin.com>
+Date: Tue, 13 Mar 2018 17:48:42 +0100
+Subject: [PATCH 73/88] crypto: inside-secure - fix clock resource by adding a
+ register clock
+
+On Armada 7K/8K we need to explicitly enable the register clock. This
+clock is optional because not all the SoCs using this IP need it but at
+least for Armada 7K/8K it is actually mandatory.
+
+The binding documentation is updated accordingly.
+
+Signed-off-by: Gregory CLEMENT <gregory.clement@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ .../crypto/inside-secure-safexcel.txt         |  6 +++-
+ drivers/crypto/inside-secure/safexcel.c       | 34 ++++++++++++++-----
+ drivers/crypto/inside-secure/safexcel.h       |  1 +
+ 3 files changed, 31 insertions(+), 10 deletions(-)
+
+diff --git a/Documentation/devicetree/bindings/crypto/inside-secure-safexcel.txt b/Documentation/devicetree/bindings/crypto/inside-secure-safexcel.txt
+index fbc07d12322f..5a3d0829ddf2 100644
+--- a/Documentation/devicetree/bindings/crypto/inside-secure-safexcel.txt
++++ b/Documentation/devicetree/bindings/crypto/inside-secure-safexcel.txt
+@@ -7,7 +7,11 @@ Required properties:
+ - interrupt-names: Should be "ring0", "ring1", "ring2", "ring3", "eip", "mem".
+ 
+ Optional properties:
+-- clocks: Reference to the crypto engine clock.
++- clocks: Reference to the crypto engine clocks, the second clock is
++          needed for the Armada 7K/8K SoCs.
++- clock-names: mandatory if there is a second clock, in this case the
++               name must be "core" for the first clock and "reg" for
++               the second one.
+ 
+ Example:
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index cbcb5d9f17bd..2f68b4ed5500 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -895,16 +895,30 @@ static int safexcel_probe(struct platform_device *pdev)
+ 		}
+ 	}
+ 
++	priv->reg_clk = devm_clk_get(&pdev->dev, "reg");
++	ret = PTR_ERR_OR_ZERO(priv->reg_clk);
++	/* The clock isn't mandatory */
++	if  (ret != -ENOENT) {
++		if (ret)
++			goto err_core_clk;
++
++		ret = clk_prepare_enable(priv->reg_clk);
++		if (ret) {
++			dev_err(dev, "unable to enable reg clk (%d)\n", ret);
++			goto err_core_clk;
++		}
++	}
++
+ 	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+ 	if (ret)
+-		goto err_clk;
++		goto err_reg_clk;
+ 
+ 	priv->context_pool = dmam_pool_create("safexcel-context", dev,
+ 					      sizeof(struct safexcel_context_record),
+ 					      1, 0);
+ 	if (!priv->context_pool) {
+ 		ret = -ENOMEM;
+-		goto err_clk;
++		goto err_reg_clk;
+ 	}
+ 
+ 	safexcel_configure(priv);
+@@ -919,12 +933,12 @@ static int safexcel_probe(struct platform_device *pdev)
+ 						     &priv->ring[i].cdr,
+ 						     &priv->ring[i].rdr);
+ 		if (ret)
+-			goto err_clk;
++			goto err_reg_clk;
+ 
+ 		ring_irq = devm_kzalloc(dev, sizeof(*ring_irq), GFP_KERNEL);
+ 		if (!ring_irq) {
+ 			ret = -ENOMEM;
+-			goto err_clk;
++			goto err_reg_clk;
+ 		}
+ 
+ 		ring_irq->priv = priv;
+@@ -936,7 +950,7 @@ static int safexcel_probe(struct platform_device *pdev)
+ 						ring_irq);
+ 		if (irq < 0) {
+ 			ret = irq;
+-			goto err_clk;
++			goto err_reg_clk;
+ 		}
+ 
+ 		priv->ring[i].work_data.priv = priv;
+@@ -947,7 +961,7 @@ static int safexcel_probe(struct platform_device *pdev)
+ 		priv->ring[i].workqueue = create_singlethread_workqueue(wq_name);
+ 		if (!priv->ring[i].workqueue) {
+ 			ret = -ENOMEM;
+-			goto err_clk;
++			goto err_reg_clk;
+ 		}
+ 
+ 		priv->ring[i].requests = 0;
+@@ -968,18 +982,20 @@ static int safexcel_probe(struct platform_device *pdev)
+ 	ret = safexcel_hw_init(priv);
+ 	if (ret) {
+ 		dev_err(dev, "EIP h/w init failed (%d)\n", ret);
+-		goto err_clk;
++		goto err_reg_clk;
+ 	}
+ 
+ 	ret = safexcel_register_algorithms(priv);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to register algorithms (%d)\n", ret);
+-		goto err_clk;
++		goto err_reg_clk;
+ 	}
+ 
+ 	return 0;
+ 
+-err_clk:
++err_reg_clk:
++	clk_disable_unprepare(priv->reg_clk);
++err_core_clk:
+ 	clk_disable_unprepare(priv->clk);
+ 	return ret;
+ }
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index d8dff65fc311..4efeb0251daf 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -525,6 +525,7 @@ struct safexcel_crypto_priv {
+ 	void __iomem *base;
+ 	struct device *dev;
+ 	struct clk *clk;
++	struct clk *reg_clk;
+ 	struct safexcel_config config;
+ 
+ 	enum safexcel_eip_version version;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0074-crypto-inside-secure-move-the-digest-to-the-request-.patch b/target/linux/mvebu/patches-4.14/0074-crypto-inside-secure-move-the-digest-to-the-request-.patch
new file mode 100644
index 0000000..c02a9d3
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0074-crypto-inside-secure-move-the-digest-to-the-request-.patch
@@ -0,0 +1,161 @@
+From 0284c282179665d126a35fb88f21487edb72300c Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:13 +0100
+Subject: [PATCH 74/88] crypto: inside-secure - move the digest to the request
+ context
+
+This patches moves the digest information from the transformation
+context to the request context. This fixes cases where HMAC init
+functions were called and override the digest value for a short period
+of time, as the HMAC init functions call the SHA init one which reset
+the value. This lead to a small percentage of HMAC being incorrectly
+computed under heavy load.
+
+Fixes: 1b44c5a60c13 ("crypto: inside-secure - add SafeXcel EIP197 crypto engine driver")
+Suggested-by: Ofer Heifetz <oferh@marvell.com>
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+[Ofer here did all the work, from seeing the issue to understanding the
+root cause. I only made the patch.]
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 30 ++++++++++++--------
+ 1 file changed, 18 insertions(+), 12 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index a7702da92b02..cfcae5e51b9d 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -21,7 +21,6 @@ struct safexcel_ahash_ctx {
+ 	struct safexcel_crypto_priv *priv;
+ 
+ 	u32 alg;
+-	u32 digest;
+ 
+ 	u32 ipad[SHA1_DIGEST_SIZE / sizeof(u32)];
+ 	u32 opad[SHA1_DIGEST_SIZE / sizeof(u32)];
+@@ -36,6 +35,8 @@ struct safexcel_ahash_req {
+ 	int nents;
+ 	dma_addr_t result_dma;
+ 
++	u32 digest;
++
+ 	u8 state_sz;    /* expected sate size, only set once */
+ 	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)] __aligned(sizeof(u32));
+ 
+@@ -53,6 +54,8 @@ struct safexcel_ahash_export_state {
+ 	u64 len;
+ 	u64 processed;
+ 
++	u32 digest;
++
+ 	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)];
+ 	u8 cache[SHA256_BLOCK_SIZE];
+ };
+@@ -86,9 +89,9 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
+ 
+ 	cdesc->control_data.control0 |= CONTEXT_CONTROL_TYPE_HASH_OUT;
+ 	cdesc->control_data.control0 |= ctx->alg;
+-	cdesc->control_data.control0 |= ctx->digest;
++	cdesc->control_data.control0 |= req->digest;
+ 
+-	if (ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED) {
++	if (req->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED) {
+ 		if (req->processed) {
+ 			if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA1)
+ 				cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(6);
+@@ -116,7 +119,7 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
+ 			if (req->finish)
+ 				ctx->base.ctxr->data[i] = cpu_to_le32(req->processed / blocksize);
+ 		}
+-	} else if (ctx->digest == CONTEXT_CONTROL_DIGEST_HMAC) {
++	} else if (req->digest == CONTEXT_CONTROL_DIGEST_HMAC) {
+ 		cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(10);
+ 
+ 		memcpy(ctx->base.ctxr->data, ctx->ipad, digestsize);
+@@ -545,7 +548,7 @@ static int safexcel_ahash_enqueue(struct ahash_request *areq)
+ 	if (ctx->base.ctxr) {
+ 		if (priv->version == EIP197 &&
+ 		    !ctx->base.needs_inv && req->processed &&
+-		    ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
++		    req->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
+ 			/* We're still setting needs_inv here, even though it is
+ 			 * cleared right away, because the needs_inv flag can be
+ 			 * set in other functions and we want to keep the same
+@@ -580,7 +583,6 @@ static int safexcel_ahash_enqueue(struct ahash_request *areq)
+ 
+ static int safexcel_ahash_update(struct ahash_request *areq)
+ {
+-	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
+ 
+@@ -596,7 +598,7 @@ static int safexcel_ahash_update(struct ahash_request *areq)
+ 	 * We're not doing partial updates when performing an hmac request.
+ 	 * Everything will be handled by the final() call.
+ 	 */
+-	if (ctx->digest == CONTEXT_CONTROL_DIGEST_HMAC)
++	if (req->digest == CONTEXT_CONTROL_DIGEST_HMAC)
+ 		return 0;
+ 
+ 	if (req->hmac)
+@@ -655,6 +657,8 @@ static int safexcel_ahash_export(struct ahash_request *areq, void *out)
+ 	export->len = req->len;
+ 	export->processed = req->processed;
+ 
++	export->digest = req->digest;
++
+ 	memcpy(export->state, req->state, req->state_sz);
+ 	memcpy(export->cache, req->cache, crypto_ahash_blocksize(ahash));
+ 
+@@ -675,6 +679,8 @@ static int safexcel_ahash_import(struct ahash_request *areq, const void *in)
+ 	req->len = export->len;
+ 	req->processed = export->processed;
+ 
++	req->digest = export->digest;
++
+ 	memcpy(req->cache, export->cache, crypto_ahash_blocksize(ahash));
+ 	memcpy(req->state, export->state, req->state_sz);
+ 
+@@ -711,7 +717,7 @@ static int safexcel_sha1_init(struct ahash_request *areq)
+ 	req->state[4] = SHA1_H4;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA1;
+-	ctx->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
++	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA1_DIGEST_SIZE;
+ 
+ 	return 0;
+@@ -778,10 +784,10 @@ struct safexcel_alg_template safexcel_alg_sha1 = {
+ 
+ static int safexcel_hmac_sha1_init(struct ahash_request *areq)
+ {
+-	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
++	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	safexcel_sha1_init(areq);
+-	ctx->digest = CONTEXT_CONTROL_DIGEST_HMAC;
++	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	return 0;
+ }
+ 
+@@ -1019,7 +1025,7 @@ static int safexcel_sha256_init(struct ahash_request *areq)
+ 	req->state[7] = SHA256_H7;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA256;
+-	ctx->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
++	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA256_DIGEST_SIZE;
+ 
+ 	return 0;
+@@ -1081,7 +1087,7 @@ static int safexcel_sha224_init(struct ahash_request *areq)
+ 	req->state[7] = SHA224_H7;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA224;
+-	ctx->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
++	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA256_DIGEST_SIZE;
+ 
+ 	return 0;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0075-crypto-inside-secure-fix-typo-s-allways-always-in-a-.patch b/target/linux/mvebu/patches-4.14/0075-crypto-inside-secure-fix-typo-s-allways-always-in-a-.patch
new file mode 100644
index 0000000..d2a50ae
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0075-crypto-inside-secure-fix-typo-s-allways-always-in-a-.patch
@@ -0,0 +1,45 @@
+From 8202bb062ed5bad674045d83ae1d908cf5230070 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:14 +0100
+Subject: [PATCH 75/88] crypto: inside-secure - fix typo s/allways/always/ in a
+ define
+
+Small cosmetic patch fixing one typo in the
+EIP197_HIA_DSE_CFG_ALLWAYS_BUFFERABLE macro, it should be _ALWAYS_.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 2 +-
+ drivers/crypto/inside-secure/safexcel.h | 2 +-
+ 2 files changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 2f68b4ed5500..cc9d2e9126b4 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -332,7 +332,7 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	val = EIP197_HIA_DSE_CFG_DIS_DEBUG;
+ 	val |= EIP197_HIA_DxE_CFG_MIN_DATA_SIZE(7) | EIP197_HIA_DxE_CFG_MAX_DATA_SIZE(8);
+ 	val |= EIP197_HIA_DxE_CFG_DATA_CACHE_CTRL(WR_CACHE_3BITS);
+-	val |= EIP197_HIA_DSE_CFG_ALLWAYS_BUFFERABLE;
++	val |= EIP197_HIA_DSE_CFG_ALWAYS_BUFFERABLE;
+ 	/* FIXME: instability issues can occur for EIP97 but disabling it impact
+ 	 * performances.
+ 	 */
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 4efeb0251daf..9ca1654136e0 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -179,7 +179,7 @@
+ #define EIP197_HIA_DxE_CFG_MIN_DATA_SIZE(n)	((n) << 0)
+ #define EIP197_HIA_DxE_CFG_DATA_CACHE_CTRL(n)	(((n) & 0x7) << 4)
+ #define EIP197_HIA_DxE_CFG_MAX_DATA_SIZE(n)	((n) << 8)
+-#define EIP197_HIA_DSE_CFG_ALLWAYS_BUFFERABLE	GENMASK(15, 14)
++#define EIP197_HIA_DSE_CFG_ALWAYS_BUFFERABLE	GENMASK(15, 14)
+ #define EIP197_HIA_DxE_CFG_MIN_CTRL_SIZE(n)	((n) << 16)
+ #define EIP197_HIA_DxE_CFG_CTRL_CACHE_CTRL(n)	(((n) & 0x7) << 20)
+ #define EIP197_HIA_DxE_CFG_MAX_CTRL_SIZE(n)	((n) << 24)
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0076-crypto-inside-secure-fix-a-typo-in-a-register-name.patch b/target/linux/mvebu/patches-4.14/0076-crypto-inside-secure-fix-a-typo-in-a-register-name.patch
new file mode 100644
index 0000000..dc001a1
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0076-crypto-inside-secure-fix-a-typo-in-a-register-name.patch
@@ -0,0 +1,45 @@
+From 2350dcc417c6ea36ebdc0e38437e287da9c6c21b Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:15 +0100
+Subject: [PATCH 76/88] crypto: inside-secure - fix a typo in a register name
+
+This patch fixes a typo in the EIP197_HIA_xDR_WR_CTRL_BUG register name,
+as it should be EIP197_HIA_xDR_WR_CTRL_BUF. This is a cosmetic only
+change.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c | 2 +-
+ drivers/crypto/inside-secure/safexcel.h | 2 +-
+ 2 files changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index cc9d2e9126b4..f7d7293de699 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -235,7 +235,7 @@ static int safexcel_hw_setup_rdesc_rings(struct safexcel_crypto_priv *priv)
+ 		/* Configure DMA tx control */
+ 		val = EIP197_HIA_xDR_CFG_WR_CACHE(WR_CACHE_3BITS);
+ 		val |= EIP197_HIA_xDR_CFG_RD_CACHE(RD_CACHE_3BITS);
+-		val |= EIP197_HIA_xDR_WR_RES_BUF | EIP197_HIA_xDR_WR_CTRL_BUG;
++		val |= EIP197_HIA_xDR_WR_RES_BUF | EIP197_HIA_xDR_WR_CTRL_BUF;
+ 		writel(val,
+ 		       EIP197_HIA_RDR(priv, i) + EIP197_HIA_xDR_DMA_CFG);
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 9ca1654136e0..295813920618 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -135,7 +135,7 @@
+ 
+ /* EIP197_HIA_xDR_DMA_CFG */
+ #define EIP197_HIA_xDR_WR_RES_BUF		BIT(22)
+-#define EIP197_HIA_xDR_WR_CTRL_BUG		BIT(23)
++#define EIP197_HIA_xDR_WR_CTRL_BUF		BIT(23)
+ #define EIP197_HIA_xDR_WR_OWN_BUF		BIT(24)
+ #define EIP197_HIA_xDR_CFG_WR_CACHE(n)		(((n) & 0x7) << 25)
+ #define EIP197_HIA_xDR_CFG_RD_CACHE(n)		(((n) & 0x7) << 29)
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0077-crypto-inside-secure-improve-the-send-error-path.patch b/target/linux/mvebu/patches-4.14/0077-crypto-inside-secure-improve-the-send-error-path.patch
new file mode 100644
index 0000000..4b06d67
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0077-crypto-inside-secure-improve-the-send-error-path.patch
@@ -0,0 +1,50 @@
+From 54d132b860a7fcd9e2a0e5cf568c19bdb5f6c79d Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:16 +0100
+Subject: [PATCH 77/88] crypto: inside-secure - improve the send error path
+
+This patch improves the send error path as it wasn't handling all error
+cases. A new label is added, and some of the goto are updated to point
+to the right labels, so that the code is more robust to errors.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 7 +++++--
+ 1 file changed, 5 insertions(+), 2 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index cfcae5e51b9d..4ff3f7615b3d 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -281,7 +281,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 					   sglen, len, ctx->base.ctxr_dma);
+ 		if (IS_ERR(cdesc)) {
+ 			ret = PTR_ERR(cdesc);
+-			goto cdesc_rollback;
++			goto unmap_sg;
+ 		}
+ 		n_cdesc++;
+ 
+@@ -305,7 +305,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 					 DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(priv->dev, req->result_dma)) {
+ 		ret = -EINVAL;
+-		goto cdesc_rollback;
++		goto unmap_sg;
+ 	}
+ 
+ 	/* Add a result descriptor */
+@@ -326,6 +326,9 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
+ 	return 0;
+ 
+ unmap_result:
++	dma_unmap_single(priv->dev, req->result_dma, req->state_sz,
++			 DMA_FROM_DEVICE);
++unmap_sg:
+ 	dma_unmap_sg(priv->dev, areq->src, req->nents, DMA_TO_DEVICE);
+ cdesc_rollback:
+ 	for (i = 0; i < n_cdesc; i++)
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0078-crypto-inside-secure-do-not-access-buffers-mapped-to.patch b/target/linux/mvebu/patches-4.14/0078-crypto-inside-secure-do-not-access-buffers-mapped-to.patch
new file mode 100644
index 0000000..9f6d197
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0078-crypto-inside-secure-do-not-access-buffers-mapped-to.patch
@@ -0,0 +1,46 @@
+From 562d28bc8d7ae385ecf292f9209b2bfab68c1a09 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:17 +0100
+Subject: [PATCH 78/88] crypto: inside-secure - do not access buffers mapped to
+ the device
+
+This patches update the way the digest is copied from the state buffer
+to the result buffer, so that the copy only happen after the state
+buffer was DMA unmapped, as otherwise the buffer would be owned by the
+device.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 8 ++++----
+ 1 file changed, 4 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 4ff3f7615b3d..573b12e4d9dd 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -156,10 +156,6 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 	safexcel_complete(priv, ring);
+ 	spin_unlock_bh(&priv->ring[ring].egress_lock);
+ 
+-	if (sreq->finish)
+-		memcpy(areq->result, sreq->state,
+-		       crypto_ahash_digestsize(ahash));
+-
+ 	if (sreq->nents) {
+ 		dma_unmap_sg(priv->dev, areq->src, sreq->nents, DMA_TO_DEVICE);
+ 		sreq->nents = 0;
+@@ -177,6 +173,10 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
+ 		sreq->cache_dma = 0;
+ 	}
+ 
++	if (sreq->finish)
++		memcpy(areq->result, sreq->state,
++		       crypto_ahash_digestsize(ahash));
++
+ 	cache_len = sreq->len - sreq->processed;
+ 	if (cache_len)
+ 		memcpy(sreq->cache, sreq->cache_next, cache_len);
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0079-crypto-inside-secure-improve-the-skcipher-token.patch b/target/linux/mvebu/patches-4.14/0079-crypto-inside-secure-improve-the-skcipher-token.patch
new file mode 100644
index 0000000..ca480f0
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0079-crypto-inside-secure-improve-the-skcipher-token.patch
@@ -0,0 +1,36 @@
+From afec5524c4dea4d954afb568a3b769f1fb88d71f Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:18 +0100
+Subject: [PATCH 79/88] crypto: inside-secure - improve the skcipher token
+
+The token used for encryption and decryption of skcipher algorithms sets
+its stat field to "last packet". As it's a cipher only algorithm, there
+is not hash operation and thus the "last hash" bit should be set to tell
+the internal engine no hash operation should be performed.
+
+This does not fix a bug, but improves the token definition to follow
+exactly what's advised by the datasheet.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_cipher.c | 3 ++-
+ 1 file changed, 2 insertions(+), 1 deletion(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_cipher.c b/drivers/crypto/inside-secure/safexcel_cipher.c
+index 17a7725a6f6d..bafb60505fab 100644
+--- a/drivers/crypto/inside-secure/safexcel_cipher.c
++++ b/drivers/crypto/inside-secure/safexcel_cipher.c
+@@ -58,7 +58,8 @@ static void safexcel_cipher_token(struct safexcel_cipher_ctx *ctx,
+ 
+ 	token[0].opcode = EIP197_TOKEN_OPCODE_DIRECTION;
+ 	token[0].packet_length = length;
+-	token[0].stat = EIP197_TOKEN_STAT_LAST_PACKET;
++	token[0].stat = EIP197_TOKEN_STAT_LAST_PACKET |
++			EIP197_TOKEN_STAT_LAST_HASH;
+ 	token[0].instructions = EIP197_TOKEN_INS_LAST |
+ 				EIP197_TOKEN_INS_TYPE_CRYTO |
+ 				EIP197_TOKEN_INS_TYPE_OUTPUT;
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0080-crypto-inside-secure-the-context-ipad-opad-should-us.patch b/target/linux/mvebu/patches-4.14/0080-crypto-inside-secure-the-context-ipad-opad-should-us.patch
new file mode 100644
index 0000000..277caa8
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0080-crypto-inside-secure-the-context-ipad-opad-should-us.patch
@@ -0,0 +1,42 @@
+From e3a08bdcd1b3f38d683d752f0263271ce5eada4a Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:19 +0100
+Subject: [PATCH 80/88] crypto: inside-secure - the context ipad/opad should
+ use the state sz
+
+This patches uses the state size of the algorithms instead of their
+digest size to copy the ipad and opad in the context. This doesn't fix
+anything as the state and digest size are the same for many algorithms,
+and for all the hmac currently supported by this driver. However
+hmac(sha224) use the sha224 hash function which has a different digest
+and state size. This commit prepares the addition of such algorithms.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel_hash.c | 8 ++++----
+ 1 file changed, 4 insertions(+), 4 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 573b12e4d9dd..d152f2eb0271 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -120,11 +120,11 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
+ 				ctx->base.ctxr->data[i] = cpu_to_le32(req->processed / blocksize);
+ 		}
+ 	} else if (req->digest == CONTEXT_CONTROL_DIGEST_HMAC) {
+-		cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(10);
++		cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(2 * req->state_sz / sizeof(u32));
+ 
+-		memcpy(ctx->base.ctxr->data, ctx->ipad, digestsize);
+-		memcpy(ctx->base.ctxr->data + digestsize / sizeof(u32),
+-		       ctx->opad, digestsize);
++		memcpy(ctx->base.ctxr->data, ctx->ipad, req->state_sz);
++		memcpy(ctx->base.ctxr->data + req->state_sz / sizeof(u32),
++		       ctx->opad, req->state_sz);
+ 	}
+ }
+ 
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0081-crypto-inside-secure-hmac-sha256-support.patch b/target/linux/mvebu/patches-4.14/0081-crypto-inside-secure-hmac-sha256-support.patch
new file mode 100644
index 0000000..fb04b87
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0081-crypto-inside-secure-hmac-sha256-support.patch
@@ -0,0 +1,174 @@
+From b9a928b95de3f663fe37cdf1aca00d7b3d59a3fb Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:20 +0100
+Subject: [PATCH 81/88] crypto: inside-secure - hmac(sha256) support
+
+This patch adds the hmac(sha256) support to the Inside Secure
+cryptographic engine driver.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c      |  3 +-
+ drivers/crypto/inside-secure/safexcel.h      |  1 +
+ drivers/crypto/inside-secure/safexcel_hash.c | 80 ++++++++++++++++++--
+ 3 files changed, 75 insertions(+), 9 deletions(-)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index f7d7293de699..33595f41586f 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -354,7 +354,7 @@ static int safexcel_hw_init(struct safexcel_crypto_priv *priv)
+ 	val |= EIP197_PROTOCOL_ENCRYPT_ONLY | EIP197_PROTOCOL_HASH_ONLY;
+ 	val |= EIP197_ALG_AES_ECB | EIP197_ALG_AES_CBC;
+ 	val |= EIP197_ALG_SHA1 | EIP197_ALG_HMAC_SHA1;
+-	val |= EIP197_ALG_SHA2;
++	val |= EIP197_ALG_SHA2 | EIP197_ALG_HMAC_SHA2;
+ 	writel(val, EIP197_PE(priv) + EIP197_PE_EIP96_FUNCTION_EN);
+ 
+ 	/* Command Descriptor Rings prepare */
+@@ -768,6 +768,7 @@ static struct safexcel_alg_template *safexcel_algs[] = {
+ 	&safexcel_alg_sha224,
+ 	&safexcel_alg_sha256,
+ 	&safexcel_alg_hmac_sha1,
++	&safexcel_alg_hmac_sha256,
+ };
+ 
+ static int safexcel_register_algorithms(struct safexcel_crypto_priv *priv)
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 295813920618..99e0f32452ff 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -633,5 +633,6 @@ extern struct safexcel_alg_template safexcel_alg_sha1;
+ extern struct safexcel_alg_template safexcel_alg_sha224;
+ extern struct safexcel_alg_template safexcel_alg_sha256;
+ extern struct safexcel_alg_template safexcel_alg_hmac_sha1;
++extern struct safexcel_alg_template safexcel_alg_hmac_sha256;
+ 
+ #endif
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index d152f2eb0271..2917a902596d 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -22,8 +22,8 @@ struct safexcel_ahash_ctx {
+ 
+ 	u32 alg;
+ 
+-	u32 ipad[SHA1_DIGEST_SIZE / sizeof(u32)];
+-	u32 opad[SHA1_DIGEST_SIZE / sizeof(u32)];
++	u32 ipad[SHA256_DIGEST_SIZE / sizeof(u32)];
++	u32 opad[SHA256_DIGEST_SIZE / sizeof(u32)];
+ };
+ 
+ struct safexcel_ahash_req {
+@@ -953,20 +953,21 @@ static int safexcel_hmac_setkey(const char *alg, const u8 *key,
+ 	return ret;
+ }
+ 
+-static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+-				     unsigned int keylen)
++static int safexcel_hmac_alg_setkey(struct crypto_ahash *tfm, const u8 *key,
++				    unsigned int keylen, const char *alg,
++				    unsigned int state_sz)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct safexcel_ahash_export_state istate, ostate;
+ 	int ret, i;
+ 
+-	ret = safexcel_hmac_setkey("safexcel-sha1", key, keylen, &istate, &ostate);
++	ret = safexcel_hmac_setkey(alg, key, keylen, &istate, &ostate);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->version == EIP197 && ctx->base.ctxr) {
+-		for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {
++		for (i = 0; i < state_sz / sizeof(u32); i++) {
+ 			if (ctx->ipad[i] != le32_to_cpu(istate.state[i]) ||
+ 			    ctx->opad[i] != le32_to_cpu(ostate.state[i])) {
+ 				ctx->base.needs_inv = true;
+@@ -975,12 +976,19 @@ static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 		}
+ 	}
+ 
+-	memcpy(ctx->ipad, &istate.state, SHA1_DIGEST_SIZE);
+-	memcpy(ctx->opad, &ostate.state, SHA1_DIGEST_SIZE);
++	memcpy(ctx->ipad, &istate.state, state_sz);
++	memcpy(ctx->opad, &ostate.state, state_sz);
+ 
+ 	return 0;
+ }
+ 
++static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
++				     unsigned int keylen)
++{
++	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha1",
++					SHA1_DIGEST_SIZE);
++}
++
+ struct safexcel_alg_template safexcel_alg_hmac_sha1 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.alg.ahash = {
+@@ -1134,3 +1142,59 @@ struct safexcel_alg_template safexcel_alg_sha224 = {
+ 		},
+ 	},
+ };
++
++static int safexcel_hmac_sha256_setkey(struct crypto_ahash *tfm, const u8 *key,
++				     unsigned int keylen)
++{
++	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha256",
++					SHA256_DIGEST_SIZE);
++}
++
++static int safexcel_hmac_sha256_init(struct ahash_request *areq)
++{
++	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
++
++	safexcel_sha256_init(areq);
++	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
++	return 0;
++}
++
++static int safexcel_hmac_sha256_digest(struct ahash_request *areq)
++{
++	int ret = safexcel_hmac_sha256_init(areq);
++
++	if (ret)
++		return ret;
++
++	return safexcel_ahash_finup(areq);
++}
++
++struct safexcel_alg_template safexcel_alg_hmac_sha256 = {
++	.type = SAFEXCEL_ALG_TYPE_AHASH,
++	.alg.ahash = {
++		.init = safexcel_hmac_sha256_init,
++		.update = safexcel_ahash_update,
++		.final = safexcel_ahash_final,
++		.finup = safexcel_ahash_finup,
++		.digest = safexcel_hmac_sha256_digest,
++		.setkey = safexcel_hmac_sha256_setkey,
++		.export = safexcel_ahash_export,
++		.import = safexcel_ahash_import,
++		.halg = {
++			.digestsize = SHA256_DIGEST_SIZE,
++			.statesize = sizeof(struct safexcel_ahash_export_state),
++			.base = {
++				.cra_name = "hmac(sha256)",
++				.cra_driver_name = "safexcel-hmac-sha256",
++				.cra_priority = 300,
++				.cra_flags = CRYPTO_ALG_ASYNC |
++					     CRYPTO_ALG_KERN_DRIVER_ONLY,
++				.cra_blocksize = SHA256_BLOCK_SIZE,
++				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
++				.cra_init = safexcel_ahash_cra_init,
++				.cra_exit = safexcel_ahash_cra_exit,
++				.cra_module = THIS_MODULE,
++			},
++		},
++	},
++};
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0082-crypto-inside-secure-hmac-sha224-support.patch b/target/linux/mvebu/patches-4.14/0082-crypto-inside-secure-hmac-sha224-support.patch
new file mode 100644
index 0000000..b76da51
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0082-crypto-inside-secure-hmac-sha224-support.patch
@@ -0,0 +1,110 @@
+From 236f2b5e615e16bee2e582245cef47aa4fd0cb52 Mon Sep 17 00:00:00 2001
+From: Antoine Tenart <antoine.tenart@bootlin.com>
+Date: Mon, 19 Mar 2018 09:21:21 +0100
+Subject: [PATCH 82/88] crypto: inside-secure - hmac(sha224) support
+
+This patch adds the hmac(sha224) support to the Inside Secure
+cryptographic engine driver.
+
+Signed-off-by: Antoine Tenart <antoine.tenart@bootlin.com>
+Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
+---
+ drivers/crypto/inside-secure/safexcel.c      |  1 +
+ drivers/crypto/inside-secure/safexcel.h      |  1 +
+ drivers/crypto/inside-secure/safexcel_hash.c | 56 ++++++++++++++++++++
+ 3 files changed, 58 insertions(+)
+
+diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
+index 33595f41586f..d4a81be0d7d2 100644
+--- a/drivers/crypto/inside-secure/safexcel.c
++++ b/drivers/crypto/inside-secure/safexcel.c
+@@ -768,6 +768,7 @@ static struct safexcel_alg_template *safexcel_algs[] = {
+ 	&safexcel_alg_sha224,
+ 	&safexcel_alg_sha256,
+ 	&safexcel_alg_hmac_sha1,
++	&safexcel_alg_hmac_sha224,
+ 	&safexcel_alg_hmac_sha256,
+ };
+ 
+diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
+index 99e0f32452ff..b470a849721f 100644
+--- a/drivers/crypto/inside-secure/safexcel.h
++++ b/drivers/crypto/inside-secure/safexcel.h
+@@ -633,6 +633,7 @@ extern struct safexcel_alg_template safexcel_alg_sha1;
+ extern struct safexcel_alg_template safexcel_alg_sha224;
+ extern struct safexcel_alg_template safexcel_alg_sha256;
+ extern struct safexcel_alg_template safexcel_alg_hmac_sha1;
++extern struct safexcel_alg_template safexcel_alg_hmac_sha224;
+ extern struct safexcel_alg_template safexcel_alg_hmac_sha256;
+ 
+ #endif
+diff --git a/drivers/crypto/inside-secure/safexcel_hash.c b/drivers/crypto/inside-secure/safexcel_hash.c
+index 2917a902596d..d9ddf776c799 100644
+--- a/drivers/crypto/inside-secure/safexcel_hash.c
++++ b/drivers/crypto/inside-secure/safexcel_hash.c
+@@ -1143,6 +1143,62 @@ struct safexcel_alg_template safexcel_alg_sha224 = {
+ 	},
+ };
+ 
++static int safexcel_hmac_sha224_setkey(struct crypto_ahash *tfm, const u8 *key,
++				       unsigned int keylen)
++{
++	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha224",
++					SHA256_DIGEST_SIZE);
++}
++
++static int safexcel_hmac_sha224_init(struct ahash_request *areq)
++{
++	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
++
++	safexcel_sha224_init(areq);
++	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
++	return 0;
++}
++
++static int safexcel_hmac_sha224_digest(struct ahash_request *areq)
++{
++	int ret = safexcel_hmac_sha224_init(areq);
++
++	if (ret)
++		return ret;
++
++	return safexcel_ahash_finup(areq);
++}
++
++struct safexcel_alg_template safexcel_alg_hmac_sha224 = {
++	.type = SAFEXCEL_ALG_TYPE_AHASH,
++	.alg.ahash = {
++		.init = safexcel_hmac_sha224_init,
++		.update = safexcel_ahash_update,
++		.final = safexcel_ahash_final,
++		.finup = safexcel_ahash_finup,
++		.digest = safexcel_hmac_sha224_digest,
++		.setkey = safexcel_hmac_sha224_setkey,
++		.export = safexcel_ahash_export,
++		.import = safexcel_ahash_import,
++		.halg = {
++			.digestsize = SHA224_DIGEST_SIZE,
++			.statesize = sizeof(struct safexcel_ahash_export_state),
++			.base = {
++				.cra_name = "hmac(sha224)",
++				.cra_driver_name = "safexcel-hmac-sha224",
++				.cra_priority = 300,
++				.cra_flags = CRYPTO_ALG_ASYNC |
++					     CRYPTO_ALG_KERN_DRIVER_ONLY,
++				.cra_blocksize = SHA224_BLOCK_SIZE,
++				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
++				.cra_init = safexcel_ahash_cra_init,
++				.cra_exit = safexcel_ahash_cra_exit,
++				.cra_module = THIS_MODULE,
++			},
++		},
++	},
++};
++
+ static int safexcel_hmac_sha256_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+-- 
+2.17.0
+
diff --git a/target/linux/mvebu/patches-4.14/0088-mox-new-prototype-changes.patch b/target/linux/mvebu/patches-4.14/0088-mox-new-prototype-changes.patch
new file mode 100644
index 0000000..b83666e
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/0088-mox-new-prototype-changes.patch
@@ -0,0 +1,11417 @@
+From 0ddf153a1957fe477889009ac4ec542b76312f02 Mon Sep 17 00:00:00 2001
+From: =?UTF-8?q?Marek=20Beh=C3=BAn?= <marek.behun@nic.cz>
+Date: Fri, 18 May 2018 19:54:30 +0200
+Subject: [PATCH 88/88] mox new prototype changes
+
+---
+ .../dts/marvell/armada-3720-turris-mox-sd.dts |   2 +
+ .../marvell/armada-3720-turris-mox-sfp.dtsi   |  22 +-
+ .../dts/marvell/armada-3720-turris-mox.dtsi   |  24 +-
+ drivers/gpio/gpio-turris-mox-sfp.c            | 129 +--
+ drivers/net/wireless/ath/ath10k/Kconfig       |  21 +-
+ drivers/net/wireless/ath/ath10k/Makefile      |  10 +-
+ drivers/net/wireless/ath/ath10k/ahb.c         |  11 +-
+ drivers/net/wireless/ath/ath10k/bmi.c         |   2 +-
+ drivers/net/wireless/ath/ath10k/bmi.h         |   2 +-
+ drivers/net/wireless/ath/ath10k/ce.c          | 660 ++++++++++++---
+ drivers/net/wireless/ath/ath10k/ce.h          |  71 +-
+ drivers/net/wireless/ath/ath10k/core.c        | 730 +++++++++++++----
+ drivers/net/wireless/ath/ath10k/core.h        | 146 +++-
+ drivers/net/wireless/ath/ath10k/debug.c       | 459 ++++-------
+ drivers/net/wireless/ath/ath10k/debug.h       |  79 +-
+ drivers/net/wireless/ath/ath10k/debugfs_sta.c | 315 ++++++-
+ drivers/net/wireless/ath/ath10k/hif.h         |  17 +-
+ drivers/net/wireless/ath/ath10k/htc.c         |  31 +-
+ drivers/net/wireless/ath/ath10k/htc.h         |   7 +-
+ drivers/net/wireless/ath/ath10k/htt.c         |   7 +-
+ drivers/net/wireless/ath/ath10k/htt.h         | 310 ++++++-
+ drivers/net/wireless/ath/ath10k/htt_rx.c      | 688 +++++++++++++---
+ drivers/net/wireless/ath/ath10k/htt_tx.c      | 775 ++++++++++++++++--
+ drivers/net/wireless/ath/ath10k/hw.c          |  19 +-
+ drivers/net/wireless/ath/ath10k/hw.h          |  74 +-
+ drivers/net/wireless/ath/ath10k/mac.c         | 306 +++++--
+ drivers/net/wireless/ath/ath10k/mac.h         |   2 +-
+ drivers/net/wireless/ath/ath10k/pci.c         | 337 +++++++-
+ drivers/net/wireless/ath/ath10k/pci.h         |   4 +-
+ drivers/net/wireless/ath/ath10k/rx_desc.h     |  67 +-
+ drivers/net/wireless/ath/ath10k/sdio.c        | 340 +++++---
+ drivers/net/wireless/ath/ath10k/sdio.h        |  10 +-
+ drivers/net/wireless/ath/ath10k/spectral.c    |   4 +-
+ drivers/net/wireless/ath/ath10k/spectral.h    |   6 +-
+ drivers/net/wireless/ath/ath10k/swap.c        |   2 +-
+ drivers/net/wireless/ath/ath10k/swap.h        |   2 +-
+ drivers/net/wireless/ath/ath10k/targaddrs.h   |   2 +-
+ drivers/net/wireless/ath/ath10k/testmode.c    |   2 +-
+ drivers/net/wireless/ath/ath10k/testmode_i.h  |   2 +-
+ drivers/net/wireless/ath/ath10k/thermal.c     |   2 +-
+ drivers/net/wireless/ath/ath10k/thermal.h     |   2 +-
+ drivers/net/wireless/ath/ath10k/trace.h       |  14 +-
+ drivers/net/wireless/ath/ath10k/txrx.c        |  27 +-
+ drivers/net/wireless/ath/ath10k/txrx.h        |   2 +-
+ drivers/net/wireless/ath/ath10k/usb.c         |   8 +-
+ drivers/net/wireless/ath/ath10k/wmi-ops.h     |  51 +-
+ drivers/net/wireless/ath/ath10k/wmi-tlv.c     | 239 +++++-
+ drivers/net/wireless/ath/ath10k/wmi-tlv.h     | 133 ++-
+ drivers/net/wireless/ath/ath10k/wmi.c         | 548 ++++++++++++-
+ drivers/net/wireless/ath/ath10k/wmi.h         | 113 ++-
+ drivers/net/wireless/ath/ath10k/wow.c         |   2 +-
+ drivers/net/wireless/ath/ath10k/wow.h         |   2 +-
+ include/linux/pci_ids.h                       |  11 +
+ include/net/mac80211.h                        |  46 +-
+ 54 files changed, 5616 insertions(+), 1281 deletions(-)
+
+Index: linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
+===================================================================
+--- linux-4.14.54.orig/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
++++ linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sd.dts
+@@ -12,5 +12,7 @@
+ 	cd-gpios = <&gpionb 10 GPIO_ACTIVE_HIGH>;
+ 	no-uhs-voltage;
+ 	marvell,pad-type = "fixed-1-8v";
++/*	vqmmc-supply = <&vsdc_reg>;
++	marvell,pad-type = "sd";*/
+ 	status = "okay";
+ };
+Index: linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
+===================================================================
+--- linux-4.14.54.orig/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
++++ linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox-sfp.dtsi
+@@ -8,27 +8,17 @@
+ 	sfp: sfp {
+ 		compatible = "sff,sfp+";
+ 		i2c-bus = <&i2c0>;
+-		mod-def0-gpio = <&gpio_spi 2 GPIO_ACTIVE_LOW>;
+-		los-gpio = <&gpio_spi 0 GPIO_ACTIVE_HIGH>;
+-		tx-fault-gpio = <&gpio_spi 1 GPIO_ACTIVE_HIGH>;
+-		tx-disable-gpio = <&gpio_spi 3 GPIO_ACTIVE_HIGH>;
+-		rate-select0-gpio = <&gpio_spi 4 GPIO_ACTIVE_HIGH>;
++		mod-def0-gpio = <&mox_spi_bus 2 GPIO_ACTIVE_LOW>;
++		los-gpio = <&mox_spi_bus 0 GPIO_ACTIVE_HIGH>;
++		tx-fault-gpio = <&mox_spi_bus 1 GPIO_ACTIVE_HIGH>;
++		tx-disable-gpio = <&mox_spi_bus 3 GPIO_ACTIVE_HIGH>;
++		rate-select0-gpio = <&mox_spi_bus 4 GPIO_ACTIVE_HIGH>;
+ 	};
+ };
+ 
+ &eth1 {
+ 	status = "okay";
+-	phy-mode = "sgmii";
++	phy-mode = "1000base-x";
+ 	sfp = <&sfp>;
+ 	managed = "in-band-status";
+ };
+-
+-&spi0 {
+-	gpio_spi: gpio_spi@0 {
+-		compatible = "cznic,turris-mox-sfp";
+-		gpio-controller;
+-		#gpio-cells = <2>;
+-		reg = <1>;
+-		spi-max-frequency = <50000000>;
+-	};
+-};
+Index: linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
+===================================================================
+--- linux-4.14.54.orig/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
++++ linux-4.14.54/arch/arm64/boot/dts/marvell/armada-3720-turris-mox.dtsi
+@@ -30,7 +30,7 @@
+ 	leds {
+ 		compatible = "gpio-leds";
+ 		red {
+-			gpios = <&gpionb 11 GPIO_ACTIVE_LOW>;
++			gpios = <&gpiosb 21 GPIO_ACTIVE_LOW>;
+ 			linux,default-trigger = "heartbeat";
+ 		};
+ 	};
+@@ -62,6 +62,20 @@
+ 		compatible = "usb-nop-xceiv";
+ 		vcc-supply = <&exp_usb3_vbus>;
+ 	};
++
++	vsdc_reg: regulator {
++		compatible = "regulator-gpio";
++		regulator-name = "vsdc";
++		regulator-min-microvolt = <1800000>;
++		regulator-max-microvolt = <3300000>;
++		regulator-boot-on;
++
++		gpios = <&gpiosb 23 GPIO_ACTIVE_HIGH>;
++		gpios-states = <0>;
++		states = <1800000 0x1
++			  3300000 0x0>;
++		enable-active-high;
++	};
+ };
+ 
+ &pinctrl_nb {
+@@ -169,6 +183,14 @@
+ 			reg = <0x190000 0x670000>;
+ 		};
+ 	};
++
++	mox_spi_bus: mox_spi_bus@0 {
++		compatible = "cznic,turris-mox-bus";
++		gpio-controller;
++		#gpio-cells = <2>;
++		reg = <1>;
++		spi-max-frequency = <50000000>;
++	};
+ };
+ 
+ &usb3 {
+Index: linux-4.14.54/drivers/gpio/gpio-turris-mox-sfp.c
+===================================================================
+--- linux-4.14.54.orig/drivers/gpio/gpio-turris-mox-sfp.c
++++ linux-4.14.54/drivers/gpio/gpio-turris-mox-sfp.c
+@@ -20,65 +20,81 @@ struct mox_sfp_chip {
+ 	u8			state;
+ 	int			sfp_idx;
+ 	struct gpio_desc	*gpiod_oe;
++	char			module_topology[128];
+ };
+ 
+ static int mox_sfp_find(struct mox_sfp_chip *chip) {
++	static const char *module_names[] = {
++		[0x1] = "sfp ",
++		[0x2] = "pci ",
++		[0x3] = "topaz ",
++	};
+ 	struct spi_transfer t;
+-	u8 rx[5], tx[5];
+-	int i, res, idx = -1;
++	u8 rx[10], tx[10];
++	size_t len = 0;
++	int i, res;
+ 
+-	memset(rx, 0, 5);
+-	memset(tx, 1, 5); /* DET has to be set to 1 for current boards */
++	memset(rx, 0, 10);
++	memset(tx, 1, 10); /* DET has to be set to 1 for current boards */
+ 
+ 	memset(&t, 0, sizeof(t));
+ 
+ 	t.rx_buf = rx;
+ 	t.tx_buf = tx;
+-	t.len = 5;
++	t.len = 10;
+ 
+ 	res = spi_sync_transfer(to_spi_device(chip->gpio_chip.parent), &t, 1);
+ 	if (res < 0)
+ 		return res;
+ 
+-	if (rx[0] != 0xff && rx[0] != 0)
++	if (rx[0] != 0xff && rx[0] != 0x00 && rx[0] != 0x10)
+ 		return -ENODEV;
+ 
+-	for (i = 1; i < 5; ++i) {
+-		if ((rx[i] & 0xf) == 0x1) {
+-			idx = i;
+-			break;
+-		}
+-	}
++	chip->sfp_idx = 0;
+ 
+-	if (idx < 0)
+-		return -ENODEV;
++	for (i = 1; i < 10; ++i) {
++		int midx = rx[i] & 0xf;
+ 
+-	chip->sfp_idx = idx;
++		if (!chip->sfp_idx && midx == 0x1)
++			chip->sfp_idx = i;
++
++		if (0x1 <= midx && midx <= 0x3) {
++			size_t mlen = strlen(module_names[midx]);
++
++			if (len + mlen < sizeof(chip->module_topology)) {
++				strcpy(&chip->module_topology[len],
++				       module_names[midx]);
++				len += mlen;
++			}
++		}
++	}
++	chip->module_topology[len > 0 ? len - 1 : 0] = '\0';
+ 
+ 	return 0;
+ }
+ 
+ static int mox_sfp_do_spi(struct mox_sfp_chip *chip) {
+ 	struct spi_transfer t;
+-	u8 rx[5], tx[5], val;
++	u8 rx[10], tx[10], val;
+ 	int res;
+ 
+ 	val = (chip->state >> 2);
+ 	val |= 1; /* DET has to be set to 1 for current boards */
+ 
+-	memset(tx, 0, 5);
++	memset(tx, 0, 10);
+ 	tx[chip->sfp_idx] = val;
+ 
+ 	memset(&t, 0, sizeof(t));
+ 	t.rx_buf = rx;
+ 	t.tx_buf = tx;
+-	t.len = 5;
++	t.len = 10;
+ 
+ 	res = spi_sync_transfer(to_spi_device(chip->gpio_chip.parent), &t, 1);
+ 	if (res < 0)
+ 		return res;
+ 
+-	if ((rx[0] != 0xff && rx[0] != 0) || (rx[chip->sfp_idx] & 0xf) != 0x1)
++	if ((rx[0] != 0xff && rx[0] != 0x00 && rx[0] != 0x10)
++	    || (rx[chip->sfp_idx] & 0xf) != 0x1)
+ 		return -ENODEV;
+ 
+ 	chip->state = (chip->state & ~3) | ((rx[1] >> 4) & 3);
+@@ -141,6 +157,14 @@ static int mox_sfp_direction_output(stru
+ 	return 0;
+ }
+ 
++static ssize_t module_topology_show(struct device *dev,
++				    struct device_attribute *attr, char *buf)
++{
++	struct mox_sfp_chip *chip = dev_get_drvdata(dev);
++	return sprintf(buf, "%s\n", chip->module_topology);
++}
++static DEVICE_ATTR_RO(module_topology);
++
+ static int mox_sfp_probe(struct spi_device *spi)
+ {
+ 	struct mox_sfp_chip *chip;
+@@ -158,42 +182,50 @@ static int mox_sfp_probe(struct spi_devi
+ 	if (!chip)
+ 		return -ENOMEM;
+ 
+-	chip->gpiod_oe = devm_gpiod_get_optional(&spi->dev, "enable",
+-						 GPIOD_OUT_LOW);
+-	if (IS_ERR(chip->gpiod_oe))
+-		return PTR_ERR(chip->gpiod_oe);
++	chip->gpio_chip.parent = &spi->dev;
+ 
+-	gpiod_set_value_cansleep(chip->gpiod_oe, 1);
++	ret = mox_sfp_find(chip);
++	if (ret < 0) {
++		dev_err(&spi->dev, "Failed writing: %d\n", ret);
++		return ret;
++	}
+ 
++	device_create_file(&spi->dev, &dev_attr_module_topology);
+ 	spi_set_drvdata(spi, chip);
++	mutex_init(&chip->lock);
+ 
+-	chip->gpio_chip.label = spi->modalias;
+-	chip->gpio_chip.get_direction = mox_sfp_get_direction;
+-	chip->gpio_chip.direction_input = mox_sfp_direction_input;
+-	chip->gpio_chip.direction_output = mox_sfp_direction_output;
+-	chip->gpio_chip.get = mox_sfp_get_value;
+-	chip->gpio_chip.set = mox_sfp_set_value;
+-	chip->gpio_chip.base = -1;
+-
+-	chip->gpio_chip.ngpio = 5;
+-
+-	chip->gpio_chip.can_sleep = true;
+-	chip->gpio_chip.parent = &spi->dev;
+-	chip->gpio_chip.owner = THIS_MODULE;
++	if (chip->sfp_idx) {
++		chip->gpiod_oe = devm_gpiod_get_optional(&spi->dev, "enable",
++							 GPIOD_OUT_LOW);
++		if (IS_ERR(chip->gpiod_oe)) {
++			ret = PTR_ERR(chip->gpiod_oe);
++			goto exit_destroy;
++		}
+ 
+-	mutex_init(&chip->lock);
++		gpiod_set_value_cansleep(chip->gpiod_oe, 1);
+ 
+-	ret = mox_sfp_find(chip);
+-	if (ret < 0) {
+-		dev_err(&spi->dev, "Failed writing: %d\n", ret);
+-		goto exit_destroy;
++		chip->gpio_chip.label = spi->modalias;
++		chip->gpio_chip.get_direction = mox_sfp_get_direction;
++		chip->gpio_chip.direction_input = mox_sfp_direction_input;
++		chip->gpio_chip.direction_output = mox_sfp_direction_output;
++		chip->gpio_chip.get = mox_sfp_get_value;
++		chip->gpio_chip.set = mox_sfp_set_value;
++		chip->gpio_chip.base = -1;
++
++		chip->gpio_chip.ngpio = 5;
++
++		chip->gpio_chip.can_sleep = true;
++		chip->gpio_chip.owner = THIS_MODULE;
++
++		ret = gpiochip_add_data(&chip->gpio_chip, chip);
++		if (ret)
++			goto exit_destroy;
+ 	}
+ 
+-	ret = gpiochip_add_data(&chip->gpio_chip, chip);
+-	if (!ret)
+-		return 0;
++	return 0;
+ 
+ exit_destroy:
++	device_remove_file(&spi->dev, &dev_attr_module_topology);
+ 	mutex_destroy(&chip->lock);
+ 
+ 	return ret;
+@@ -203,15 +235,20 @@ static int mox_sfp_remove(struct spi_dev
+ {
+ 	struct mox_sfp_chip *chip = spi_get_drvdata(spi);
+ 
+-	gpiod_set_value_cansleep(chip->gpiod_oe, 0);
+-	gpiochip_remove(&chip->gpio_chip);
++	device_remove_file(&spi->dev, &dev_attr_module_topology);
++
++	if (chip->sfp_idx) {
++		gpiod_set_value_cansleep(chip->gpiod_oe, 0);
++		gpiochip_remove(&chip->gpio_chip);
++	}
++
+ 	mutex_destroy(&chip->lock);
+ 
+ 	return 0;
+ }
+ 
+ static const struct of_device_id mox_sfp_dt_ids[] = {
+-	{ .compatible = "cznic,turris-mox-sfp" },
++	{ .compatible = "cznic,turris-mox-bus" },
+ 	{},
+ };
+ MODULE_DEVICE_TABLE(of, mox_sfp_dt_ids);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/Kconfig
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/Kconfig
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/Kconfig
+@@ -4,12 +4,16 @@ config ATH10K
+ 	select ATH_COMMON
+ 	select CRC32
+ 	select WANT_DEV_COREDUMP
++	select ATH10K_CE
+         ---help---
+           This module adds support for wireless adapters based on
+           Atheros IEEE 802.11ac family of chipsets.
+ 
+           If you choose to build a module, it'll be called ath10k.
+ 
++config ATH10K_CE
++	bool
++
+ config ATH10K_PCI
+ 	tristate "Atheros ath10k PCI support"
+ 	depends on ATH10K && PCI
+@@ -36,6 +40,14 @@ config ATH10K_USB
+ 	  This module adds experimental support for USB bus. Currently
+ 	  work in progress and will not fully work.
+ 
++config ATH10K_SNOC
++        tristate "Qualcomm ath10k SNOC support (EXPERIMENTAL)"
++        depends on ATH10K && ARCH_QCOM
++        ---help---
++          This module adds support for integrated WCN3990 chip connected
++          to system NOC(SNOC). Currently work in progress and will not
++          fully work.
++
+ config ATH10K_DEBUG
+ 	bool "Atheros ath10k debugging"
+ 	depends on ATH10K
+@@ -47,12 +59,19 @@ config ATH10K_DEBUG
+ config ATH10K_DEBUGFS
+ 	bool "Atheros ath10k debugfs support"
+ 	depends on ATH10K && DEBUG_FS
+-	select RELAY
+ 	---help---
+ 	  Enabled debugfs support
+ 
+ 	  If unsure, say Y to make it easier to debug problems.
+ 
++config ATH10K_SPECTRAL
++	bool "Atheros ath10k spectral scan support"
++	depends on ATH10K_DEBUGFS
++	select RELAY
++	default n
++	---help---
++	  Say Y to enable access to the FFT/spectral data via debugfs.
++
+ config ATH10K_TRACING
+ 	bool "Atheros ath10k tracing support"
+ 	depends on ATH10K
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/Makefile
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/Makefile
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/Makefile
+@@ -15,16 +15,17 @@ ath10k_core-y += mac.o \
+ 		 p2p.o \
+ 		 swap.o
+ 
+-ath10k_core-$(CONFIG_ATH10K_DEBUGFS) += spectral.o
++ath10k_core-$(CONFIG_ATH10K_SPECTRAL) += spectral.o
+ ath10k_core-$(CONFIG_NL80211_TESTMODE) += testmode.o
+ ath10k_core-$(CONFIG_ATH10K_TRACING) += trace.o
+ ath10k_core-$(CONFIG_THERMAL) += thermal.o
+ ath10k_core-$(CONFIG_MAC80211_DEBUGFS) += debugfs_sta.o
+ ath10k_core-$(CONFIG_PM) += wow.o
++ath10k_core-$(CONFIG_DEV_COREDUMP) += coredump.o
++ath10k_core-$(CONFIG_ATH10K_CE) += ce.o
+ 
+ obj-$(CONFIG_ATH10K_PCI) += ath10k_pci.o
+-ath10k_pci-y += pci.o \
+-		ce.o
++ath10k_pci-y += pci.o
+ 
+ ath10k_pci-$(CONFIG_ATH10K_AHB) += ahb.o
+ 
+@@ -34,5 +35,8 @@ ath10k_sdio-y += sdio.o
+ obj-$(CONFIG_ATH10K_USB) += ath10k_usb.o
+ ath10k_usb-y += usb.o
+ 
++obj-$(CONFIG_ATH10K_SNOC) += ath10k_snoc.o
++ath10k_snoc-y += snoc.o
++
+ # for tracing framework to find trace.h
+ CFLAGS_trace.o := -I$(src)
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/ahb.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/ahb.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/ahb.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2016 Qualcomm Atheros, Inc. All rights reserved.
++ * Copyright (c) 2016-2017 Qualcomm Atheros, Inc. All rights reserved.
+  * Copyright (c) 2015 The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+@@ -758,7 +758,7 @@ static int ath10k_ahb_probe(struct platf
+ 	enum ath10k_hw_rev hw_rev;
+ 	size_t size;
+ 	int ret;
+-	u32 chip_id;
++	struct ath10k_bus_params bus_params;
+ 
+ 	of_id = of_match_device(ath10k_ahb_of_match, &pdev->dev);
+ 	if (!of_id) {
+@@ -814,14 +814,15 @@ static int ath10k_ahb_probe(struct platf
+ 
+ 	ath10k_pci_ce_deinit(ar);
+ 
+-	chip_id = ath10k_ahb_soc_read32(ar, SOC_CHIP_ID_ADDRESS);
+-	if (chip_id == 0xffffffff) {
++	bus_params.is_high_latency = false;
++	bus_params.chip_id = ath10k_ahb_soc_read32(ar, SOC_CHIP_ID_ADDRESS);
++	if (bus_params.chip_id == 0xffffffff) {
+ 		ath10k_err(ar, "failed to get chip id\n");
+ 		ret = -ENODEV;
+ 		goto err_halt_device;
+ 	}
+ 
+-	ret = ath10k_core_register(ar, chip_id);
++	ret = ath10k_core_register(ar, &bus_params);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to register driver core: %d\n", ret);
+ 		goto err_halt_device;
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/bmi.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/bmi.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/bmi.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2014,2016-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/bmi.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/bmi.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/bmi.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2015,2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/ce.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/ce.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/ce.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -327,12 +327,12 @@ static inline void ath10k_ce_engine_int_
+  * Guts of ath10k_ce_send.
+  * The caller takes responsibility for any needed locking.
+  */
+-int ath10k_ce_send_nolock(struct ath10k_ce_pipe *ce_state,
+-			  void *per_transfer_context,
+-			  u32 buffer,
+-			  unsigned int nbytes,
+-			  unsigned int transfer_id,
+-			  unsigned int flags)
++static int _ath10k_ce_send_nolock(struct ath10k_ce_pipe *ce_state,
++				  void *per_transfer_context,
++				  dma_addr_t buffer,
++				  unsigned int nbytes,
++				  unsigned int transfer_id,
++				  unsigned int flags)
+ {
+ 	struct ath10k *ar = ce_state->ar;
+ 	struct ath10k_ce_ring *src_ring = ce_state->src_ring;
+@@ -384,6 +384,88 @@ exit:
+ 	return ret;
+ }
+ 
++static int _ath10k_ce_send_nolock_64(struct ath10k_ce_pipe *ce_state,
++				     void *per_transfer_context,
++				     dma_addr_t buffer,
++				     unsigned int nbytes,
++				     unsigned int transfer_id,
++				     unsigned int flags)
++{
++	struct ath10k *ar = ce_state->ar;
++	struct ath10k_ce_ring *src_ring = ce_state->src_ring;
++	struct ce_desc_64 *desc, sdesc;
++	unsigned int nentries_mask = src_ring->nentries_mask;
++	unsigned int sw_index = src_ring->sw_index;
++	unsigned int write_index = src_ring->write_index;
++	u32 ctrl_addr = ce_state->ctrl_addr;
++	__le32 *addr;
++	u32 desc_flags = 0;
++	int ret = 0;
++
++	if (test_bit(ATH10K_FLAG_CRASH_FLUSH, &ar->dev_flags))
++		return -ESHUTDOWN;
++
++	if (nbytes > ce_state->src_sz_max)
++		ath10k_warn(ar, "%s: send more we can (nbytes: %d, max: %d)\n",
++			    __func__, nbytes, ce_state->src_sz_max);
++
++	if (unlikely(CE_RING_DELTA(nentries_mask,
++				   write_index, sw_index - 1) <= 0)) {
++		ret = -ENOSR;
++		goto exit;
++	}
++
++	desc = CE_SRC_RING_TO_DESC_64(src_ring->base_addr_owner_space,
++				      write_index);
++
++	desc_flags |= SM(transfer_id, CE_DESC_FLAGS_META_DATA);
++
++	if (flags & CE_SEND_FLAG_GATHER)
++		desc_flags |= CE_DESC_FLAGS_GATHER;
++
++	if (flags & CE_SEND_FLAG_BYTE_SWAP)
++		desc_flags |= CE_DESC_FLAGS_BYTE_SWAP;
++
++	addr = (__le32 *)&sdesc.addr;
++
++	flags |= upper_32_bits(buffer) & CE_DESC_FLAGS_GET_MASK;
++	addr[0] = __cpu_to_le32(buffer);
++	addr[1] = __cpu_to_le32(flags);
++	if (flags & CE_SEND_FLAG_GATHER)
++		addr[1] |= __cpu_to_le32(CE_WCN3990_DESC_FLAGS_GATHER);
++	else
++		addr[1] &= ~(__cpu_to_le32(CE_WCN3990_DESC_FLAGS_GATHER));
++
++	sdesc.nbytes = __cpu_to_le16(nbytes);
++	sdesc.flags  = __cpu_to_le16(desc_flags);
++
++	*desc = sdesc;
++
++	src_ring->per_transfer_context[write_index] = per_transfer_context;
++
++	/* Update Source Ring Write Index */
++	write_index = CE_RING_IDX_INCR(nentries_mask, write_index);
++
++	if (!(flags & CE_SEND_FLAG_GATHER))
++		ath10k_ce_src_ring_write_index_set(ar, ctrl_addr, write_index);
++
++	src_ring->write_index = write_index;
++exit:
++	return ret;
++}
++
++int ath10k_ce_send_nolock(struct ath10k_ce_pipe *ce_state,
++			  void *per_transfer_context,
++			  dma_addr_t buffer,
++			  unsigned int nbytes,
++			  unsigned int transfer_id,
++			  unsigned int flags)
++{
++	return ce_state->ops->ce_send_nolock(ce_state, per_transfer_context,
++				    buffer, nbytes, transfer_id, flags);
++}
++EXPORT_SYMBOL(ath10k_ce_send_nolock);
++
+ void __ath10k_ce_send_revert(struct ath10k_ce_pipe *pipe)
+ {
+ 	struct ath10k *ar = pipe->ar;
+@@ -410,10 +492,11 @@ void __ath10k_ce_send_revert(struct ath1
+ 
+ 	src_ring->per_transfer_context[src_ring->write_index] = NULL;
+ }
++EXPORT_SYMBOL(__ath10k_ce_send_revert);
+ 
+ int ath10k_ce_send(struct ath10k_ce_pipe *ce_state,
+ 		   void *per_transfer_context,
+-		   u32 buffer,
++		   dma_addr_t buffer,
+ 		   unsigned int nbytes,
+ 		   unsigned int transfer_id,
+ 		   unsigned int flags)
+@@ -429,6 +512,7 @@ int ath10k_ce_send(struct ath10k_ce_pipe
+ 
+ 	return ret;
+ }
++EXPORT_SYMBOL(ath10k_ce_send);
+ 
+ int ath10k_ce_num_free_src_entries(struct ath10k_ce_pipe *pipe)
+ {
+@@ -444,6 +528,7 @@ int ath10k_ce_num_free_src_entries(struc
+ 
+ 	return delta;
+ }
++EXPORT_SYMBOL(ath10k_ce_num_free_src_entries);
+ 
+ int __ath10k_ce_rx_num_free_bufs(struct ath10k_ce_pipe *pipe)
+ {
+@@ -458,8 +543,10 @@ int __ath10k_ce_rx_num_free_bufs(struct
+ 
+ 	return CE_RING_DELTA(nentries_mask, write_index, sw_index - 1);
+ }
++EXPORT_SYMBOL(__ath10k_ce_rx_num_free_bufs);
+ 
+-int __ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx, u32 paddr)
++static int __ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx,
++				   dma_addr_t paddr)
+ {
+ 	struct ath10k *ar = pipe->ar;
+ 	struct ath10k_ce *ce = ath10k_ce_priv(ar);
+@@ -488,6 +575,39 @@ int __ath10k_ce_rx_post_buf(struct ath10
+ 	return 0;
+ }
+ 
++static int __ath10k_ce_rx_post_buf_64(struct ath10k_ce_pipe *pipe,
++				      void *ctx,
++				      dma_addr_t paddr)
++{
++	struct ath10k *ar = pipe->ar;
++	struct ath10k_ce *ce = ath10k_ce_priv(ar);
++	struct ath10k_ce_ring *dest_ring = pipe->dest_ring;
++	unsigned int nentries_mask = dest_ring->nentries_mask;
++	unsigned int write_index = dest_ring->write_index;
++	unsigned int sw_index = dest_ring->sw_index;
++	struct ce_desc_64 *base = dest_ring->base_addr_owner_space;
++	struct ce_desc_64 *desc =
++			CE_DEST_RING_TO_DESC_64(base, write_index);
++	u32 ctrl_addr = pipe->ctrl_addr;
++
++	lockdep_assert_held(&ce->ce_lock);
++
++	if (CE_RING_DELTA(nentries_mask, write_index, sw_index - 1) == 0)
++		return -ENOSPC;
++
++	desc->addr = __cpu_to_le64(paddr);
++	desc->addr &= __cpu_to_le64(CE_DESC_37BIT_ADDR_MASK);
++
++	desc->nbytes = 0;
++
++	dest_ring->per_transfer_context[write_index] = ctx;
++	write_index = CE_RING_IDX_INCR(nentries_mask, write_index);
++	ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);
++	dest_ring->write_index = write_index;
++
++	return 0;
++}
++
+ void ath10k_ce_rx_update_write_idx(struct ath10k_ce_pipe *pipe, u32 nentries)
+ {
+ 	struct ath10k *ar = pipe->ar;
+@@ -500,34 +620,38 @@ void ath10k_ce_rx_update_write_idx(struc
+ 	/* Prevent CE ring stuck issue that will occur when ring is full.
+ 	 * Make sure that write index is 1 less than read index.
+ 	 */
+-	if ((cur_write_idx + nentries)  == dest_ring->sw_index)
++	if (((cur_write_idx + nentries) & nentries_mask) == dest_ring->sw_index)
+ 		nentries -= 1;
+ 
+ 	write_index = CE_RING_IDX_ADD(nentries_mask, write_index, nentries);
+ 	ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);
+ 	dest_ring->write_index = write_index;
+ }
++EXPORT_SYMBOL(ath10k_ce_rx_update_write_idx);
+ 
+-int ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx, u32 paddr)
++int ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx,
++			  dma_addr_t paddr)
+ {
+ 	struct ath10k *ar = pipe->ar;
+ 	struct ath10k_ce *ce = ath10k_ce_priv(ar);
+ 	int ret;
+ 
+ 	spin_lock_bh(&ce->ce_lock);
+-	ret = __ath10k_ce_rx_post_buf(pipe, ctx, paddr);
++	ret = pipe->ops->ce_rx_post_buf(pipe, ctx, paddr);
+ 	spin_unlock_bh(&ce->ce_lock);
+ 
+ 	return ret;
+ }
++EXPORT_SYMBOL(ath10k_ce_rx_post_buf);
+ 
+ /*
+  * Guts of ath10k_ce_completed_recv_next.
+  * The caller takes responsibility for any necessary locking.
+  */
+-int ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe *ce_state,
+-					 void **per_transfer_contextp,
+-					 unsigned int *nbytesp)
++static int
++	 _ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe *ce_state,
++					       void **per_transfer_contextp,
++					       unsigned int *nbytesp)
+ {
+ 	struct ath10k_ce_ring *dest_ring = ce_state->dest_ring;
+ 	unsigned int nentries_mask = dest_ring->nentries_mask;
+@@ -574,6 +698,65 @@ int ath10k_ce_completed_recv_next_nolock
+ 	return 0;
+ }
+ 
++static int
++_ath10k_ce_completed_recv_next_nolock_64(struct ath10k_ce_pipe *ce_state,
++					 void **per_transfer_contextp,
++					 unsigned int *nbytesp)
++{
++	struct ath10k_ce_ring *dest_ring = ce_state->dest_ring;
++	unsigned int nentries_mask = dest_ring->nentries_mask;
++	unsigned int sw_index = dest_ring->sw_index;
++	struct ce_desc_64 *base = dest_ring->base_addr_owner_space;
++	struct ce_desc_64 *desc =
++		CE_DEST_RING_TO_DESC_64(base, sw_index);
++	struct ce_desc_64 sdesc;
++	u16 nbytes;
++
++	/* Copy in one go for performance reasons */
++	sdesc = *desc;
++
++	nbytes = __le16_to_cpu(sdesc.nbytes);
++	if (nbytes == 0) {
++		/* This closes a relatively unusual race where the Host
++		 * sees the updated DRRI before the update to the
++		 * corresponding descriptor has completed. We treat this
++		 * as a descriptor that is not yet done.
++		 */
++		return -EIO;
++	}
++
++	desc->nbytes = 0;
++
++	/* Return data from completed destination descriptor */
++	*nbytesp = nbytes;
++
++	if (per_transfer_contextp)
++		*per_transfer_contextp =
++			dest_ring->per_transfer_context[sw_index];
++
++	/* Copy engine 5 (HTT Rx) will reuse the same transfer context.
++	 * So update transfer context all CEs except CE5.
++	 */
++	if (ce_state->id != 5)
++		dest_ring->per_transfer_context[sw_index] = NULL;
++
++	/* Update sw_index */
++	sw_index = CE_RING_IDX_INCR(nentries_mask, sw_index);
++	dest_ring->sw_index = sw_index;
++
++	return 0;
++}
++
++int ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe *ce_state,
++					 void **per_transfer_ctx,
++					 unsigned int *nbytesp)
++{
++	return ce_state->ops->ce_completed_recv_next_nolock(ce_state,
++							    per_transfer_ctx,
++							    nbytesp);
++}
++EXPORT_SYMBOL(ath10k_ce_completed_recv_next_nolock);
++
+ int ath10k_ce_completed_recv_next(struct ath10k_ce_pipe *ce_state,
+ 				  void **per_transfer_contextp,
+ 				  unsigned int *nbytesp)
+@@ -583,17 +766,19 @@ int ath10k_ce_completed_recv_next(struct
+ 	int ret;
+ 
+ 	spin_lock_bh(&ce->ce_lock);
+-	ret = ath10k_ce_completed_recv_next_nolock(ce_state,
++	ret = ce_state->ops->ce_completed_recv_next_nolock(ce_state,
+ 						   per_transfer_contextp,
+ 						   nbytesp);
++
+ 	spin_unlock_bh(&ce->ce_lock);
+ 
+ 	return ret;
+ }
++EXPORT_SYMBOL(ath10k_ce_completed_recv_next);
+ 
+-int ath10k_ce_revoke_recv_next(struct ath10k_ce_pipe *ce_state,
+-			       void **per_transfer_contextp,
+-			       u32 *bufferp)
++static int _ath10k_ce_revoke_recv_next(struct ath10k_ce_pipe *ce_state,
++				       void **per_transfer_contextp,
++				       dma_addr_t *bufferp)
+ {
+ 	struct ath10k_ce_ring *dest_ring;
+ 	unsigned int nentries_mask;
+@@ -644,6 +829,70 @@ int ath10k_ce_revoke_recv_next(struct at
+ 	return ret;
+ }
+ 
++static int _ath10k_ce_revoke_recv_next_64(struct ath10k_ce_pipe *ce_state,
++					  void **per_transfer_contextp,
++					  dma_addr_t *bufferp)
++{
++	struct ath10k_ce_ring *dest_ring;
++	unsigned int nentries_mask;
++	unsigned int sw_index;
++	unsigned int write_index;
++	int ret;
++	struct ath10k *ar;
++	struct ath10k_ce *ce;
++
++	dest_ring = ce_state->dest_ring;
++
++	if (!dest_ring)
++		return -EIO;
++
++	ar = ce_state->ar;
++	ce = ath10k_ce_priv(ar);
++
++	spin_lock_bh(&ce->ce_lock);
++
++	nentries_mask = dest_ring->nentries_mask;
++	sw_index = dest_ring->sw_index;
++	write_index = dest_ring->write_index;
++	if (write_index != sw_index) {
++		struct ce_desc_64 *base = dest_ring->base_addr_owner_space;
++		struct ce_desc_64 *desc =
++			CE_DEST_RING_TO_DESC_64(base, sw_index);
++
++		/* Return data from completed destination descriptor */
++		*bufferp = __le64_to_cpu(desc->addr);
++
++		if (per_transfer_contextp)
++			*per_transfer_contextp =
++				dest_ring->per_transfer_context[sw_index];
++
++		/* sanity */
++		dest_ring->per_transfer_context[sw_index] = NULL;
++		desc->nbytes = 0;
++
++		/* Update sw_index */
++		sw_index = CE_RING_IDX_INCR(nentries_mask, sw_index);
++		dest_ring->sw_index = sw_index;
++		ret = 0;
++	} else {
++		ret = -EIO;
++	}
++
++	spin_unlock_bh(&ce->ce_lock);
++
++	return ret;
++}
++
++int ath10k_ce_revoke_recv_next(struct ath10k_ce_pipe *ce_state,
++			       void **per_transfer_contextp,
++			       dma_addr_t *bufferp)
++{
++	return ce_state->ops->ce_revoke_recv_next(ce_state,
++						  per_transfer_contextp,
++						  bufferp);
++}
++EXPORT_SYMBOL(ath10k_ce_revoke_recv_next);
++
+ /*
+  * Guts of ath10k_ce_completed_send_next.
+  * The caller takes responsibility for any necessary locking.
+@@ -697,11 +946,47 @@ int ath10k_ce_completed_send_next_nolock
+ 
+ 	return 0;
+ }
++EXPORT_SYMBOL(ath10k_ce_completed_send_next_nolock);
++
++static void ath10k_ce_extract_desc_data(struct ath10k *ar,
++					struct ath10k_ce_ring *src_ring,
++					u32 sw_index,
++					dma_addr_t *bufferp,
++					u32 *nbytesp,
++					u32 *transfer_idp)
++{
++		struct ce_desc *base = src_ring->base_addr_owner_space;
++		struct ce_desc *desc = CE_SRC_RING_TO_DESC(base, sw_index);
++
++		/* Return data from completed source descriptor */
++		*bufferp = __le32_to_cpu(desc->addr);
++		*nbytesp = __le16_to_cpu(desc->nbytes);
++		*transfer_idp = MS(__le16_to_cpu(desc->flags),
++				   CE_DESC_FLAGS_META_DATA);
++}
++
++static void ath10k_ce_extract_desc_data_64(struct ath10k *ar,
++					   struct ath10k_ce_ring *src_ring,
++					   u32 sw_index,
++					   dma_addr_t *bufferp,
++					   u32 *nbytesp,
++					   u32 *transfer_idp)
++{
++		struct ce_desc_64 *base = src_ring->base_addr_owner_space;
++		struct ce_desc_64 *desc =
++			CE_SRC_RING_TO_DESC_64(base, sw_index);
++
++		/* Return data from completed source descriptor */
++		*bufferp = __le64_to_cpu(desc->addr);
++		*nbytesp = __le16_to_cpu(desc->nbytes);
++		*transfer_idp = MS(__le16_to_cpu(desc->flags),
++				   CE_DESC_FLAGS_META_DATA);
++}
+ 
+ /* NB: Modeled after ath10k_ce_completed_send_next */
+ int ath10k_ce_cancel_send_next(struct ath10k_ce_pipe *ce_state,
+ 			       void **per_transfer_contextp,
+-			       u32 *bufferp,
++			       dma_addr_t *bufferp,
+ 			       unsigned int *nbytesp,
+ 			       unsigned int *transfer_idp)
+ {
+@@ -728,14 +1013,9 @@ int ath10k_ce_cancel_send_next(struct at
+ 	write_index = src_ring->write_index;
+ 
+ 	if (write_index != sw_index) {
+-		struct ce_desc *base = src_ring->base_addr_owner_space;
+-		struct ce_desc *desc = CE_SRC_RING_TO_DESC(base, sw_index);
+-
+-		/* Return data from completed source descriptor */
+-		*bufferp = __le32_to_cpu(desc->addr);
+-		*nbytesp = __le16_to_cpu(desc->nbytes);
+-		*transfer_idp = MS(__le16_to_cpu(desc->flags),
+-						CE_DESC_FLAGS_META_DATA);
++		ce_state->ops->ce_extract_desc_data(ar, src_ring, sw_index,
++						    bufferp, nbytesp,
++						    transfer_idp);
+ 
+ 		if (per_transfer_contextp)
+ 			*per_transfer_contextp =
+@@ -756,6 +1036,7 @@ int ath10k_ce_cancel_send_next(struct at
+ 
+ 	return ret;
+ }
++EXPORT_SYMBOL(ath10k_ce_cancel_send_next);
+ 
+ int ath10k_ce_completed_send_next(struct ath10k_ce_pipe *ce_state,
+ 				  void **per_transfer_contextp)
+@@ -771,6 +1052,7 @@ int ath10k_ce_completed_send_next(struct
+ 
+ 	return ret;
+ }
++EXPORT_SYMBOL(ath10k_ce_completed_send_next);
+ 
+ /*
+  * Guts of interrupt handler for per-engine interrupts on a particular CE.
+@@ -809,6 +1091,7 @@ void ath10k_ce_per_engine_service(struct
+ 
+ 	spin_unlock_bh(&ce->ce_lock);
+ }
++EXPORT_SYMBOL(ath10k_ce_per_engine_service);
+ 
+ /*
+  * Handler for per-engine interrupts on ALL active CEs.
+@@ -833,6 +1116,7 @@ void ath10k_ce_per_engine_service_any(st
+ 		ath10k_ce_per_engine_service(ar, ce_id);
+ 	}
+ }
++EXPORT_SYMBOL(ath10k_ce_per_engine_service_any);
+ 
+ /*
+  * Adjust interrupts for the copy complete handler.
+@@ -870,6 +1154,7 @@ int ath10k_ce_disable_interrupts(struct
+ 
+ 	return 0;
+ }
++EXPORT_SYMBOL(ath10k_ce_disable_interrupts);
+ 
+ void ath10k_ce_enable_interrupts(struct ath10k *ar)
+ {
+@@ -885,6 +1170,7 @@ void ath10k_ce_enable_interrupts(struct
+ 		ath10k_ce_per_engine_handler_adjust(ce_state);
+ 	}
+ }
++EXPORT_SYMBOL(ath10k_ce_enable_interrupts);
+ 
+ static int ath10k_ce_init_src_ring(struct ath10k *ar,
+ 				   unsigned int ce_id,
+@@ -897,8 +1183,12 @@ static int ath10k_ce_init_src_ring(struc
+ 
+ 	nentries = roundup_pow_of_two(attr->src_nentries);
+ 
+-	memset(src_ring->base_addr_owner_space, 0,
+-	       nentries * sizeof(struct ce_desc));
++	if (ar->hw_params.target_64bit)
++		memset(src_ring->base_addr_owner_space, 0,
++		       nentries * sizeof(struct ce_desc_64));
++	else
++		memset(src_ring->base_addr_owner_space, 0,
++		       nentries * sizeof(struct ce_desc));
+ 
+ 	src_ring->sw_index = ath10k_ce_src_ring_read_index_get(ar, ctrl_addr);
+ 	src_ring->sw_index &= src_ring->nentries_mask;
+@@ -934,8 +1224,12 @@ static int ath10k_ce_init_dest_ring(stru
+ 
+ 	nentries = roundup_pow_of_two(attr->dest_nentries);
+ 
+-	memset(dest_ring->base_addr_owner_space, 0,
+-	       nentries * sizeof(struct ce_desc));
++	if (ar->hw_params.target_64bit)
++		memset(dest_ring->base_addr_owner_space, 0,
++		       nentries * sizeof(struct ce_desc_64));
++	else
++		memset(dest_ring->base_addr_owner_space, 0,
++		       nentries * sizeof(struct ce_desc));
+ 
+ 	dest_ring->sw_index = ath10k_ce_dest_ring_read_index_get(ar, ctrl_addr);
+ 	dest_ring->sw_index &= dest_ring->nentries_mask;
+@@ -993,12 +1287,57 @@ ath10k_ce_alloc_src_ring(struct ath10k *
+ 
+ 	src_ring->base_addr_ce_space_unaligned = base_addr;
+ 
+-	src_ring->base_addr_owner_space = PTR_ALIGN(
+-			src_ring->base_addr_owner_space_unaligned,
+-			CE_DESC_RING_ALIGN);
+-	src_ring->base_addr_ce_space = ALIGN(
+-			src_ring->base_addr_ce_space_unaligned,
+-			CE_DESC_RING_ALIGN);
++	src_ring->base_addr_owner_space =
++			PTR_ALIGN(src_ring->base_addr_owner_space_unaligned,
++				  CE_DESC_RING_ALIGN);
++	src_ring->base_addr_ce_space =
++			ALIGN(src_ring->base_addr_ce_space_unaligned,
++			      CE_DESC_RING_ALIGN);
++
++	return src_ring;
++}
++
++static struct ath10k_ce_ring *
++ath10k_ce_alloc_src_ring_64(struct ath10k *ar, unsigned int ce_id,
++			    const struct ce_attr *attr)
++{
++	struct ath10k_ce_ring *src_ring;
++	u32 nentries = attr->src_nentries;
++	dma_addr_t base_addr;
++
++	nentries = roundup_pow_of_two(nentries);
++
++	src_ring = kzalloc(sizeof(*src_ring) +
++			   (nentries *
++			    sizeof(*src_ring->per_transfer_context)),
++			   GFP_KERNEL);
++	if (!src_ring)
++		return ERR_PTR(-ENOMEM);
++
++	src_ring->nentries = nentries;
++	src_ring->nentries_mask = nentries - 1;
++
++	/* Legacy platforms that do not support cache
++	 * coherent DMA are unsupported
++	 */
++	src_ring->base_addr_owner_space_unaligned =
++		dma_alloc_coherent(ar->dev,
++				   (nentries * sizeof(struct ce_desc_64) +
++				    CE_DESC_RING_ALIGN),
++				   &base_addr, GFP_KERNEL);
++	if (!src_ring->base_addr_owner_space_unaligned) {
++		kfree(src_ring);
++		return ERR_PTR(-ENOMEM);
++	}
++
++	src_ring->base_addr_ce_space_unaligned = base_addr;
++
++	src_ring->base_addr_owner_space =
++			PTR_ALIGN(src_ring->base_addr_owner_space_unaligned,
++				  CE_DESC_RING_ALIGN);
++	src_ring->base_addr_ce_space =
++			ALIGN(src_ring->base_addr_ce_space_unaligned,
++			      CE_DESC_RING_ALIGN);
+ 
+ 	return src_ring;
+ }
+@@ -1039,12 +1378,63 @@ ath10k_ce_alloc_dest_ring(struct ath10k
+ 
+ 	dest_ring->base_addr_ce_space_unaligned = base_addr;
+ 
+-	dest_ring->base_addr_owner_space = PTR_ALIGN(
+-			dest_ring->base_addr_owner_space_unaligned,
+-			CE_DESC_RING_ALIGN);
+-	dest_ring->base_addr_ce_space = ALIGN(
+-			dest_ring->base_addr_ce_space_unaligned,
+-			CE_DESC_RING_ALIGN);
++	dest_ring->base_addr_owner_space =
++			PTR_ALIGN(dest_ring->base_addr_owner_space_unaligned,
++				  CE_DESC_RING_ALIGN);
++	dest_ring->base_addr_ce_space =
++				ALIGN(dest_ring->base_addr_ce_space_unaligned,
++				      CE_DESC_RING_ALIGN);
++
++	return dest_ring;
++}
++
++static struct ath10k_ce_ring *
++ath10k_ce_alloc_dest_ring_64(struct ath10k *ar, unsigned int ce_id,
++			     const struct ce_attr *attr)
++{
++	struct ath10k_ce_ring *dest_ring;
++	u32 nentries;
++	dma_addr_t base_addr;
++
++	nentries = roundup_pow_of_two(attr->dest_nentries);
++
++	dest_ring = kzalloc(sizeof(*dest_ring) +
++			    (nentries *
++			     sizeof(*dest_ring->per_transfer_context)),
++			    GFP_KERNEL);
++	if (!dest_ring)
++		return ERR_PTR(-ENOMEM);
++
++	dest_ring->nentries = nentries;
++	dest_ring->nentries_mask = nentries - 1;
++
++	/* Legacy platforms that do not support cache
++	 * coherent DMA are unsupported
++	 */
++	dest_ring->base_addr_owner_space_unaligned =
++		dma_alloc_coherent(ar->dev,
++				   (nentries * sizeof(struct ce_desc_64) +
++				    CE_DESC_RING_ALIGN),
++				   &base_addr, GFP_KERNEL);
++	if (!dest_ring->base_addr_owner_space_unaligned) {
++		kfree(dest_ring);
++		return ERR_PTR(-ENOMEM);
++	}
++
++	dest_ring->base_addr_ce_space_unaligned = base_addr;
++
++	/* Correctly initialize memory to 0 to prevent garbage
++	 * data crashing system when download firmware
++	 */
++	memset(dest_ring->base_addr_owner_space_unaligned, 0,
++	       nentries * sizeof(struct ce_desc_64) + CE_DESC_RING_ALIGN);
++
++	dest_ring->base_addr_owner_space =
++			PTR_ALIGN(dest_ring->base_addr_owner_space_unaligned,
++				  CE_DESC_RING_ALIGN);
++	dest_ring->base_addr_ce_space =
++			ALIGN(dest_ring->base_addr_ce_space_unaligned,
++			      CE_DESC_RING_ALIGN);
+ 
+ 	return dest_ring;
+ }
+@@ -1081,6 +1471,7 @@ int ath10k_ce_init_pipe(struct ath10k *a
+ 
+ 	return 0;
+ }
++EXPORT_SYMBOL(ath10k_ce_init_pipe);
+ 
+ static void ath10k_ce_deinit_src_ring(struct ath10k *ar, unsigned int ce_id)
+ {
+@@ -1106,66 +1497,38 @@ void ath10k_ce_deinit_pipe(struct ath10k
+ 	ath10k_ce_deinit_src_ring(ar, ce_id);
+ 	ath10k_ce_deinit_dest_ring(ar, ce_id);
+ }
++EXPORT_SYMBOL(ath10k_ce_deinit_pipe);
+ 
+-int ath10k_ce_alloc_pipe(struct ath10k *ar, int ce_id,
+-			 const struct ce_attr *attr)
++static void _ath10k_ce_free_pipe(struct ath10k *ar, int ce_id)
+ {
+ 	struct ath10k_ce *ce = ath10k_ce_priv(ar);
+ 	struct ath10k_ce_pipe *ce_state = &ce->ce_states[ce_id];
+-	int ret;
+-
+-	/*
+-	 * Make sure there's enough CE ringbuffer entries for HTT TX to avoid
+-	 * additional TX locking checks.
+-	 *
+-	 * For the lack of a better place do the check here.
+-	 */
+-	BUILD_BUG_ON(2 * TARGET_NUM_MSDU_DESC >
+-		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
+-	BUILD_BUG_ON(2 * TARGET_10_4_NUM_MSDU_DESC_PFC >
+-		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
+-	BUILD_BUG_ON(2 * TARGET_TLV_NUM_MSDU_DESC >
+-		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
+-
+-	ce_state->ar = ar;
+-	ce_state->id = ce_id;
+-	ce_state->ctrl_addr = ath10k_ce_base_address(ar, ce_id);
+-	ce_state->attr_flags = attr->flags;
+-	ce_state->src_sz_max = attr->src_sz_max;
+-
+-	if (attr->src_nentries)
+-		ce_state->send_cb = attr->send_cb;
+-
+-	if (attr->dest_nentries)
+-		ce_state->recv_cb = attr->recv_cb;
+ 
+-	if (attr->src_nentries) {
+-		ce_state->src_ring = ath10k_ce_alloc_src_ring(ar, ce_id, attr);
+-		if (IS_ERR(ce_state->src_ring)) {
+-			ret = PTR_ERR(ce_state->src_ring);
+-			ath10k_err(ar, "failed to allocate copy engine source ring %d: %d\n",
+-				   ce_id, ret);
+-			ce_state->src_ring = NULL;
+-			return ret;
+-		}
++	if (ce_state->src_ring) {
++		dma_free_coherent(ar->dev,
++				  (ce_state->src_ring->nentries *
++				   sizeof(struct ce_desc) +
++				   CE_DESC_RING_ALIGN),
++				  ce_state->src_ring->base_addr_owner_space,
++				  ce_state->src_ring->base_addr_ce_space);
++		kfree(ce_state->src_ring);
+ 	}
+ 
+-	if (attr->dest_nentries) {
+-		ce_state->dest_ring = ath10k_ce_alloc_dest_ring(ar, ce_id,
+-								attr);
+-		if (IS_ERR(ce_state->dest_ring)) {
+-			ret = PTR_ERR(ce_state->dest_ring);
+-			ath10k_err(ar, "failed to allocate copy engine destination ring %d: %d\n",
+-				   ce_id, ret);
+-			ce_state->dest_ring = NULL;
+-			return ret;
+-		}
++	if (ce_state->dest_ring) {
++		dma_free_coherent(ar->dev,
++				  (ce_state->dest_ring->nentries *
++				   sizeof(struct ce_desc) +
++				   CE_DESC_RING_ALIGN),
++				  ce_state->dest_ring->base_addr_owner_space,
++				  ce_state->dest_ring->base_addr_ce_space);
++		kfree(ce_state->dest_ring);
+ 	}
+ 
+-	return 0;
++	ce_state->src_ring = NULL;
++	ce_state->dest_ring = NULL;
+ }
+ 
+-void ath10k_ce_free_pipe(struct ath10k *ar, int ce_id)
++static void _ath10k_ce_free_pipe_64(struct ath10k *ar, int ce_id)
+ {
+ 	struct ath10k_ce *ce = ath10k_ce_priv(ar);
+ 	struct ath10k_ce_pipe *ce_state = &ce->ce_states[ce_id];
+@@ -1173,7 +1536,7 @@ void ath10k_ce_free_pipe(struct ath10k *
+ 	if (ce_state->src_ring) {
+ 		dma_free_coherent(ar->dev,
+ 				  (ce_state->src_ring->nentries *
+-				   sizeof(struct ce_desc) +
++				   sizeof(struct ce_desc_64) +
+ 				   CE_DESC_RING_ALIGN),
+ 				  ce_state->src_ring->base_addr_owner_space,
+ 				  ce_state->src_ring->base_addr_ce_space);
+@@ -1183,7 +1546,7 @@ void ath10k_ce_free_pipe(struct ath10k *
+ 	if (ce_state->dest_ring) {
+ 		dma_free_coherent(ar->dev,
+ 				  (ce_state->dest_ring->nentries *
+-				   sizeof(struct ce_desc) +
++				   sizeof(struct ce_desc_64) +
+ 				   CE_DESC_RING_ALIGN),
+ 				  ce_state->dest_ring->base_addr_owner_space,
+ 				  ce_state->dest_ring->base_addr_ce_space);
+@@ -1194,6 +1557,15 @@ void ath10k_ce_free_pipe(struct ath10k *
+ 	ce_state->dest_ring = NULL;
+ }
+ 
++void ath10k_ce_free_pipe(struct ath10k *ar, int ce_id)
++{
++	struct ath10k_ce *ce = ath10k_ce_priv(ar);
++	struct ath10k_ce_pipe *ce_state = &ce->ce_states[ce_id];
++
++	ce_state->ops->ce_free_pipe(ar, ce_id);
++}
++EXPORT_SYMBOL(ath10k_ce_free_pipe);
++
+ void ath10k_ce_dump_registers(struct ath10k *ar,
+ 			      struct ath10k_fw_crash_data *crash_data)
+ {
+@@ -1232,3 +1604,101 @@ void ath10k_ce_dump_registers(struct ath
+ 
+ 	spin_unlock_bh(&ce->ce_lock);
+ }
++EXPORT_SYMBOL(ath10k_ce_dump_registers);
++
++static const struct ath10k_ce_ops ce_ops = {
++	.ce_alloc_src_ring = ath10k_ce_alloc_src_ring,
++	.ce_alloc_dst_ring = ath10k_ce_alloc_dest_ring,
++	.ce_rx_post_buf = __ath10k_ce_rx_post_buf,
++	.ce_completed_recv_next_nolock = _ath10k_ce_completed_recv_next_nolock,
++	.ce_revoke_recv_next = _ath10k_ce_revoke_recv_next,
++	.ce_extract_desc_data = ath10k_ce_extract_desc_data,
++	.ce_free_pipe = _ath10k_ce_free_pipe,
++	.ce_send_nolock = _ath10k_ce_send_nolock,
++};
++
++static const struct ath10k_ce_ops ce_64_ops = {
++	.ce_alloc_src_ring = ath10k_ce_alloc_src_ring_64,
++	.ce_alloc_dst_ring = ath10k_ce_alloc_dest_ring_64,
++	.ce_rx_post_buf = __ath10k_ce_rx_post_buf_64,
++	.ce_completed_recv_next_nolock =
++				_ath10k_ce_completed_recv_next_nolock_64,
++	.ce_revoke_recv_next = _ath10k_ce_revoke_recv_next_64,
++	.ce_extract_desc_data = ath10k_ce_extract_desc_data_64,
++	.ce_free_pipe = _ath10k_ce_free_pipe_64,
++	.ce_send_nolock = _ath10k_ce_send_nolock_64,
++};
++
++static void ath10k_ce_set_ops(struct ath10k *ar,
++			      struct ath10k_ce_pipe *ce_state)
++{
++	switch (ar->hw_rev) {
++	case ATH10K_HW_WCN3990:
++		ce_state->ops = &ce_64_ops;
++		break;
++	default:
++		ce_state->ops = &ce_ops;
++		break;
++	}
++}
++
++int ath10k_ce_alloc_pipe(struct ath10k *ar, int ce_id,
++			 const struct ce_attr *attr)
++{
++	struct ath10k_ce *ce = ath10k_ce_priv(ar);
++	struct ath10k_ce_pipe *ce_state = &ce->ce_states[ce_id];
++	int ret;
++
++	ath10k_ce_set_ops(ar, ce_state);
++	/* Make sure there's enough CE ringbuffer entries for HTT TX to avoid
++	 * additional TX locking checks.
++	 *
++	 * For the lack of a better place do the check here.
++	 */
++	BUILD_BUG_ON(2 * TARGET_NUM_MSDU_DESC >
++		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
++	BUILD_BUG_ON(2 * TARGET_10_4_NUM_MSDU_DESC_PFC >
++		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
++	BUILD_BUG_ON(2 * TARGET_TLV_NUM_MSDU_DESC >
++		     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));
++
++	ce_state->ar = ar;
++	ce_state->id = ce_id;
++	ce_state->ctrl_addr = ath10k_ce_base_address(ar, ce_id);
++	ce_state->attr_flags = attr->flags;
++	ce_state->src_sz_max = attr->src_sz_max;
++
++	if (attr->src_nentries)
++		ce_state->send_cb = attr->send_cb;
++
++	if (attr->dest_nentries)
++		ce_state->recv_cb = attr->recv_cb;
++
++	if (attr->src_nentries) {
++		ce_state->src_ring =
++			ce_state->ops->ce_alloc_src_ring(ar, ce_id, attr);
++		if (IS_ERR(ce_state->src_ring)) {
++			ret = PTR_ERR(ce_state->src_ring);
++			ath10k_err(ar, "failed to alloc CE src ring %d: %d\n",
++				   ce_id, ret);
++			ce_state->src_ring = NULL;
++			return ret;
++		}
++	}
++
++	if (attr->dest_nentries) {
++		ce_state->dest_ring = ce_state->ops->ce_alloc_dst_ring(ar,
++									ce_id,
++									attr);
++		if (IS_ERR(ce_state->dest_ring)) {
++			ret = PTR_ERR(ce_state->dest_ring);
++			ath10k_err(ar, "failed to alloc CE dest ring %d: %d\n",
++				   ce_id, ret);
++			ce_state->dest_ring = NULL;
++			return ret;
++		}
++	}
++
++	return 0;
++}
++EXPORT_SYMBOL(ath10k_ce_alloc_pipe);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/ce.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/ce.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/ce.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -36,6 +36,10 @@ struct ath10k_ce_pipe;
+ 
+ #define CE_DESC_FLAGS_GATHER         (1 << 0)
+ #define CE_DESC_FLAGS_BYTE_SWAP      (1 << 1)
++#define CE_WCN3990_DESC_FLAGS_GATHER BIT(31)
++
++#define CE_DESC_FLAGS_GET_MASK		GENMASK(4, 0)
++#define CE_DESC_37BIT_ADDR_MASK		GENMASK_ULL(37, 0)
+ 
+ /* Following desc flags are used in QCA99X0 */
+ #define CE_DESC_FLAGS_HOST_INT_DIS	(1 << 2)
+@@ -50,6 +54,16 @@ struct ce_desc {
+ 	__le16 flags; /* %CE_DESC_FLAGS_ */
+ };
+ 
++struct ce_desc_64 {
++	__le64 addr;
++	__le16 nbytes; /* length in register map */
++	__le16 flags; /* fw_metadata_high */
++	__le32 toeplitz_hash_result;
++};
++
++#define CE_DESC_SIZE sizeof(struct ce_desc)
++#define CE_DESC_SIZE_64 sizeof(struct ce_desc_64)
++
+ struct ath10k_ce_ring {
+ 	/* Number of entries in this ring; must be power of 2 */
+ 	unsigned int nentries;
+@@ -117,6 +131,7 @@ struct ath10k_ce_pipe {
+ 	unsigned int src_sz_max;
+ 	struct ath10k_ce_ring *src_ring;
+ 	struct ath10k_ce_ring *dest_ring;
++	const struct ath10k_ce_ops *ops;
+ };
+ 
+ /* Copy Engine settable attributes */
+@@ -160,7 +175,7 @@ struct ath10k_ce {
+  */
+ int ath10k_ce_send(struct ath10k_ce_pipe *ce_state,
+ 		   void *per_transfer_send_context,
+-		   u32 buffer,
++		   dma_addr_t buffer,
+ 		   unsigned int nbytes,
+ 		   /* 14 bits */
+ 		   unsigned int transfer_id,
+@@ -168,7 +183,7 @@ int ath10k_ce_send(struct ath10k_ce_pipe
+ 
+ int ath10k_ce_send_nolock(struct ath10k_ce_pipe *ce_state,
+ 			  void *per_transfer_context,
+-			  u32 buffer,
++			  dma_addr_t buffer,
+ 			  unsigned int nbytes,
+ 			  unsigned int transfer_id,
+ 			  unsigned int flags);
+@@ -180,8 +195,8 @@ int ath10k_ce_num_free_src_entries(struc
+ /*==================Recv=======================*/
+ 
+ int __ath10k_ce_rx_num_free_bufs(struct ath10k_ce_pipe *pipe);
+-int __ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx, u32 paddr);
+-int ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx, u32 paddr);
++int ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx,
++			  dma_addr_t paddr);
+ void ath10k_ce_rx_update_write_idx(struct ath10k_ce_pipe *pipe, u32 nentries);
+ 
+ /* recv flags */
+@@ -222,7 +237,7 @@ void ath10k_ce_free_pipe(struct ath10k *
+  */
+ int ath10k_ce_revoke_recv_next(struct ath10k_ce_pipe *ce_state,
+ 			       void **per_transfer_contextp,
+-			       u32 *bufferp);
++			       dma_addr_t *bufferp);
+ 
+ int ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe *ce_state,
+ 					 void **per_transfer_contextp,
+@@ -235,7 +250,7 @@ int ath10k_ce_completed_recv_next_nolock
+  */
+ int ath10k_ce_cancel_send_next(struct ath10k_ce_pipe *ce_state,
+ 			       void **per_transfer_contextp,
+-			       u32 *bufferp,
++			       dma_addr_t *bufferp,
+ 			       unsigned int *nbytesp,
+ 			       unsigned int *transfer_idp);
+ 
+@@ -281,6 +296,32 @@ struct ce_attr {
+ 	void (*recv_cb)(struct ath10k_ce_pipe *);
+ };
+ 
++struct ath10k_ce_ops {
++	struct ath10k_ce_ring *(*ce_alloc_src_ring)(struct ath10k *ar,
++						    u32 ce_id,
++						    const struct ce_attr *attr);
++	struct ath10k_ce_ring *(*ce_alloc_dst_ring)(struct ath10k *ar,
++						    u32 ce_id,
++						    const struct ce_attr *attr);
++	int (*ce_rx_post_buf)(struct ath10k_ce_pipe *pipe, void *ctx,
++			      dma_addr_t paddr);
++	int (*ce_completed_recv_next_nolock)(struct ath10k_ce_pipe *ce_state,
++					     void **per_transfer_contextp,
++					     u32 *nbytesp);
++	int (*ce_revoke_recv_next)(struct ath10k_ce_pipe *ce_state,
++				   void **per_transfer_contextp,
++				   dma_addr_t *nbytesp);
++	void (*ce_extract_desc_data)(struct ath10k *ar,
++				     struct ath10k_ce_ring *src_ring,
++				     u32 sw_index, dma_addr_t *bufferp,
++				     u32 *nbytesp, u32 *transfer_idp);
++	void (*ce_free_pipe)(struct ath10k *ar, int ce_id);
++	int (*ce_send_nolock)(struct ath10k_ce_pipe *pipe,
++			      void *per_transfer_context,
++			      dma_addr_t buffer, u32 nbytes,
++			      u32 transfer_id, u32 flags);
++};
++
+ static inline u32 ath10k_ce_base_address(struct ath10k *ar, unsigned int ce_id)
+ {
+ 	return CE0_BASE_ADDRESS + (CE1_BASE_ADDRESS - CE0_BASE_ADDRESS) * ce_id;
+@@ -292,6 +333,12 @@ static inline u32 ath10k_ce_base_address
+ #define CE_DEST_RING_TO_DESC(baddr, idx) \
+ 	(&(((struct ce_desc *)baddr)[idx]))
+ 
++#define CE_SRC_RING_TO_DESC_64(baddr, idx) \
++	(&(((struct ce_desc_64 *)baddr)[idx]))
++
++#define CE_DEST_RING_TO_DESC_64(baddr, idx) \
++	(&(((struct ce_desc_64 *)baddr)[idx]))
++
+ /* Ring arithmetic (modulus number of entries in ring, which is a pwr of 2). */
+ #define CE_RING_DELTA(nentries_mask, fromidx, toidx) \
+ 	(((int)(toidx) - (int)(fromidx)) & (nentries_mask))
+@@ -308,14 +355,18 @@ static inline u32 ath10k_ce_base_address
+ 	(((x) & CE_WRAPPER_INTERRUPT_SUMMARY_HOST_MSI_MASK) >> \
+ 		CE_WRAPPER_INTERRUPT_SUMMARY_HOST_MSI_LSB)
+ #define CE_WRAPPER_INTERRUPT_SUMMARY_ADDRESS			0x0000
++#define CE_INTERRUPT_SUMMARY		(GENMASK(CE_COUNT_MAX - 1, 0))
+ 
+ static inline u32 ath10k_ce_interrupt_summary(struct ath10k *ar)
+ {
+ 	struct ath10k_ce *ce = ath10k_ce_priv(ar);
+ 
+-	return CE_WRAPPER_INTERRUPT_SUMMARY_HOST_MSI_GET(
+-		ce->bus_ops->read32((ar), CE_WRAPPER_BASE_ADDRESS +
+-		CE_WRAPPER_INTERRUPT_SUMMARY_ADDRESS));
++	if (!ar->hw_params.per_ce_irq)
++		return CE_WRAPPER_INTERRUPT_SUMMARY_HOST_MSI_GET(
++			ce->bus_ops->read32((ar), CE_WRAPPER_BASE_ADDRESS +
++			CE_WRAPPER_INTERRUPT_SUMMARY_ADDRESS));
++	else
++		return CE_INTERRUPT_SUMMARY;
+ }
+ 
+ #endif /* _CE_H_ */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/core.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/core.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/core.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -32,6 +33,7 @@
+ #include "htt.h"
+ #include "testmode.h"
+ #include "wmi-ops.h"
++#include "coredump.h"
+ 
+ unsigned int ath10k_debug_mask;
+ static unsigned int ath10k_cryptmode_param;
+@@ -39,22 +41,31 @@ static bool uart_print;
+ static bool skip_otp;
+ static bool rawmode;
+ 
++/* Enable ATH10K_FW_CRASH_DUMP_REGISTERS and ATH10K_FW_CRASH_DUMP_CE_DATA
++ * by default.
++ */
++unsigned long ath10k_coredump_mask = 0x3;
++
++/* FIXME: most of these should be readonly */
+ module_param_named(debug_mask, ath10k_debug_mask, uint, 0644);
+ module_param_named(cryptmode, ath10k_cryptmode_param, uint, 0644);
+ module_param(uart_print, bool, 0644);
+ module_param(skip_otp, bool, 0644);
+ module_param(rawmode, bool, 0644);
++module_param_named(coredump_mask, ath10k_coredump_mask, ulong, 0444);
+ 
+ MODULE_PARM_DESC(debug_mask, "Debugging mask");
+ MODULE_PARM_DESC(uart_print, "Uart target debugging");
+ MODULE_PARM_DESC(skip_otp, "Skip otp failure for calibration in testmode");
+ MODULE_PARM_DESC(cryptmode, "Crypto mode: 0-hardware, 1-software");
+ MODULE_PARM_DESC(rawmode, "Use raw 802.11 frame datapath");
++MODULE_PARM_DESC(coredump_mask, "Bitfield of what to include in firmware crash file");
+ 
+ static const struct ath10k_hw_params ath10k_hw_params_list[] = {
+ 	{
+ 		.id = QCA988X_HW_2_0_VERSION,
+ 		.dev_id = QCA988X_2_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca988x hw2.0",
+ 		.patch_load_addr = QCA988X_HW_2_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -74,10 +85,47 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++	},
++	{
++		.id = QCA988X_HW_2_0_VERSION,
++		.dev_id = QCA988X_2_0_DEVICE_ID_UBNT,
++		.name = "qca988x hw2.0 ubiquiti",
++		.patch_load_addr = QCA988X_HW_2_0_PATCH_LOAD_ADDR,
++		.uart_pin = 7,
++		.cc_wraparound_type = ATH10K_HW_CC_WRAP_SHIFTED_ALL,
++		.otp_exe_param = 0,
++		.channel_counters_freq_hz = 88000,
++		.max_probe_resp_desc_thres = 0,
++		.cal_data_len = 2116,
++		.fw = {
++			.dir = QCA988X_HW_2_0_FW_DIR,
++			.board = QCA988X_HW_2_0_BOARD_DATA_FILE,
++			.board_size = QCA988X_BOARD_DATA_SZ,
++			.board_ext_size = QCA988X_BOARD_EXT_DATA_SZ,
++		},
++		.hw_ops = &qca988x_ops,
++		.decap_align_bytes = 4,
++		.spectral_bin_discard = 0,
++		.vht160_mcs_rx_highest = 0,
++		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA9887_HW_1_0_VERSION,
+ 		.dev_id = QCA9887_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca9887 hw1.0",
+ 		.patch_load_addr = QCA9887_HW_1_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -97,10 +145,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA6174_HW_2_1_VERSION,
+ 		.dev_id = QCA6164_2_1_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca6164 hw2.1",
+ 		.patch_load_addr = QCA6174_HW_2_1_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -119,10 +175,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA6174_HW_2_1_VERSION,
+ 		.dev_id = QCA6174_2_1_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca6174 hw2.1",
+ 		.patch_load_addr = QCA6174_HW_2_1_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -141,10 +205,45 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
++	},
++	{
++		.id = QCA6174_HW_3_2_VERSION,
++		.dev_id = QCA9377_1_0_DEVICE_ID,
++		.name = "qca6174 hw3.2 sdio",
++		.patch_load_addr = QCA6174_HW_3_0_PATCH_LOAD_ADDR,
++		.uart_pin = 19,
++		.otp_exe_param = 0,
++		.channel_counters_freq_hz = 88000,
++		.max_probe_resp_desc_thres = 0,
++		.cal_data_len = 8124,
++		.fw = {
++			/* uses same binaries as hw3.0 */
++			.dir = QCA6174_HW_3_0_FW_DIR,
++			.board = QCA6174_HW_3_0_BOARD_DATA_FILE_SDIO,
++			.board_size = QCA6174_BOARD_DATA_SZ,
++			.board_ext_size = QCA6174_BOARD_EXT_DATA_SZ,
++		},
++		.hw_ops = &qca6174_ops,
++		.hw_clk = qca6174_clk,
++		.target_cpu_freq = 176000000,
++		.decap_align_bytes = 4,
++		.spectral_bin_discard = 0,
++		.bus = ATH10K_BUS_SDIO,
++		.start_once = true,
++		.vht160_mcs_rx_highest = 0,
++		.vht160_mcs_tx_highest = 0,
+ 	},
+ 	{
+ 		.id = QCA6174_HW_3_0_VERSION,
+ 		.dev_id = QCA6174_2_1_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca6174 hw3.0",
+ 		.patch_load_addr = QCA6174_HW_3_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -163,10 +262,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA6174_HW_3_2_VERSION,
+ 		.dev_id = QCA6174_2_1_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca6174 hw3.2",
+ 		.patch_load_addr = QCA6174_HW_3_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -188,10 +295,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA99X0_HW_2_0_DEV_VERSION,
+ 		.dev_id = QCA99X0_2_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca99x0 hw2.0",
+ 		.patch_load_addr = QCA99X0_HW_2_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -216,10 +331,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 4,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 11,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA9984_HW_1_0_DEV_VERSION,
+ 		.dev_id = QCA9984_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca9984/qca9994 hw1.0",
+ 		.patch_load_addr = QCA9984_HW_1_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -249,10 +372,18 @@ static const struct ath10k_hw_params ath
+ 		 */
+ 		.vht160_mcs_rx_highest = 1560,
+ 		.vht160_mcs_tx_highest = 1560,
++		.n_cipher_suites = 11,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA9888_HW_2_0_DEV_VERSION,
+ 		.dev_id = QCA9888_2_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca9888 hw2.0",
+ 		.patch_load_addr = QCA9888_HW_2_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -281,10 +412,18 @@ static const struct ath10k_hw_params ath
+ 		 */
+ 		.vht160_mcs_rx_highest = 780,
+ 		.vht160_mcs_tx_highest = 780,
++		.n_cipher_suites = 11,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA9377_HW_1_0_DEV_VERSION,
+ 		.dev_id = QCA9377_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca9377 hw1.0",
+ 		.patch_load_addr = QCA9377_HW_1_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -303,10 +442,18 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
+ 	},
+ 	{
+ 		.id = QCA9377_HW_1_1_DEV_VERSION,
+ 		.dev_id = QCA9377_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_PCI,
+ 		.name = "qca9377 hw1.1",
+ 		.patch_load_addr = QCA9377_HW_1_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 6,
+@@ -327,10 +474,92 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 0,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 8,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
++	},
++	{
++		.id = QCA9377_HW_1_1_DEV_VERSION,
++		.dev_id = QCA9377_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_USB,
++		.name = "qca9377 hw1.1 usb",
++		.patch_load_addr = QCA9377_HW_1_0_PATCH_LOAD_ADDR,
++		.uart_pin = 6,
++		.otp_exe_param = 0,
++		.channel_counters_freq_hz = 88000,
++		.max_probe_resp_desc_thres = 0,
++		.cal_data_len = 8124,
++		.fw = {
++			.dir = QCA9377_HW_1_0_FW_DIR,
++			.board = QCA9377_HW_1_0_BOARD_DATA_FILE_USB,
++			.board_size = QCA9377_BOARD_DATA_SZ,
++			.board_ext_size = QCA9377_BOARD_EXT_DATA_SZ,
++		},
++		.hw_ops = &qca988x_ops,
++		.decap_align_bytes = 4,
++		.start_once = true,
++		.num_peers = TARGET_QCA9377_HL_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++	},
++	{
++		.id = QCA9377_HW_1_1_DEV_VERSION,
++		.dev_id = SPARKLAN_WPEQ_160N_DEVICE_ID,
++		.bus = ATH10K_BUS_USB,
++		.name = "qca9377 hw1.1 SparkLAN WPEQ-160ACN",
++		.patch_load_addr = QCA9377_HW_1_0_PATCH_LOAD_ADDR,
++		.uart_pin = 6,
++		.otp_exe_param = 0,
++		.channel_counters_freq_hz = 88000,
++		.max_probe_resp_desc_thres = 0,
++		.cal_data_len = 8124,
++		.fw = {
++			.dir = QCA9377_HW_1_0_FW_DIR,
++			.board = QCA9377_HW_1_0_BOARD_DATA_FILE_USB,
++			.board_size = QCA9377_BOARD_DATA_SZ,
++			.board_ext_size = QCA9377_BOARD_EXT_DATA_SZ,
++		},
++		.hw_ops = &qca988x_ops,
++		.decap_align_bytes = 4,
++		.start_once = true,
++		.num_peers = TARGET_QCA9377_HL_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++	},
++	{
++		.id = QCA9377_HW_1_1_DEV_VERSION,
++		.dev_id = QCA9377_1_0_DEVICE_ID,
++		.bus = ATH10K_BUS_SDIO,
++		.name = "qca9377 hw1.1 sdio",
++		.patch_load_addr = QCA9377_HW_1_0_PATCH_LOAD_ADDR,
++		.uart_pin = 19,
++		.otp_exe_param = 0,
++		.channel_counters_freq_hz = 88000,
++		.max_probe_resp_desc_thres = 0,
++		.cal_data_len = 8124,
++		.fw = {
++			.dir = QCA9377_HW_1_0_FW_DIR,
++			.board = QCA9377_HW_1_0_BOARD_DATA_FILE_SDIO,
++			.board_size = QCA9377_BOARD_DATA_SZ,
++			.board_ext_size = QCA9377_BOARD_EXT_DATA_SZ,
++		},
++		.hw_ops = &qca6174_ops,
++		.hw_clk = qca6174_clk,
++		.target_cpu_freq = 176000000,
++		.decap_align_bytes = 4,
++		.start_once = true,
++		.num_peers = TARGET_QCA9377_HL_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
+ 	},
+ 	{
+ 		.id = QCA4019_HW_1_0_DEV_VERSION,
+ 		.dev_id = 0,
++		.bus = ATH10K_BUS_AHB,
+ 		.name = "qca4019 hw1.0",
+ 		.patch_load_addr = QCA4019_HW_1_0_PATCH_LOAD_ADDR,
+ 		.uart_pin = 7,
+@@ -356,6 +585,35 @@ static const struct ath10k_hw_params ath
+ 		.spectral_bin_discard = 4,
+ 		.vht160_mcs_rx_highest = 0,
+ 		.vht160_mcs_tx_highest = 0,
++		.n_cipher_suites = 11,
++		.num_peers = TARGET_TLV_NUM_PEERS,
++		.ast_skid_limit = 0x10,
++		.num_wds_entries = 0x20,
++		.target_64bit = false,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL,
++		.per_ce_irq = false,
++	},
++	{
++		.id = WCN3990_HW_1_0_DEV_VERSION,
++		.dev_id = 0,
++		.bus = ATH10K_BUS_PCI,
++		.name = "wcn3990 hw1.0",
++		.continuous_frag_desc = true,
++		.tx_chain_mask = 0x7,
++		.rx_chain_mask = 0x7,
++		.max_spatial_stream = 4,
++		.fw = {
++			.dir = WCN3990_HW_1_0_FW_DIR,
++		},
++		.sw_decrypt_mcast_mgmt = true,
++		.hw_ops = &wcn3990_ops,
++		.decap_align_bytes = 1,
++		.num_peers = TARGET_HL_10_TLV_NUM_PEERS,
++		.ast_skid_limit = TARGET_HL_10_TLV_AST_SKID_LIMIT,
++		.num_wds_entries = TARGET_HL_10_TLV_NUM_WDS_ENTRIES,
++		.target_64bit = true,
++		.rx_ring_fill_level = HTT_RX_RING_FILL_LEVEL_DUAL_MAC,
++		.per_ce_irq = true,
+ 	},
+ };
+ 
+@@ -377,6 +635,9 @@ static const char *const ath10k_core_fw_
+ 	[ATH10K_FW_FEATURE_BTCOEX_PARAM] = "btcoex-param",
+ 	[ATH10K_FW_FEATURE_SKIP_NULL_FUNC_WAR] = "skip-null-func-war",
+ 	[ATH10K_FW_FEATURE_ALLOWS_MESH_BCAST] = "allows-mesh-bcast",
++	[ATH10K_FW_FEATURE_NO_PS] = "no-ps",
++	[ATH10K_FW_FEATURE_MGMT_TX_BY_REF] = "mgmt-tx-by-reference",
++	[ATH10K_FW_FEATURE_NON_BMI] = "non-bmi",
+ };
+ 
+ static unsigned int ath10k_core_get_fw_feature_str(char *buf,
+@@ -429,11 +690,25 @@ static void ath10k_init_sdio(struct ath1
+ 	ath10k_bmi_write32(ar, hi_mbox_isr_yield_limit, 99);
+ 	ath10k_bmi_read32(ar, hi_acs_flags, &param);
+ 
+-	param |= (HI_ACS_FLAGS_SDIO_SWAP_MAILBOX_SET |
+-		  HI_ACS_FLAGS_SDIO_REDUCE_TX_COMPL_SET |
+-		  HI_ACS_FLAGS_ALT_DATA_CREDIT_SIZE);
++	/* Data transfer is not initiated, when reduced Tx completion
++	 * is used for SDIO. disable it until fixed
++	 */
++	param &= ~HI_ACS_FLAGS_SDIO_REDUCE_TX_COMPL_SET;
++
++	/* Alternate credit size of 1544 as used by SDIO firmware is
++	 * not big enough for mac80211 / native wifi frames. disable it
++	 */
++	param &= ~HI_ACS_FLAGS_ALT_DATA_CREDIT_SIZE;
+ 
++	param |= HI_ACS_FLAGS_SDIO_SWAP_MAILBOX_SET;
+ 	ath10k_bmi_write32(ar, hi_acs_flags, param);
++
++	/* Explicitly set fwlog prints to zero as target may turn it on
++	 * based on scratch registers.
++	 */
++	ath10k_bmi_read32(ar, hi_option_flag, &param);
++	param |= HI_OPTION_DISABLE_DBGLOG;
++	ath10k_bmi_write32(ar, hi_option_flag, param);
+ }
+ 
+ static int ath10k_init_configure_target(struct ath10k *ar)
+@@ -847,6 +1122,28 @@ static int ath10k_core_check_smbios(stru
+ 	return 0;
+ }
+ 
++static int ath10k_core_check_dt(struct ath10k *ar)
++{
++	struct device_node *node;
++	const char *variant = NULL;
++
++	node = ar->dev->of_node;
++	if (!node)
++		return -ENOENT;
++
++	of_property_read_string(node, "qcom,ath10k-calibration-variant",
++				&variant);
++	if (!variant)
++		return -ENODATA;
++
++	if (strscpy(ar->id.bdf_ext, variant, sizeof(ar->id.bdf_ext)) < 0)
++		ath10k_dbg(ar, ATH10K_DBG_BOOT,
++			   "bdf variant string is longer than the buffer can accommodate (variant: %s)\n",
++			    variant);
++
++	return 0;
++}
++
+ static int ath10k_download_and_run_otp(struct ath10k *ar)
+ {
+ 	u32 result, address = ar->hw_params.patch_load_addr;
+@@ -1097,14 +1394,61 @@ out:
+ 	return ret;
+ }
+ 
++static int ath10k_core_search_bd(struct ath10k *ar,
++				 const char *boardname,
++				 const u8 *data,
++				 size_t len)
++{
++	size_t ie_len;
++	struct ath10k_fw_ie *hdr;
++	int ret = -ENOENT, ie_id;
++
++	while (len > sizeof(struct ath10k_fw_ie)) {
++		hdr = (struct ath10k_fw_ie *)data;
++		ie_id = le32_to_cpu(hdr->id);
++		ie_len = le32_to_cpu(hdr->len);
++
++		len -= sizeof(*hdr);
++		data = hdr->data;
++
++		if (len < ALIGN(ie_len, 4)) {
++			ath10k_err(ar, "invalid length for board ie_id %d ie_len %zu len %zu\n",
++				   ie_id, ie_len, len);
++			return -EINVAL;
++		}
++
++		switch (ie_id) {
++		case ATH10K_BD_IE_BOARD:
++			ret = ath10k_core_parse_bd_ie_board(ar, data, ie_len,
++							    boardname);
++			if (ret == -ENOENT)
++				/* no match found, continue */
++				break;
++
++			/* either found or error, so stop searching */
++			goto out;
++		}
++
++		/* jump over the padding */
++		ie_len = ALIGN(ie_len, 4);
++
++		len -= ie_len;
++		data += ie_len;
++	}
++
++out:
++	/* return result of parse_bd_ie_board() or -ENOENT */
++	return ret;
++}
++
+ static int ath10k_core_fetch_board_data_api_n(struct ath10k *ar,
+ 					      const char *boardname,
++					      const char *fallback_boardname,
+ 					      const char *filename)
+ {
+-	size_t len, magic_len, ie_len;
+-	struct ath10k_fw_ie *hdr;
++	size_t len, magic_len;
+ 	const u8 *data;
+-	int ret, ie_id;
++	int ret;
+ 
+ 	ar->normal_mode_fw.board = ath10k_fetch_fw_file(ar,
+ 							ar->hw_params.fw.dir,
+@@ -1142,69 +1486,23 @@ static int ath10k_core_fetch_board_data_
+ 	data += magic_len;
+ 	len -= magic_len;
+ 
+-	while (len > sizeof(struct ath10k_fw_ie)) {
+-		hdr = (struct ath10k_fw_ie *)data;
+-		ie_id = le32_to_cpu(hdr->id);
+-		ie_len = le32_to_cpu(hdr->len);
+-
+-		len -= sizeof(*hdr);
+-		data = hdr->data;
+-
+-		if (len < ALIGN(ie_len, 4)) {
+-			ath10k_err(ar, "invalid length for board ie_id %d ie_len %zu len %zu\n",
+-				   ie_id, ie_len, len);
+-			ret = -EINVAL;
+-			goto err;
+-		}
+-
+-		switch (ie_id) {
+-		case ATH10K_BD_IE_BOARD:
+-			ret = ath10k_core_parse_bd_ie_board(ar, data, ie_len,
+-							    boardname);
+-			if (ret == -ENOENT && ar->id.bdf_ext[0] != '\0') {
+-				/* try default bdf if variant was not found */
+-				char *s, *v = ",variant=";
+-				char boardname2[100];
+-
+-				strlcpy(boardname2, boardname,
+-					sizeof(boardname2));
+-
+-				s = strstr(boardname2, v);
+-				if (s)
+-					*s = '\0';  /* strip ",variant=%s" */
+-
+-				ret = ath10k_core_parse_bd_ie_board(ar, data,
+-								    ie_len,
+-								    boardname2);
+-			}
+-
+-			if (ret == -ENOENT)
+-				/* no match found, continue */
+-				break;
+-			else if (ret)
+-				/* there was an error, bail out */
+-				goto err;
+-
+-			/* board data found */
+-			goto out;
+-		}
++	/* attempt to find boardname in the IE list */
++	ret = ath10k_core_search_bd(ar, boardname, data, len);
+ 
+-		/* jump over the padding */
+-		ie_len = ALIGN(ie_len, 4);
++	/* if we didn't find it and have a fallback name, try that */
++	if (ret == -ENOENT && fallback_boardname)
++		ret = ath10k_core_search_bd(ar, fallback_boardname, data, len);
+ 
+-		len -= ie_len;
+-		data += ie_len;
+-	}
+-
+-out:
+-	if (!ar->normal_mode_fw.board_data || !ar->normal_mode_fw.board_len) {
++	if (ret == -ENOENT) {
+ 		ath10k_err(ar,
+ 			   "failed to fetch board data for %s from %s/%s\n",
+ 			   boardname, ar->hw_params.fw.dir, filename);
+ 		ret = -ENODATA;
+-		goto err;
+ 	}
+ 
++	if (ret)
++		goto err;
++
+ 	return 0;
+ 
+ err:
+@@ -1213,24 +1511,24 @@ err:
+ }
+ 
+ static int ath10k_core_create_board_name(struct ath10k *ar, char *name,
+-					 size_t name_len)
++					 size_t name_len, bool with_variant)
+ {
+ 	/* strlen(',variant=') + strlen(ar->id.bdf_ext) */
+ 	char variant[9 + ATH10K_SMBIOS_BDF_EXT_STR_LENGTH] = { 0 };
+ 
++	if (with_variant && ar->id.bdf_ext[0] != '\0')
++		scnprintf(variant, sizeof(variant), ",variant=%s",
++			  ar->id.bdf_ext);
++
+ 	if (ar->id.bmi_ids_valid) {
+ 		scnprintf(name, name_len,
+-			  "bus=%s,bmi-chip-id=%d,bmi-board-id=%d",
++			  "bus=%s,bmi-chip-id=%d,bmi-board-id=%d%s",
+ 			  ath10k_bus_str(ar->hif.bus),
+ 			  ar->id.bmi_chip_id,
+-			  ar->id.bmi_board_id);
++			  ar->id.bmi_board_id, variant);
+ 		goto out;
+ 	}
+ 
+-	if (ar->id.bdf_ext[0] != '\0')
+-		scnprintf(variant, sizeof(variant), ",variant=%s",
+-			  ar->id.bdf_ext);
+-
+ 	scnprintf(name, name_len,
+ 		  "bus=%s,vendor=%04x,device=%04x,subsystem-vendor=%04x,subsystem-device=%04x%s",
+ 		  ath10k_bus_str(ar->hif.bus),
+@@ -1244,17 +1542,26 @@ out:
+ 
+ static int ath10k_core_fetch_board_file(struct ath10k *ar)
+ {
+-	char boardname[100];
++	char boardname[100], fallback_boardname[100];
+ 	int ret;
+ 
+-	ret = ath10k_core_create_board_name(ar, boardname, sizeof(boardname));
++	ret = ath10k_core_create_board_name(ar, boardname,
++					    sizeof(boardname), true);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to create board name: %d", ret);
+ 		return ret;
+ 	}
+ 
++	ret = ath10k_core_create_board_name(ar, fallback_boardname,
++					    sizeof(boardname), false);
++	if (ret) {
++		ath10k_err(ar, "failed to create fallback board name: %d", ret);
++		return ret;
++	}
++
+ 	ar->bd_api = 2;
+ 	ret = ath10k_core_fetch_board_data_api_n(ar, boardname,
++						 fallback_boardname,
+ 						 ATH10K_BOARD_API2_FILE);
+ 	if (!ret)
+ 		goto success;
+@@ -1434,8 +1741,8 @@ int ath10k_core_fetch_firmware_api_n(str
+ 		data += ie_len;
+ 	}
+ 
+-	if (!fw_file->firmware_data ||
+-	    !fw_file->firmware_len) {
++	if (!test_bit(ATH10K_FW_FEATURE_NON_BMI, fw_file->fw_features) &&
++	    (!fw_file->firmware_data || !fw_file->firmware_len)) {
+ 		ath10k_warn(ar, "No ATH10K_FW_IE_FW_IMAGE found from '%s/%s', skipping\n",
+ 			    ar->hw_params.fw.dir, name);
+ 		ret = -ENOMEDIUM;
+@@ -1461,6 +1768,7 @@ static void ath10k_core_get_fw_name(stru
+ 		break;
+ 	case ATH10K_BUS_PCI:
+ 	case ATH10K_BUS_AHB:
++	case ATH10K_BUS_SNOC:
+ 		scnprintf(fw_name, fw_name_len, "%s-%d.bin",
+ 			  ATH10K_FW_FILE_BASE, fw_api);
+ 		break;
+@@ -1629,8 +1937,15 @@ static int ath10k_init_uart(struct ath10
+ 		return ret;
+ 	}
+ 
+-	if (!uart_print)
++	if (!uart_print) {
++		/* Hack: override dbg TX pin to avoid side effects of default
++		 * GPIO_6 in QCA9377 WB396 reference card
++		 */
++		if (ar->hif.bus == ATH10K_BUS_SDIO)
++			ath10k_bmi_write32(ar, hi_dbg_uart_txpin,
++					   ar->hw_params.uart_pin);
+ 		return 0;
++	}
+ 
+ 	ret = ath10k_bmi_write32(ar, hi_dbg_uart_txpin, ar->hw_params.uart_pin);
+ 	if (ret) {
+@@ -1663,14 +1978,15 @@ static int ath10k_init_hw_params(struct
+ 	for (i = 0; i < ARRAY_SIZE(ath10k_hw_params_list); i++) {
+ 		hw_params = &ath10k_hw_params_list[i];
+ 
+-		if (hw_params->id == ar->target_version &&
++		if (hw_params->bus == ar->hif.bus &&
++		    hw_params->id == ar->target_version &&
+ 		    hw_params->dev_id == ar->dev_id)
+ 			break;
+ 	}
+ 
+ 	if (i == ARRAY_SIZE(ath10k_hw_params_list)) {
+-		ath10k_err(ar, "Unsupported hardware version: 0x%x\n",
+-			   ar->target_version);
++		ath10k_err(ar, "Unsupported hardware version: 0x%x %x %x\n",
++			   ar->target_version, ar->hif.bus, ar->dev_id);
+ 		return -EINVAL;
+ 	}
+ 
+@@ -1746,7 +2062,7 @@ static void ath10k_core_restart(struct w
+ 
+ 	mutex_unlock(&ar->conf_mutex);
+ 
+-	ret = ath10k_debug_fw_devcoredump(ar);
++	ret = ath10k_coredump_submit(ar);
+ 	if (ret)
+ 		ath10k_warn(ar, "failed to send firmware crash dump via devcoredump: %d",
+ 			    ret);
+@@ -1764,6 +2080,7 @@ static void ath10k_core_set_coverage_cla
+ static int ath10k_core_init_firmware_features(struct ath10k *ar)
+ {
+ 	struct ath10k_fw_file *fw_file = &ar->normal_mode_fw.fw_file;
++	int max_num_peers;
+ 
+ 	if (test_bit(ATH10K_FW_FEATURE_WMI_10_2, fw_file->fw_features) &&
+ 	    !test_bit(ATH10K_FW_FEATURE_WMI_10X, fw_file->fw_features)) {
+@@ -1843,7 +2160,7 @@ static int ath10k_core_init_firmware_fea
+ 
+ 	switch (fw_file->wmi_op_version) {
+ 	case ATH10K_FW_WMI_OP_VERSION_MAIN:
+-		ar->max_num_peers = TARGET_NUM_PEERS;
++		max_num_peers = TARGET_NUM_PEERS;
+ 		ar->max_num_stations = TARGET_NUM_STATIONS;
+ 		ar->max_num_vdevs = TARGET_NUM_VDEVS;
+ 		ar->htt.max_num_pending_tx = TARGET_NUM_MSDU_DESC;
+@@ -1855,10 +2172,10 @@ static int ath10k_core_init_firmware_fea
+ 	case ATH10K_FW_WMI_OP_VERSION_10_2:
+ 	case ATH10K_FW_WMI_OP_VERSION_10_2_4:
+ 		if (ath10k_peer_stats_enabled(ar)) {
+-			ar->max_num_peers = TARGET_10X_TX_STATS_NUM_PEERS;
++			max_num_peers = TARGET_10X_TX_STATS_NUM_PEERS;
+ 			ar->max_num_stations = TARGET_10X_TX_STATS_NUM_STATIONS;
+ 		} else {
+-			ar->max_num_peers = TARGET_10X_NUM_PEERS;
++			max_num_peers = TARGET_10X_NUM_PEERS;
+ 			ar->max_num_stations = TARGET_10X_NUM_STATIONS;
+ 		}
+ 		ar->max_num_vdevs = TARGET_10X_NUM_VDEVS;
+@@ -1867,24 +2184,29 @@ static int ath10k_core_init_firmware_fea
+ 		ar->max_spatial_stream = WMI_MAX_SPATIAL_STREAM;
+ 		break;
+ 	case ATH10K_FW_WMI_OP_VERSION_TLV:
+-		ar->max_num_peers = TARGET_TLV_NUM_PEERS;
++		max_num_peers = TARGET_TLV_NUM_PEERS;
+ 		ar->max_num_stations = TARGET_TLV_NUM_STATIONS;
+ 		ar->max_num_vdevs = TARGET_TLV_NUM_VDEVS;
+ 		ar->max_num_tdls_vdevs = TARGET_TLV_NUM_TDLS_VDEVS;
+-		ar->htt.max_num_pending_tx = TARGET_TLV_NUM_MSDU_DESC;
++		if (ar->hif.bus == ATH10K_BUS_SDIO)
++			ar->htt.max_num_pending_tx =
++				TARGET_TLV_NUM_MSDU_DESC_HL;
++		else
++			ar->htt.max_num_pending_tx = TARGET_TLV_NUM_MSDU_DESC;
+ 		ar->wow.max_num_patterns = TARGET_TLV_NUM_WOW_PATTERNS;
+ 		ar->fw_stats_req_mask = WMI_STAT_PDEV | WMI_STAT_VDEV |
+ 			WMI_STAT_PEER;
+ 		ar->max_spatial_stream = WMI_MAX_SPATIAL_STREAM;
+ 		break;
+ 	case ATH10K_FW_WMI_OP_VERSION_10_4:
+-		ar->max_num_peers = TARGET_10_4_NUM_PEERS;
++		max_num_peers = TARGET_10_4_NUM_PEERS;
+ 		ar->max_num_stations = TARGET_10_4_NUM_STATIONS;
+ 		ar->num_active_peers = TARGET_10_4_ACTIVE_PEERS;
+ 		ar->max_num_vdevs = TARGET_10_4_NUM_VDEVS;
+ 		ar->num_tids = TARGET_10_4_TGT_NUM_TIDS;
+ 		ar->fw_stats_req_mask = WMI_10_4_STAT_PEER |
+-					WMI_10_4_STAT_PEER_EXTD;
++					WMI_10_4_STAT_PEER_EXTD |
++					WMI_10_4_STAT_VDEV_EXTD;
+ 		ar->max_spatial_stream = ar->hw_params.max_spatial_stream;
+ 		ar->max_num_tdls_vdevs = TARGET_10_4_NUM_TDLS_VDEVS;
+ 
+@@ -1896,10 +2218,16 @@ static int ath10k_core_init_firmware_fea
+ 		break;
+ 	case ATH10K_FW_WMI_OP_VERSION_UNSET:
+ 	case ATH10K_FW_WMI_OP_VERSION_MAX:
++	default:
+ 		WARN_ON(1);
+ 		return -EINVAL;
+ 	}
+ 
++	if (ar->hw_params.num_peers)
++		ar->max_num_peers = ar->hw_params.num_peers;
++	else
++		ar->max_num_peers = max_num_peers;
++
+ 	/* Backwards compatibility for firmwares without
+ 	 * ATH10K_FW_IE_HTT_OP_VERSION.
+ 	 */
+@@ -1982,49 +2310,56 @@ int ath10k_core_start(struct ath10k *ar,
+ 	int status;
+ 	u32 val;
+ 
++	if (ar->is_started && ar->hw_params.start_once)
++		return 0;
++
+ 	lockdep_assert_held(&ar->conf_mutex);
+ 
+ 	clear_bit(ATH10K_FLAG_CRASH_FLUSH, &ar->dev_flags);
+ 
+ 	ar->running_fw = fw;
+ 
+-	ath10k_bmi_start(ar);
+-
+-	if (ath10k_init_configure_target(ar)) {
+-		status = -EINVAL;
+-		goto err;
+-	}
++	if (!test_bit(ATH10K_FW_FEATURE_NON_BMI,
++		      ar->running_fw->fw_file.fw_features)) {
++		ath10k_bmi_start(ar);
+ 
+-	status = ath10k_download_cal_data(ar);
+-	if (status)
+-		goto err;
++		if (ath10k_init_configure_target(ar)) {
++			status = -EINVAL;
++			goto err;
++		}
+ 
+-	/* Some of of qca988x solutions are having global reset issue
+-	 * during target initialization. Bypassing PLL setting before
+-	 * downloading firmware and letting the SoC run on REF_CLK is
+-	 * fixing the problem. Corresponding firmware change is also needed
+-	 * to set the clock source once the target is initialized.
+-	 */
+-	if (test_bit(ATH10K_FW_FEATURE_SUPPORTS_SKIP_CLOCK_INIT,
+-		     ar->running_fw->fw_file.fw_features)) {
+-		status = ath10k_bmi_write32(ar, hi_skip_clock_init, 1);
+-		if (status) {
+-			ath10k_err(ar, "could not write to skip_clock_init: %d\n",
+-				   status);
++		status = ath10k_download_cal_data(ar);
++		if (status)
+ 			goto err;
++
++		/* Some of of qca988x solutions are having global reset issue
++		 * during target initialization. Bypassing PLL setting before
++		 * downloading firmware and letting the SoC run on REF_CLK is
++		 * fixing the problem. Corresponding firmware change is also
++		 * needed to set the clock source once the target is
++		 * initialized.
++		 */
++		if (test_bit(ATH10K_FW_FEATURE_SUPPORTS_SKIP_CLOCK_INIT,
++			     ar->running_fw->fw_file.fw_features)) {
++			status = ath10k_bmi_write32(ar, hi_skip_clock_init, 1);
++			if (status) {
++				ath10k_err(ar, "could not write to skip_clock_init: %d\n",
++					   status);
++				goto err;
++			}
+ 		}
+-	}
+ 
+-	status = ath10k_download_fw(ar);
+-	if (status)
+-		goto err;
++		status = ath10k_download_fw(ar);
++		if (status)
++			goto err;
+ 
+-	status = ath10k_init_uart(ar);
+-	if (status)
+-		goto err;
++		status = ath10k_init_uart(ar);
++		if (status)
++			goto err;
+ 
+-	if (ar->hif.bus == ATH10K_BUS_SDIO)
+-		ath10k_init_sdio(ar);
++		if (ar->hif.bus == ATH10K_BUS_SDIO)
++			ath10k_init_sdio(ar);
++	}
+ 
+ 	ar->htc.htc_ops.target_send_suspend_complete =
+ 		ath10k_send_suspend_complete;
+@@ -2035,9 +2370,12 @@ int ath10k_core_start(struct ath10k *ar,
+ 		goto err;
+ 	}
+ 
+-	status = ath10k_bmi_done(ar);
+-	if (status)
+-		goto err;
++	if (!test_bit(ATH10K_FW_FEATURE_NON_BMI,
++		      ar->running_fw->fw_file.fw_features)) {
++		status = ath10k_bmi_done(ar);
++		if (status)
++			goto err;
++	}
+ 
+ 	status = ath10k_wmi_attach(ar);
+ 	if (status) {
+@@ -2118,6 +2456,9 @@ int ath10k_core_start(struct ath10k *ar,
+ 		if (ath10k_peer_stats_enabled(ar))
+ 			val = WMI_10_4_PEER_STATS;
+ 
++		/* Enable vdev stats by default */
++		val |= WMI_10_4_VDEV_STATS;
++
+ 		if (test_bit(WMI_SERVICE_BSS_CHANNEL_INFO_64, ar->wmi.svc_map))
+ 			val |= WMI_10_4_BSS_CHANNEL_INFO_64;
+ 
+@@ -2209,6 +2550,7 @@ int ath10k_core_start(struct ath10k *ar,
+ 	if (status)
+ 		goto err_hif_stop;
+ 
++	ar->is_started = true;
+ 	return 0;
+ 
+ err_hif_stop:
+@@ -2261,6 +2603,7 @@ void ath10k_core_stop(struct ath10k *ar)
+ 	ath10k_htt_tx_stop(&ar->htt);
+ 	ath10k_htt_rx_free(&ar->htt);
+ 	ath10k_wmi_detach(ar);
++	ar->is_started = false;
+ }
+ EXPORT_SYMBOL(ath10k_core_stop);
+ 
+@@ -2276,23 +2619,47 @@ static int ath10k_core_probe_fw(struct a
+ 
+ 	ret = ath10k_hif_power_up(ar);
+ 	if (ret) {
+-		ath10k_err(ar, "could not start pci hif (%d)\n", ret);
++		ath10k_err(ar, "could not power on hif bus (%d)\n", ret);
+ 		return ret;
+ 	}
+ 
+-	memset(&target_info, 0, sizeof(target_info));
+-	if (ar->hif.bus == ATH10K_BUS_SDIO)
++	switch (ar->hif.bus) {
++	case ATH10K_BUS_SDIO:
++		memset(&target_info, 0, sizeof(target_info));
+ 		ret = ath10k_bmi_get_target_info_sdio(ar, &target_info);
+-	else
++		if (ret) {
++			ath10k_err(ar, "could not get target info (%d)\n", ret);
++			goto err_power_down;
++		}
++		ar->target_version = target_info.version;
++		ar->hw->wiphy->hw_version = target_info.version;
++		break;
++	case ATH10K_BUS_PCI:
++	case ATH10K_BUS_AHB:
++	case ATH10K_BUS_USB:
++		memset(&target_info, 0, sizeof(target_info));
+ 		ret = ath10k_bmi_get_target_info(ar, &target_info);
+-	if (ret) {
+-		ath10k_err(ar, "could not get target info (%d)\n", ret);
+-		goto err_power_down;
++		if (ret) {
++			ath10k_err(ar, "could not get target info (%d)\n", ret);
++			goto err_power_down;
++		}
++		ar->target_version = target_info.version;
++		ar->hw->wiphy->hw_version = target_info.version;
++		break;
++	case ATH10K_BUS_SNOC:
++		memset(&target_info, 0, sizeof(target_info));
++		ret = ath10k_hif_get_target_info(ar, &target_info);
++		if (ret) {
++			ath10k_err(ar, "could not get target info (%d)\n", ret);
++			goto err_power_down;
++		}
++		ar->target_version = target_info.version;
++		ar->hw->wiphy->hw_version = target_info.version;
++		break;
++	default:
++		ath10k_err(ar, "incorrect hif bus type: %d\n", ar->hif.bus);
+ 	}
+ 
+-	ar->target_version = target_info.version;
+-	ar->hw->wiphy->hw_version = target_info.version;
+-
+ 	ret = ath10k_init_hw_params(ar);
+ 	if (ret) {
+ 		ath10k_err(ar, "could not get hw params (%d)\n", ret);
+@@ -2312,34 +2679,41 @@ static int ath10k_core_probe_fw(struct a
+ 
+ 	ath10k_debug_print_hwfw_info(ar);
+ 
+-	ret = ath10k_core_pre_cal_download(ar);
+-	if (ret) {
+-		/* pre calibration data download is not necessary
+-		 * for all the chipsets. Ignore failures and continue.
+-		 */
+-		ath10k_dbg(ar, ATH10K_DBG_BOOT,
+-			   "could not load pre cal data: %d\n", ret);
+-	}
++	if (!test_bit(ATH10K_FW_FEATURE_NON_BMI,
++		      ar->normal_mode_fw.fw_file.fw_features)) {
++		ret = ath10k_core_pre_cal_download(ar);
++		if (ret) {
++			/* pre calibration data download is not necessary
++			 * for all the chipsets. Ignore failures and continue.
++			 */
++			ath10k_dbg(ar, ATH10K_DBG_BOOT,
++				   "could not load pre cal data: %d\n", ret);
++		}
+ 
+-	ret = ath10k_core_get_board_id_from_otp(ar);
+-	if (ret && ret != -EOPNOTSUPP) {
+-		ath10k_err(ar, "failed to get board id from otp: %d\n",
+-			   ret);
+-		goto err_free_firmware_files;
+-	}
++		ret = ath10k_core_get_board_id_from_otp(ar);
++		if (ret && ret != -EOPNOTSUPP) {
++			ath10k_err(ar, "failed to get board id from otp: %d\n",
++				   ret);
++			goto err_free_firmware_files;
++		}
+ 
+-	ret = ath10k_core_check_smbios(ar);
+-	if (ret)
+-		ath10k_dbg(ar, ATH10K_DBG_BOOT, "bdf variant name not set.\n");
++		ret = ath10k_core_check_smbios(ar);
++		if (ret)
++			ath10k_dbg(ar, ATH10K_DBG_BOOT, "SMBIOS bdf variant name not set.\n");
++
++		ret = ath10k_core_check_dt(ar);
++		if (ret)
++			ath10k_dbg(ar, ATH10K_DBG_BOOT, "DT bdf variant name not set.\n");
++
++		ret = ath10k_core_fetch_board_file(ar);
++		if (ret) {
++			ath10k_err(ar, "failed to fetch board file: %d\n", ret);
++			goto err_free_firmware_files;
++		}
+ 
+-	ret = ath10k_core_fetch_board_file(ar);
+-	if (ret) {
+-		ath10k_err(ar, "failed to fetch board file: %d\n", ret);
+-		goto err_free_firmware_files;
++		ath10k_debug_print_board_info(ar);
+ 	}
+ 
+-	ath10k_debug_print_board_info(ar);
+-
+ 	ret = ath10k_core_init_firmware_features(ar);
+ 	if (ret) {
+ 		ath10k_err(ar, "fatal problem with firmware features: %d\n",
+@@ -2347,11 +2721,15 @@ static int ath10k_core_probe_fw(struct a
+ 		goto err_free_firmware_files;
+ 	}
+ 
+-	ret = ath10k_swap_code_seg_init(ar, &ar->normal_mode_fw.fw_file);
+-	if (ret) {
+-		ath10k_err(ar, "failed to initialize code swap segment: %d\n",
+-			   ret);
+-		goto err_free_firmware_files;
++	if (!test_bit(ATH10K_FW_FEATURE_NON_BMI,
++		      ar->normal_mode_fw.fw_file.fw_features)) {
++		ret = ath10k_swap_code_seg_init(ar,
++						&ar->normal_mode_fw.fw_file);
++		if (ret) {
++			ath10k_err(ar, "failed to initialize code swap segment: %d\n",
++				   ret);
++			goto err_free_firmware_files;
++		}
+ 	}
+ 
+ 	mutex_lock(&ar->conf_mutex);
+@@ -2363,12 +2741,18 @@ static int ath10k_core_probe_fw(struct a
+ 		goto err_unlock;
+ 	}
+ 
+-	ath10k_debug_print_boot_info(ar);
+-	ath10k_core_stop(ar);
++	/* Leave target running if hw_params.start_once is set */
++	if (ar->hw_params.start_once) {
++		mutex_unlock(&ar->conf_mutex);
++	} else {
++		ath10k_debug_print_boot_info(ar);
++		ath10k_core_stop(ar);
+ 
+-	mutex_unlock(&ar->conf_mutex);
++		mutex_unlock(&ar->conf_mutex);
++
++		ath10k_hif_power_down(ar);
++	}
+ 
+-	ath10k_hif_power_down(ar);
+ 	return 0;
+ 
+ err_unlock:
+@@ -2403,10 +2787,16 @@ static void ath10k_core_register_work(st
+ 		goto err_release_fw;
+ 	}
+ 
++	status = ath10k_coredump_register(ar);
++	if (status) {
++		ath10k_err(ar, "unable to register coredump\n");
++		goto err_unregister_mac;
++	}
++
+ 	status = ath10k_debug_register(ar);
+ 	if (status) {
+ 		ath10k_err(ar, "unable to initialize debugfs\n");
+-		goto err_unregister_mac;
++		goto err_unregister_coredump;
+ 	}
+ 
+ 	status = ath10k_spectral_create(ar);
+@@ -2429,6 +2819,8 @@ err_spectral_destroy:
+ 	ath10k_spectral_destroy(ar);
+ err_debug_destroy:
+ 	ath10k_debug_destroy(ar);
++err_unregister_coredump:
++	ath10k_coredump_unregister(ar);
+ err_unregister_mac:
+ 	ath10k_mac_unregister(ar);
+ err_release_fw:
+@@ -2440,9 +2832,11 @@ err:
+ 	return;
+ }
+ 
+-int ath10k_core_register(struct ath10k *ar, u32 chip_id)
++int ath10k_core_register(struct ath10k *ar,
++			 const struct ath10k_bus_params *bus_params)
+ {
+-	ar->chip_id = chip_id;
++	ar->chip_id = bus_params->chip_id;
++	ar->is_high_latency = bus_params->is_high_latency;
+ 	queue_work(ar->workqueue, &ar->register_work);
+ 
+ 	return 0;
+@@ -2583,12 +2977,19 @@ struct ath10k *ath10k_core_create(size_t
+ 
+ 	init_dummy_netdev(&ar->napi_dev);
+ 
+-	ret = ath10k_debug_create(ar);
++	ret = ath10k_coredump_create(ar);
+ 	if (ret)
+ 		goto err_free_aux_wq;
+ 
++	ret = ath10k_debug_create(ar);
++	if (ret)
++		goto err_free_coredump;
++
+ 	return ar;
+ 
++err_free_coredump:
++	ath10k_coredump_destroy(ar);
++
+ err_free_aux_wq:
+ 	destroy_workqueue(ar->workqueue_aux);
+ err_free_wq:
+@@ -2610,6 +3011,7 @@ void ath10k_core_destroy(struct ath10k *
+ 	destroy_workqueue(ar->workqueue_aux);
+ 
+ 	ath10k_debug_destroy(ar);
++	ath10k_coredump_destroy(ar);
+ 	ath10k_htt_tx_destroy(&ar->htt);
+ 	ath10k_wmi_free_host_mem(ar);
+ 	ath10k_mac_destroy(ar);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/core.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/core.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/core.h
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -51,6 +52,8 @@
+ /* Antenna noise floor */
+ #define ATH10K_DEFAULT_NOISE_FLOOR -95
+ 
++#define ATH10K_INVALID_RSSI 128
++
+ #define ATH10K_MAX_NUM_MGMT_PENDING 128
+ 
+ /* number of failed packets (20 packets with 16 sw reties each) */
+@@ -67,7 +70,6 @@
+ 
+ /* NAPI poll budget */
+ #define ATH10K_NAPI_BUDGET      64
+-#define ATH10K_NAPI_QUOTA_LIMIT 60
+ 
+ /* SMBIOS type containing Board Data File Name Extension */
+ #define ATH10K_SMBIOS_BDF_EXT_TYPE 0xF8
+@@ -88,13 +90,6 @@
+ 
+ struct ath10k;
+ 
+-enum ath10k_bus {
+-	ATH10K_BUS_PCI,
+-	ATH10K_BUS_AHB,
+-	ATH10K_BUS_SDIO,
+-	ATH10K_BUS_USB,
+-};
+-
+ static inline const char *ath10k_bus_str(enum ath10k_bus bus)
+ {
+ 	switch (bus) {
+@@ -106,6 +101,8 @@ static inline const char *ath10k_bus_str
+ 		return "sdio";
+ 	case ATH10K_BUS_USB:
+ 		return "usb";
++	case ATH10K_BUS_SNOC:
++		return "snoc";
+ 	}
+ 
+ 	return "unknown";
+@@ -219,6 +216,27 @@ struct ath10k_fw_stats_vdev {
+ 	u32 beacon_rssi_history[10];
+ };
+ 
++struct ath10k_fw_stats_vdev_extd {
++	struct list_head list;
++
++	u32 vdev_id;
++	u32 ppdu_aggr_cnt;
++	u32 ppdu_noack;
++	u32 mpdu_queued;
++	u32 ppdu_nonaggr_cnt;
++	u32 mpdu_sw_requeued;
++	u32 mpdu_suc_retry;
++	u32 mpdu_suc_multitry;
++	u32 mpdu_fail_retry;
++	u32 tx_ftm_suc;
++	u32 tx_ftm_suc_retry;
++	u32 tx_ftm_fail;
++	u32 rx_ftmr_cnt;
++	u32 rx_ftmr_dup_cnt;
++	u32 rx_iftmr_cnt;
++	u32 rx_iftmr_dup_cnt;
++};
++
+ struct ath10k_fw_stats_pdev {
+ 	struct list_head list;
+ 
+@@ -322,6 +340,27 @@ struct ath10k_tpc_stats {
+ 	struct ath10k_tpc_table tpc_table[WMI_TPC_FLAG];
+ };
+ 
++struct ath10k_tpc_table_final {
++	u32 pream_idx[WMI_TPC_FINAL_RATE_MAX];
++	u8 rate_code[WMI_TPC_FINAL_RATE_MAX];
++	char tpc_value[WMI_TPC_FINAL_RATE_MAX][WMI_TPC_TX_N_CHAIN * WMI_TPC_BUF_SIZE];
++};
++
++struct ath10k_tpc_stats_final {
++	u32 reg_domain;
++	u32 chan_freq;
++	u32 phy_mode;
++	u32 twice_antenna_reduction;
++	u32 twice_max_rd_power;
++	s32 twice_antenna_gain;
++	u32 power_limit;
++	u32 num_tx_chain;
++	u32 ctl;
++	u32 rate_max;
++	u8 flag[WMI_TPC_FLAG];
++	struct ath10k_tpc_table_final tpc_table_final[WMI_TPC_FLAG];
++};
++
+ struct ath10k_dfs_stats {
+ 	u32 phy_errors;
+ 	u32 pulses_total;
+@@ -352,6 +391,45 @@ struct ath10k_txq {
+ 	unsigned long num_push_allowed;
+ };
+ 
++enum ath10k_pkt_rx_err {
++	ATH10K_PKT_RX_ERR_FCS,
++	ATH10K_PKT_RX_ERR_TKIP,
++	ATH10K_PKT_RX_ERR_CRYPT,
++	ATH10K_PKT_RX_ERR_PEER_IDX_INVAL,
++	ATH10K_PKT_RX_ERR_MAX,
++};
++
++enum ath10k_ampdu_subfrm_num {
++	ATH10K_AMPDU_SUBFRM_NUM_10,
++	ATH10K_AMPDU_SUBFRM_NUM_20,
++	ATH10K_AMPDU_SUBFRM_NUM_30,
++	ATH10K_AMPDU_SUBFRM_NUM_40,
++	ATH10K_AMPDU_SUBFRM_NUM_50,
++	ATH10K_AMPDU_SUBFRM_NUM_60,
++	ATH10K_AMPDU_SUBFRM_NUM_MORE,
++	ATH10K_AMPDU_SUBFRM_NUM_MAX,
++};
++
++enum ath10k_amsdu_subfrm_num {
++	ATH10K_AMSDU_SUBFRM_NUM_1,
++	ATH10K_AMSDU_SUBFRM_NUM_2,
++	ATH10K_AMSDU_SUBFRM_NUM_3,
++	ATH10K_AMSDU_SUBFRM_NUM_4,
++	ATH10K_AMSDU_SUBFRM_NUM_MORE,
++	ATH10K_AMSDU_SUBFRM_NUM_MAX,
++};
++
++struct ath10k_sta_tid_stats {
++	unsigned long int rx_pkt_from_fw;
++	unsigned long int rx_pkt_unchained;
++	unsigned long int rx_pkt_drop_chained;
++	unsigned long int rx_pkt_drop_filter;
++	unsigned long int rx_pkt_err[ATH10K_PKT_RX_ERR_MAX];
++	unsigned long int rx_pkt_queued_for_mac;
++	unsigned long int rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_MAX];
++	unsigned long int rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_MAX];
++};
++
+ struct ath10k_sta {
+ 	struct ath10k_vif *arvif;
+ 
+@@ -364,11 +442,14 @@ struct ath10k_sta {
+ 	struct rate_info txrate;
+ 
+ 	struct work_struct update_wk;
++	u64 rx_duration;
+ 
+ #ifdef CONFIG_MAC80211_DEBUGFS
+ 	/* protected by conf_mutex */
+ 	bool aggr_mode;
+-	u64 rx_duration;
++
++	/* Protected with ar->data_lock */
++	struct ath10k_sta_tid_stats tid_stats[IEEE80211_NUM_TIDS + 1];
+ #endif
+ };
+ 
+@@ -458,14 +539,17 @@ struct ath10k_ce_crash_hdr {
+ 	struct ath10k_ce_crash_data entries[];
+ };
+ 
++#define MAX_MEM_DUMP_TYPE	5
++
+ /* used for crash-dump storage, protected by data-lock */
+ struct ath10k_fw_crash_data {
+-	bool crashed_since_read;
+-
+ 	guid_t guid;
+-	struct timespec timestamp;
++	struct timespec64 timestamp;
+ 	__le32 registers[REG_DUMP_COUNT_QCA988X];
+ 	struct ath10k_ce_crash_data ce_crash_data[CE_COUNT_MAX];
++
++	u8 *ramdump_buf;
++	size_t ramdump_buf_len;
+ };
+ 
+ struct ath10k_debug {
+@@ -482,18 +566,16 @@ struct ath10k_debug {
+ 
+ 	/* used for tpc-dump storage, protected by data-lock */
+ 	struct ath10k_tpc_stats *tpc_stats;
++	struct ath10k_tpc_stats_final *tpc_stats_final;
+ 
+ 	struct completion tpc_complete;
+ 
+ 	/* protected by conf_mutex */
+ 	u64 fw_dbglog_mask;
+ 	u32 fw_dbglog_level;
+-	u32 pktlog_filter;
+ 	u32 reg_addr;
+ 	u32 nf_cal_period;
+ 	void *cal_data;
+-
+-	struct ath10k_fw_crash_data *fw_crash_data;
+ };
+ 
+ enum ath10k_state {
+@@ -612,6 +694,15 @@ enum ath10k_fw_features {
+ 	 */
+ 	ATH10K_FW_FEATURE_ALLOWS_MESH_BCAST = 16,
+ 
++	/* Firmware does not support power save in station mode. */
++	ATH10K_FW_FEATURE_NO_PS = 17,
++
++	/* Firmware allows management tx by reference instead of by value. */
++	ATH10K_FW_FEATURE_MGMT_TX_BY_REF = 18,
++
++	/* Firmware load is done externally, not by bmi */
++	ATH10K_FW_FEATURE_NON_BMI = 19,
++
+ 	/* keep last */
+ 	ATH10K_FW_FEATURE_COUNT,
+ };
+@@ -758,6 +849,11 @@ struct ath10k_per_peer_tx_stats {
+ 	u32	reserved2;
+ };
+ 
++struct ath10k_bus_params {
++	u32 chip_id;
++	bool is_high_latency;
++};
++
+ struct ath10k {
+ 	struct ath_common ath_common;
+ 	struct ieee80211_hw *hw;
+@@ -768,6 +864,7 @@ struct ath10k {
+ 	enum ath10k_hw_rev hw_rev;
+ 	u16 dev_id;
+ 	u32 chip_id;
++	bool is_high_latency;
+ 	u32 target_version;
+ 	u8 fw_version_major;
+ 	u32 fw_version_minor;
+@@ -789,6 +886,8 @@ struct ath10k {
+ 
+ 	bool p2p;
+ 
++	bool is_started;
++
+ 	struct {
+ 		enum ath10k_bus bus;
+ 		const struct ath10k_hif_ops *ops;
+@@ -960,6 +1059,14 @@ struct ath10k {
+ 	} spectral;
+ #endif
+ 
++	u32 pktlog_filter;
++
++#ifdef CONFIG_DEV_COREDUMP
++	struct {
++		struct ath10k_fw_crash_data *fw_crash_data;
++	} coredump;
++#endif
++
+ 	struct {
+ 		/* protected by conf_mutex */
+ 		struct ath10k_fw_components utf_mode_fw;
+@@ -1000,6 +1107,8 @@ struct ath10k {
+ 
+ 	void *ce_priv;
+ 
++	u32 sta_tid_stats_mask;
++
+ 	/* must be last */
+ 	u8 drv_priv[0] __aligned(sizeof(void *));
+ };
+@@ -1013,6 +1122,8 @@ static inline bool ath10k_peer_stats_ena
+ 	return false;
+ }
+ 
++extern unsigned long ath10k_coredump_mask;
++
+ struct ath10k *ath10k_core_create(size_t priv_size, struct device *dev,
+ 				  enum ath10k_bus bus,
+ 				  enum ath10k_hw_rev hw_rev,
+@@ -1028,7 +1139,8 @@ int ath10k_core_start(struct ath10k *ar,
+ 		      const struct ath10k_fw_components *fw_components);
+ int ath10k_wait_for_suspend(struct ath10k *ar, u32 suspend_opt);
+ void ath10k_core_stop(struct ath10k *ar);
+-int ath10k_core_register(struct ath10k *ar, u32 chip_id);
++int ath10k_core_register(struct ath10k *ar,
++			 const struct ath10k_bus_params *bus_params);
+ void ath10k_core_unregister(struct ath10k *ar);
+ 
+ #endif /* _CORE_H_ */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/debug.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/debug.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/debug.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -18,10 +19,8 @@
+ #include <linux/module.h>
+ #include <linux/debugfs.h>
+ #include <linux/vmalloc.h>
+-#include <linux/utsname.h>
+ #include <linux/crc32.h>
+ #include <linux/firmware.h>
+-#include <linux/devcoredump.h>
+ 
+ #include "core.h"
+ #include "debug.h"
+@@ -33,86 +32,6 @@
+ 
+ #define ATH10K_DEBUG_CAL_DATA_LEN 12064
+ 
+-#define ATH10K_FW_CRASH_DUMP_VERSION 1
+-
+-/**
+- * enum ath10k_fw_crash_dump_type - types of data in the dump file
+- * @ATH10K_FW_CRASH_DUMP_REGDUMP: Register crash dump in binary format
+- */
+-enum ath10k_fw_crash_dump_type {
+-	ATH10K_FW_CRASH_DUMP_REGISTERS = 0,
+-	ATH10K_FW_CRASH_DUMP_CE_DATA = 1,
+-
+-	ATH10K_FW_CRASH_DUMP_MAX,
+-};
+-
+-struct ath10k_tlv_dump_data {
+-	/* see ath10k_fw_crash_dump_type above */
+-	__le32 type;
+-
+-	/* in bytes */
+-	__le32 tlv_len;
+-
+-	/* pad to 32-bit boundaries as needed */
+-	u8 tlv_data[];
+-} __packed;
+-
+-struct ath10k_dump_file_data {
+-	/* dump file information */
+-
+-	/* "ATH10K-FW-DUMP" */
+-	char df_magic[16];
+-
+-	__le32 len;
+-
+-	/* file dump version */
+-	__le32 version;
+-
+-	/* some info we can get from ath10k struct that might help */
+-
+-	guid_t guid;
+-
+-	__le32 chip_id;
+-
+-	/* 0 for now, in place for later hardware */
+-	__le32 bus_type;
+-
+-	__le32 target_version;
+-	__le32 fw_version_major;
+-	__le32 fw_version_minor;
+-	__le32 fw_version_release;
+-	__le32 fw_version_build;
+-	__le32 phy_capability;
+-	__le32 hw_min_tx_power;
+-	__le32 hw_max_tx_power;
+-	__le32 ht_cap_info;
+-	__le32 vht_cap_info;
+-	__le32 num_rf_chains;
+-
+-	/* firmware version string */
+-	char fw_ver[ETHTOOL_FWVERS_LEN];
+-
+-	/* Kernel related information */
+-
+-	/* time-of-day stamp */
+-	__le64 tv_sec;
+-
+-	/* time-of-day stamp, nano-seconds */
+-	__le64 tv_nsec;
+-
+-	/* LINUX_VERSION_CODE */
+-	__le32 kernel_ver_code;
+-
+-	/* VERMAGIC_STRING */
+-	char kernel_ver[64];
+-
+-	/* room for growth w/out changing binary format */
+-	u8 unused[128];
+-
+-	/* struct ath10k_tlv_dump_data + more */
+-	u8 data[0];
+-} __packed;
+-
+ void ath10k_info(struct ath10k *ar, const char *fmt, ...)
+ {
+ 	struct va_format vaf = {
+@@ -163,6 +82,8 @@ void ath10k_debug_print_hwfw_info(struct
+ void ath10k_debug_print_board_info(struct ath10k *ar)
+ {
+ 	char boardinfo[100];
++	const struct firmware *board;
++	u32 crc;
+ 
+ 	if (ar->id.bmi_ids_valid)
+ 		scnprintf(boardinfo, sizeof(boardinfo), "%d:%d",
+@@ -170,11 +91,16 @@ void ath10k_debug_print_board_info(struc
+ 	else
+ 		scnprintf(boardinfo, sizeof(boardinfo), "N/A");
+ 
++	board = ar->normal_mode_fw.board;
++	if (!IS_ERR_OR_NULL(board))
++		crc = crc32_le(0, board->data, board->size);
++	else
++		crc = 0;
++
+ 	ath10k_info(ar, "board_file api %d bmi_id %s crc32 %08x",
+ 		    ar->bd_api,
+ 		    boardinfo,
+-		    crc32_le(0, ar->normal_mode_fw.board->data,
+-			     ar->normal_mode_fw.board->size));
++		    crc);
+ }
+ 
+ void ath10k_debug_print_boot_info(struct ath10k *ar)
+@@ -711,189 +637,6 @@ static const struct file_operations fops
+ 	.llseek = default_llseek,
+ };
+ 
+-struct ath10k_fw_crash_data *
+-ath10k_debug_get_new_fw_crash_data(struct ath10k *ar)
+-{
+-	struct ath10k_fw_crash_data *crash_data = ar->debug.fw_crash_data;
+-
+-	lockdep_assert_held(&ar->data_lock);
+-
+-	crash_data->crashed_since_read = true;
+-	guid_gen(&crash_data->guid);
+-	getnstimeofday(&crash_data->timestamp);
+-
+-	return crash_data;
+-}
+-EXPORT_SYMBOL(ath10k_debug_get_new_fw_crash_data);
+-
+-static struct ath10k_dump_file_data *ath10k_build_dump_file(struct ath10k *ar,
+-							    bool mark_read)
+-{
+-	struct ath10k_fw_crash_data *crash_data = ar->debug.fw_crash_data;
+-	struct ath10k_ce_crash_hdr *ce_hdr;
+-	struct ath10k_dump_file_data *dump_data;
+-	struct ath10k_tlv_dump_data *dump_tlv;
+-	size_t hdr_len = sizeof(*dump_data);
+-	size_t len, sofar = 0;
+-	unsigned char *buf;
+-
+-	len = hdr_len;
+-	len += sizeof(*dump_tlv) + sizeof(crash_data->registers);
+-	len += sizeof(*dump_tlv) + sizeof(*ce_hdr) +
+-		CE_COUNT * sizeof(ce_hdr->entries[0]);
+-
+-	sofar += hdr_len;
+-
+-	/* This is going to get big when we start dumping FW RAM and such,
+-	 * so go ahead and use vmalloc.
+-	 */
+-	buf = vzalloc(len);
+-	if (!buf)
+-		return NULL;
+-
+-	spin_lock_bh(&ar->data_lock);
+-
+-	if (!crash_data->crashed_since_read) {
+-		spin_unlock_bh(&ar->data_lock);
+-		vfree(buf);
+-		return NULL;
+-	}
+-
+-	dump_data = (struct ath10k_dump_file_data *)(buf);
+-	strlcpy(dump_data->df_magic, "ATH10K-FW-DUMP",
+-		sizeof(dump_data->df_magic));
+-	dump_data->len = cpu_to_le32(len);
+-
+-	dump_data->version = cpu_to_le32(ATH10K_FW_CRASH_DUMP_VERSION);
+-
+-	guid_copy(&dump_data->guid, &crash_data->guid);
+-	dump_data->chip_id = cpu_to_le32(ar->chip_id);
+-	dump_data->bus_type = cpu_to_le32(0);
+-	dump_data->target_version = cpu_to_le32(ar->target_version);
+-	dump_data->fw_version_major = cpu_to_le32(ar->fw_version_major);
+-	dump_data->fw_version_minor = cpu_to_le32(ar->fw_version_minor);
+-	dump_data->fw_version_release = cpu_to_le32(ar->fw_version_release);
+-	dump_data->fw_version_build = cpu_to_le32(ar->fw_version_build);
+-	dump_data->phy_capability = cpu_to_le32(ar->phy_capability);
+-	dump_data->hw_min_tx_power = cpu_to_le32(ar->hw_min_tx_power);
+-	dump_data->hw_max_tx_power = cpu_to_le32(ar->hw_max_tx_power);
+-	dump_data->ht_cap_info = cpu_to_le32(ar->ht_cap_info);
+-	dump_data->vht_cap_info = cpu_to_le32(ar->vht_cap_info);
+-	dump_data->num_rf_chains = cpu_to_le32(ar->num_rf_chains);
+-
+-	strlcpy(dump_data->fw_ver, ar->hw->wiphy->fw_version,
+-		sizeof(dump_data->fw_ver));
+-
+-	dump_data->kernel_ver_code = 0;
+-	strlcpy(dump_data->kernel_ver, init_utsname()->release,
+-		sizeof(dump_data->kernel_ver));
+-
+-	dump_data->tv_sec = cpu_to_le64(crash_data->timestamp.tv_sec);
+-	dump_data->tv_nsec = cpu_to_le64(crash_data->timestamp.tv_nsec);
+-
+-	/* Gather crash-dump */
+-	dump_tlv = (struct ath10k_tlv_dump_data *)(buf + sofar);
+-	dump_tlv->type = cpu_to_le32(ATH10K_FW_CRASH_DUMP_REGISTERS);
+-	dump_tlv->tlv_len = cpu_to_le32(sizeof(crash_data->registers));
+-	memcpy(dump_tlv->tlv_data, &crash_data->registers,
+-	       sizeof(crash_data->registers));
+-	sofar += sizeof(*dump_tlv) + sizeof(crash_data->registers);
+-
+-	dump_tlv = (struct ath10k_tlv_dump_data *)(buf + sofar);
+-	dump_tlv->type = cpu_to_le32(ATH10K_FW_CRASH_DUMP_CE_DATA);
+-	dump_tlv->tlv_len = cpu_to_le32(sizeof(*ce_hdr) +
+-					CE_COUNT * sizeof(ce_hdr->entries[0]));
+-	ce_hdr = (struct ath10k_ce_crash_hdr *)(dump_tlv->tlv_data);
+-	ce_hdr->ce_count = cpu_to_le32(CE_COUNT);
+-	memset(ce_hdr->reserved, 0, sizeof(ce_hdr->reserved));
+-	memcpy(ce_hdr->entries, crash_data->ce_crash_data,
+-	       CE_COUNT * sizeof(ce_hdr->entries[0]));
+-	sofar += sizeof(*dump_tlv) + sizeof(*ce_hdr) +
+-		 CE_COUNT * sizeof(ce_hdr->entries[0]);
+-
+-	ar->debug.fw_crash_data->crashed_since_read = !mark_read;
+-
+-	spin_unlock_bh(&ar->data_lock);
+-
+-	return dump_data;
+-}
+-
+-int ath10k_debug_fw_devcoredump(struct ath10k *ar)
+-{
+-	struct ath10k_dump_file_data *dump;
+-	void *dump_ptr;
+-	u32 dump_len;
+-
+-	/* To keep the dump file available also for debugfs don't mark the
+-	 * file read, only debugfs should do that.
+-	 */
+-	dump = ath10k_build_dump_file(ar, false);
+-	if (!dump) {
+-		ath10k_warn(ar, "no crash dump data found for devcoredump");
+-		return -ENODATA;
+-	}
+-
+-	/* Make a copy of the dump file for dev_coredumpv() as during the
+-	 * transition period we need to own the original file. Once
+-	 * fw_crash_dump debugfs file is removed no need to have a copy
+-	 * anymore.
+-	 */
+-	dump_len = le32_to_cpu(dump->len);
+-	dump_ptr = vzalloc(dump_len);
+-
+-	if (!dump_ptr)
+-		return -ENOMEM;
+-
+-	memcpy(dump_ptr, dump, dump_len);
+-
+-	dev_coredumpv(ar->dev, dump_ptr, dump_len, GFP_KERNEL);
+-
+-	return 0;
+-}
+-
+-static int ath10k_fw_crash_dump_open(struct inode *inode, struct file *file)
+-{
+-	struct ath10k *ar = inode->i_private;
+-	struct ath10k_dump_file_data *dump;
+-
+-	ath10k_warn(ar, "fw_crash_dump debugfs file is deprecated, please use /sys/class/devcoredump instead.");
+-
+-	dump = ath10k_build_dump_file(ar, true);
+-	if (!dump)
+-		return -ENODATA;
+-
+-	file->private_data = dump;
+-
+-	return 0;
+-}
+-
+-static ssize_t ath10k_fw_crash_dump_read(struct file *file,
+-					 char __user *user_buf,
+-					 size_t count, loff_t *ppos)
+-{
+-	struct ath10k_dump_file_data *dump_file = file->private_data;
+-
+-	return simple_read_from_buffer(user_buf, count, ppos,
+-				       dump_file,
+-				       le32_to_cpu(dump_file->len));
+-}
+-
+-static int ath10k_fw_crash_dump_release(struct inode *inode,
+-					struct file *file)
+-{
+-	vfree(file->private_data);
+-
+-	return 0;
+-}
+-
+-static const struct file_operations fops_fw_crash_dump = {
+-	.open = ath10k_fw_crash_dump_open,
+-	.read = ath10k_fw_crash_dump_read,
+-	.release = ath10k_fw_crash_dump_release,
+-	.owner = THIS_MODULE,
+-	.llseek = default_llseek,
+-};
+-
+ static ssize_t ath10k_reg_addr_read(struct file *file,
+ 				    char __user *user_buf,
+ 				    size_t count, loff_t *ppos)
+@@ -1737,6 +1480,19 @@ void ath10k_debug_tpc_stats_process(stru
+ 	spin_unlock_bh(&ar->data_lock);
+ }
+ 
++void
++ath10k_debug_tpc_stats_final_process(struct ath10k *ar,
++				     struct ath10k_tpc_stats_final *tpc_stats)
++{
++	spin_lock_bh(&ar->data_lock);
++
++	kfree(ar->debug.tpc_stats_final);
++	ar->debug.tpc_stats_final = tpc_stats;
++	complete(&ar->debug.tpc_complete);
++
++	spin_unlock_bh(&ar->data_lock);
++}
++
+ static void ath10k_tpc_stats_print(struct ath10k_tpc_stats *tpc_stats,
+ 				   unsigned int j, char *buf, size_t *len)
+ {
+@@ -1950,14 +1706,14 @@ int ath10k_debug_start(struct ath10k *ar
+ 				    ret);
+ 	}
+ 
+-	if (ar->debug.pktlog_filter) {
++	if (ar->pktlog_filter) {
+ 		ret = ath10k_wmi_pdev_pktlog_enable(ar,
+-						    ar->debug.pktlog_filter);
++						    ar->pktlog_filter);
+ 		if (ret)
+ 			/* not serious */
+ 			ath10k_warn(ar,
+ 				    "failed to enable pktlog filter %x: %d\n",
+-				    ar->debug.pktlog_filter, ret);
++				    ar->pktlog_filter, ret);
+ 	} else {
+ 		ret = ath10k_wmi_pdev_pktlog_disable(ar);
+ 		if (ret)
+@@ -2097,12 +1853,12 @@ static ssize_t ath10k_write_pktlog_filte
+ 	mutex_lock(&ar->conf_mutex);
+ 
+ 	if (ar->state != ATH10K_STATE_ON) {
+-		ar->debug.pktlog_filter = filter;
++		ar->pktlog_filter = filter;
+ 		ret = count;
+ 		goto out;
+ 	}
+ 
+-	if (filter == ar->debug.pktlog_filter) {
++	if (filter == ar->pktlog_filter) {
+ 		ret = count;
+ 		goto out;
+ 	}
+@@ -2111,7 +1867,7 @@ static ssize_t ath10k_write_pktlog_filte
+ 		ret = ath10k_wmi_pdev_pktlog_enable(ar, filter);
+ 		if (ret) {
+ 			ath10k_warn(ar, "failed to enable pktlog filter %x: %d\n",
+-				    ar->debug.pktlog_filter, ret);
++				    ar->pktlog_filter, ret);
+ 			goto out;
+ 		}
+ 	} else {
+@@ -2122,7 +1878,7 @@ static ssize_t ath10k_write_pktlog_filte
+ 		}
+ 	}
+ 
+-	ar->debug.pktlog_filter = filter;
++	ar->pktlog_filter = filter;
+ 	ret = count;
+ 
+ out:
+@@ -2139,7 +1895,7 @@ static ssize_t ath10k_read_pktlog_filter
+ 
+ 	mutex_lock(&ar->conf_mutex);
+ 	len = scnprintf(buf, sizeof(buf) - len, "%08x\n",
+-			ar->debug.pktlog_filter);
++			ar->pktlog_filter);
+ 	mutex_unlock(&ar->conf_mutex);
+ 
+ 	return simple_read_from_buffer(ubuf, count, ppos, buf, len);
+@@ -2400,12 +2156,139 @@ static const struct file_operations fops
+ 	.llseek = default_llseek,
+ };
+ 
+-int ath10k_debug_create(struct ath10k *ar)
++static ssize_t ath10k_sta_tid_stats_mask_read(struct file *file,
++					      char __user *user_buf,
++					      size_t count, loff_t *ppos)
+ {
+-	ar->debug.fw_crash_data = vzalloc(sizeof(*ar->debug.fw_crash_data));
+-	if (!ar->debug.fw_crash_data)
+-		return -ENOMEM;
++	struct ath10k *ar = file->private_data;
++	char buf[32];
++	size_t len;
++
++	len = scnprintf(buf, sizeof(buf), "0x%08x\n", ar->sta_tid_stats_mask);
++	return simple_read_from_buffer(user_buf, count, ppos, buf, len);
++}
++
++static ssize_t ath10k_sta_tid_stats_mask_write(struct file *file,
++					       const char __user *user_buf,
++					       size_t count, loff_t *ppos)
++{
++	struct ath10k *ar = file->private_data;
++	char buf[32];
++	ssize_t len;
++	u32 mask;
++
++	len = min(count, sizeof(buf) - 1);
++	if (copy_from_user(buf, user_buf, len))
++		return -EFAULT;
++
++	buf[len] = '\0';
++	if (kstrtoint(buf, 0, &mask))
++		return -EINVAL;
++
++	ar->sta_tid_stats_mask = mask;
++
++	return len;
++}
++
++static const struct file_operations fops_sta_tid_stats_mask = {
++	.read = ath10k_sta_tid_stats_mask_read,
++	.write = ath10k_sta_tid_stats_mask_write,
++	.open = simple_open,
++	.owner = THIS_MODULE,
++	.llseek = default_llseek,
++};
++
++static int ath10k_debug_tpc_stats_final_request(struct ath10k *ar)
++{
++	int ret;
++	unsigned long time_left;
++
++	lockdep_assert_held(&ar->conf_mutex);
++
++	reinit_completion(&ar->debug.tpc_complete);
++
++	ret = ath10k_wmi_pdev_get_tpc_table_cmdid(ar, WMI_TPC_CONFIG_PARAM);
++	if (ret) {
++		ath10k_warn(ar, "failed to request tpc table cmdid: %d\n", ret);
++		return ret;
++	}
++
++	time_left = wait_for_completion_timeout(&ar->debug.tpc_complete,
++						1 * HZ);
++	if (time_left == 0)
++		return -ETIMEDOUT;
++
++	return 0;
++}
+ 
++static int ath10k_tpc_stats_final_open(struct inode *inode, struct file *file)
++{
++	struct ath10k *ar = inode->i_private;
++	void *buf;
++	int ret;
++
++	mutex_lock(&ar->conf_mutex);
++
++	if (ar->state != ATH10K_STATE_ON) {
++		ret = -ENETDOWN;
++		goto err_unlock;
++	}
++
++	buf = vmalloc(ATH10K_TPC_CONFIG_BUF_SIZE);
++	if (!buf) {
++		ret = -ENOMEM;
++		goto err_unlock;
++	}
++
++	ret = ath10k_debug_tpc_stats_final_request(ar);
++	if (ret) {
++		ath10k_warn(ar, "failed to request tpc stats final: %d\n",
++			    ret);
++		goto err_free;
++	}
++
++	ath10k_tpc_stats_fill(ar, ar->debug.tpc_stats, buf);
++	file->private_data = buf;
++
++	mutex_unlock(&ar->conf_mutex);
++	return 0;
++
++err_free:
++	vfree(buf);
++
++err_unlock:
++	mutex_unlock(&ar->conf_mutex);
++	return ret;
++}
++
++static int ath10k_tpc_stats_final_release(struct inode *inode,
++					  struct file *file)
++{
++	vfree(file->private_data);
++
++	return 0;
++}
++
++static ssize_t ath10k_tpc_stats_final_read(struct file *file,
++					   char __user *user_buf,
++					   size_t count, loff_t *ppos)
++{
++	const char *buf = file->private_data;
++	unsigned int len = strlen(buf);
++
++	return simple_read_from_buffer(user_buf, count, ppos, buf, len);
++}
++
++static const struct file_operations fops_tpc_stats_final = {
++	.open = ath10k_tpc_stats_final_open,
++	.release = ath10k_tpc_stats_final_release,
++	.read = ath10k_tpc_stats_final_read,
++	.owner = THIS_MODULE,
++	.llseek = default_llseek,
++};
++
++int ath10k_debug_create(struct ath10k *ar)
++{
+ 	ar->debug.cal_data = vzalloc(ATH10K_DEBUG_CAL_DATA_LEN);
+ 	if (!ar->debug.cal_data)
+ 		return -ENOMEM;
+@@ -2420,9 +2303,6 @@ int ath10k_debug_create(struct ath10k *a
+ 
+ void ath10k_debug_destroy(struct ath10k *ar)
+ {
+-	vfree(ar->debug.fw_crash_data);
+-	ar->debug.fw_crash_data = NULL;
+-
+ 	vfree(ar->debug.cal_data);
+ 	ar->debug.cal_data = NULL;
+ 
+@@ -2460,9 +2340,6 @@ int ath10k_debug_register(struct ath10k
+ 	debugfs_create_file("simulate_fw_crash", 0600, ar->debug.debugfs_phy, ar,
+ 			    &fops_simulate_fw_crash);
+ 
+-	debugfs_create_file("fw_crash_dump", 0400, ar->debug.debugfs_phy, ar,
+-			    &fops_fw_crash_dump);
+-
+ 	debugfs_create_file("reg_addr", 0600, ar->debug.debugfs_phy, ar,
+ 			    &fops_reg_addr);
+ 
+@@ -2525,6 +2402,16 @@ int ath10k_debug_register(struct ath10k
+ 	debugfs_create_file("fw_checksums", 0400, ar->debug.debugfs_phy, ar,
+ 			    &fops_fw_checksums);
+ 
++	if (IS_ENABLED(CONFIG_MAC80211_DEBUGFS))
++		debugfs_create_file("sta_tid_stats_mask", 0600,
++				    ar->debug.debugfs_phy,
++				    ar, &fops_sta_tid_stats_mask);
++
++	if (test_bit(WMI_SERVICE_TPC_STATS_FINAL, ar->wmi.svc_map))
++		debugfs_create_file("tpc_stats_final", 0400,
++				    ar->debug.debugfs_phy, ar,
++				    &fops_tpc_stats_final);
++
+ 	return 0;
+ }
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/debug.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/debug.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/debug.h
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -42,6 +43,7 @@ enum ath10k_debug_mask {
+ 	ATH10K_DBG_SDIO_DUMP	= 0x00020000,
+ 	ATH10K_DBG_USB		= 0x00040000,
+ 	ATH10K_DBG_USB_BULK	= 0x00080000,
++	ATH10K_DBG_SNOC		= 0x00100000,
+ 	ATH10K_DBG_ANY		= 0xffffffff,
+ };
+ 
+@@ -51,7 +53,8 @@ enum ath10k_pktlog_filter {
+ 	ATH10K_PKTLOG_RCFIND     = 0x000000004,
+ 	ATH10K_PKTLOG_RCUPDATE   = 0x000000008,
+ 	ATH10K_PKTLOG_DBG_PRINT  = 0x000000010,
+-	ATH10K_PKTLOG_ANY        = 0x00000001f,
++	ATH10K_PKTLOG_PEER_STATS = 0x000000040,
++	ATH10K_PKTLOG_ANY        = 0x00000005f,
+ };
+ 
+ enum ath10k_dbg_aggr_mode {
+@@ -60,6 +63,21 @@ enum ath10k_dbg_aggr_mode {
+ 	ATH10K_DBG_AGGR_MODE_MAX,
+ };
+ 
++/* Types of packet log events */
++enum ath_pktlog_type {
++	ATH_PKTLOG_TYPE_TX_CTRL = 1,
++	ATH_PKTLOG_TYPE_TX_STAT,
++};
++
++struct ath10k_pktlog_hdr {
++	__le16 flags;
++	__le16 missed_cnt;
++	__le16 log_type; /* Type of log information foll this header */
++	__le16 size; /* Size of variable length log information in bytes */
++	__le32 timestamp;
++	u8 payload[0];
++} __packed;
++
+ /* FIXME: How to calculate the buffer size sanely? */
+ #define ATH10K_FW_STATS_BUF_SIZE (1024 * 1024)
+ 
+@@ -84,13 +102,11 @@ void ath10k_debug_unregister(struct ath1
+ void ath10k_debug_fw_stats_process(struct ath10k *ar, struct sk_buff *skb);
+ void ath10k_debug_tpc_stats_process(struct ath10k *ar,
+ 				    struct ath10k_tpc_stats *tpc_stats);
+-struct ath10k_fw_crash_data *
+-ath10k_debug_get_new_fw_crash_data(struct ath10k *ar);
+-
++void
++ath10k_debug_tpc_stats_final_process(struct ath10k *ar,
++				     struct ath10k_tpc_stats_final *tpc_stats);
+ void ath10k_debug_dbglog_add(struct ath10k *ar, u8 *buffer, int len);
+ 
+-int ath10k_debug_fw_devcoredump(struct ath10k *ar);
+-
+ #define ATH10K_DFS_STAT_INC(ar, c) (ar->debug.dfs_stats.c++)
+ 
+ void ath10k_debug_get_et_strings(struct ieee80211_hw *hw,
+@@ -152,15 +168,16 @@ static inline void ath10k_debug_tpc_stat
+ 	kfree(tpc_stats);
+ }
+ 
+-static inline void ath10k_debug_dbglog_add(struct ath10k *ar, u8 *buffer,
+-					   int len)
++static inline void
++ath10k_debug_tpc_stats_final_process(struct ath10k *ar,
++				     struct ath10k_tpc_stats_final *tpc_stats)
+ {
++	kfree(tpc_stats);
+ }
+ 
+-static inline struct ath10k_fw_crash_data *
+-ath10k_debug_get_new_fw_crash_data(struct ath10k *ar)
++static inline void ath10k_debug_dbglog_add(struct ath10k *ar, u8 *buffer,
++					   int len)
+ {
+-	return NULL;
+ }
+ 
+ static inline u64 ath10k_debug_get_fw_dbglog_mask(struct ath10k *ar)
+@@ -173,11 +190,6 @@ static inline u32 ath10k_debug_get_fw_db
+ 	return 0;
+ }
+ 
+-static inline int ath10k_debug_fw_devcoredump(struct ath10k *ar)
+-{
+-	return 0;
+-}
+-
+ #define ATH10K_DFS_STAT_INC(ar, c) do { } while (0)
+ 
+ #define ath10k_debug_get_et_strings NULL
+@@ -190,15 +202,42 @@ void ath10k_sta_add_debugfs(struct ieee8
+ 			    struct ieee80211_sta *sta, struct dentry *dir);
+ void ath10k_sta_update_rx_duration(struct ath10k *ar,
+ 				   struct ath10k_fw_stats *stats);
+-void ath10k_sta_statistics(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
+-			   struct ieee80211_sta *sta,
+-			   struct station_info *sinfo);
++void ath10k_sta_update_rx_tid_stats(struct ath10k *ar, u8 *first_hdr,
++				    unsigned long int num_msdus,
++				    enum ath10k_pkt_rx_err err,
++				    unsigned long int unchain_cnt,
++				    unsigned long int drop_cnt,
++				    unsigned long int drop_cnt_filter,
++				    unsigned long int queued_msdus);
++void ath10k_sta_update_rx_tid_stats_ampdu(struct ath10k *ar,
++					  u16 peer_id, u8 tid,
++					  struct htt_rx_indication_mpdu_range *ranges,
++					  int num_ranges);
+ #else
+ static inline
+ void ath10k_sta_update_rx_duration(struct ath10k *ar,
+ 				   struct ath10k_fw_stats *stats)
+ {
+ }
++
++static inline
++void ath10k_sta_update_rx_tid_stats(struct ath10k *ar, u8 *first_hdr,
++				    unsigned long int num_msdus,
++				    enum ath10k_pkt_rx_err err,
++				    unsigned long int unchain_cnt,
++				    unsigned long int drop_cnt,
++				    unsigned long int drop_cnt_filter,
++				    unsigned long int queued_msdus)
++{
++}
++
++static inline
++void ath10k_sta_update_rx_tid_stats_ampdu(struct ath10k *ar,
++					  u16 peer_id, u8 tid,
++					  struct htt_rx_indication_mpdu_range *ranges,
++					  int num_ranges)
++{
++}
+ #endif /* CONFIG_MAC80211_DEBUGFS */
+ 
+ #ifdef CONFIG_ATH10K_DEBUG
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/debugfs_sta.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/debugfs_sta.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/debugfs_sta.c
+@@ -1,5 +1,6 @@
+ /*
+- * Copyright (c) 2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -16,8 +17,125 @@
+ 
+ #include "core.h"
+ #include "wmi-ops.h"
++#include "txrx.h"
+ #include "debug.h"
+ 
++static void ath10k_rx_stats_update_amsdu_subfrm(struct ath10k *ar,
++						struct ath10k_sta_tid_stats *stats,
++						u32 msdu_count)
++{
++	if (msdu_count == 1)
++		stats->rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_1]++;
++	else if (msdu_count == 2)
++		stats->rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_2]++;
++	else if (msdu_count == 3)
++		stats->rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_3]++;
++	else if (msdu_count == 4)
++		stats->rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_4]++;
++	else if (msdu_count > 4)
++		stats->rx_pkt_amsdu[ATH10K_AMSDU_SUBFRM_NUM_MORE]++;
++}
++
++static void ath10k_rx_stats_update_ampdu_subfrm(struct ath10k *ar,
++						struct ath10k_sta_tid_stats *stats,
++						u32 mpdu_count)
++{
++	if (mpdu_count <= 10)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_10]++;
++	else if (mpdu_count <= 20)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_20]++;
++	else if (mpdu_count <= 30)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_30]++;
++	else if (mpdu_count <= 40)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_40]++;
++	else if (mpdu_count <= 50)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_50]++;
++	else if (mpdu_count <= 60)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_60]++;
++	else if (mpdu_count > 60)
++		stats->rx_pkt_ampdu[ATH10K_AMPDU_SUBFRM_NUM_MORE]++;
++}
++
++void ath10k_sta_update_rx_tid_stats_ampdu(struct ath10k *ar, u16 peer_id, u8 tid,
++					  struct htt_rx_indication_mpdu_range *ranges,
++					  int num_ranges)
++{
++	struct ath10k_sta *arsta;
++	struct ath10k_peer *peer;
++	int i;
++
++	if (tid > IEEE80211_NUM_TIDS || !(ar->sta_tid_stats_mask & BIT(tid)))
++		return;
++
++	rcu_read_lock();
++	spin_lock_bh(&ar->data_lock);
++
++	peer = ath10k_peer_find_by_id(ar, peer_id);
++	if (!peer)
++		goto out;
++
++	arsta = (struct ath10k_sta *)peer->sta->drv_priv;
++
++	for (i = 0; i < num_ranges; i++)
++		ath10k_rx_stats_update_ampdu_subfrm(ar,
++						    &arsta->tid_stats[tid],
++						    ranges[i].mpdu_count);
++
++out:
++	spin_unlock_bh(&ar->data_lock);
++	rcu_read_unlock();
++}
++
++void ath10k_sta_update_rx_tid_stats(struct ath10k *ar, u8 *first_hdr,
++				    unsigned long int num_msdus,
++				    enum ath10k_pkt_rx_err err,
++				    unsigned long int unchain_cnt,
++				    unsigned long int drop_cnt,
++				    unsigned long int drop_cnt_filter,
++				    unsigned long int queued_msdus)
++{
++	struct ieee80211_sta *sta;
++	struct ath10k_sta *arsta;
++	struct ieee80211_hdr *hdr;
++	struct ath10k_sta_tid_stats *stats;
++	u8 tid = IEEE80211_NUM_TIDS;
++	bool non_data_frm = false;
++
++	hdr = (struct ieee80211_hdr *)first_hdr;
++	if (!ieee80211_is_data(hdr->frame_control))
++		non_data_frm = true;
++
++	if (ieee80211_is_data_qos(hdr->frame_control))
++		tid = *ieee80211_get_qos_ctl(hdr) & IEEE80211_QOS_CTL_TID_MASK;
++
++	if (!(ar->sta_tid_stats_mask & BIT(tid)) || non_data_frm)
++		return;
++
++	rcu_read_lock();
++
++	sta = ieee80211_find_sta_by_ifaddr(ar->hw, hdr->addr2, NULL);
++	if (!sta)
++		goto exit;
++
++	arsta = (struct ath10k_sta *)sta->drv_priv;
++
++	spin_lock_bh(&ar->data_lock);
++	stats = &arsta->tid_stats[tid];
++	stats->rx_pkt_from_fw += num_msdus;
++	stats->rx_pkt_unchained += unchain_cnt;
++	stats->rx_pkt_drop_chained += drop_cnt;
++	stats->rx_pkt_drop_filter += drop_cnt_filter;
++	if (err != ATH10K_PKT_RX_ERR_MAX)
++		stats->rx_pkt_err[err] += queued_msdus;
++	stats->rx_pkt_queued_for_mac += queued_msdus;
++	ath10k_rx_stats_update_amsdu_subfrm(ar, &arsta->tid_stats[tid],
++					    num_msdus);
++	spin_unlock_bh(&ar->data_lock);
++
++exit:
++	rcu_read_unlock();
++}
++
+ static void ath10k_sta_update_extd_stats_rx_duration(struct ath10k *ar,
+ 						     struct ath10k_fw_stats *stats)
+ {
+@@ -65,33 +183,6 @@ void ath10k_sta_update_rx_duration(struc
+ 		ath10k_sta_update_stats_rx_duration(ar, stats);
+ }
+ 
+-void ath10k_sta_statistics(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
+-			   struct ieee80211_sta *sta,
+-			   struct station_info *sinfo)
+-{
+-	struct ath10k_sta *arsta = (struct ath10k_sta *)sta->drv_priv;
+-	struct ath10k *ar = arsta->arvif->ar;
+-
+-	if (!ath10k_peer_stats_enabled(ar))
+-		return;
+-
+-	sinfo->rx_duration = arsta->rx_duration;
+-	sinfo->filled |= 1ULL << NL80211_STA_INFO_RX_DURATION;
+-
+-	if (!arsta->txrate.legacy && !arsta->txrate.nss)
+-		return;
+-
+-	if (arsta->txrate.legacy) {
+-		sinfo->txrate.legacy = arsta->txrate.legacy;
+-	} else {
+-		sinfo->txrate.mcs = arsta->txrate.mcs;
+-		sinfo->txrate.nss = arsta->txrate.nss;
+-		sinfo->txrate.bw = arsta->txrate.bw;
+-	}
+-	sinfo->txrate.flags = arsta->txrate.flags;
+-	sinfo->filled |= 1ULL << NL80211_STA_INFO_TX_BITRATE;
+-}
+-
+ static ssize_t ath10k_dbg_sta_read_aggr_mode(struct file *file,
+ 					     char __user *user_buf,
+ 					     size_t count, loff_t *ppos)
+@@ -369,6 +460,172 @@ static const struct file_operations fops
+ 	.llseek = default_llseek,
+ };
+ 
++static char *get_err_str(enum ath10k_pkt_rx_err i)
++{
++	switch (i) {
++	case ATH10K_PKT_RX_ERR_FCS:
++		return "fcs_err";
++	case ATH10K_PKT_RX_ERR_TKIP:
++		return "tkip_err";
++	case ATH10K_PKT_RX_ERR_CRYPT:
++		return "crypt_err";
++	case ATH10K_PKT_RX_ERR_PEER_IDX_INVAL:
++		return "peer_idx_inval";
++	case ATH10K_PKT_RX_ERR_MAX:
++		return "unknown";
++	}
++
++	return "unknown";
++}
++
++static char *get_num_ampdu_subfrm_str(enum ath10k_ampdu_subfrm_num i)
++{
++	switch (i) {
++	case ATH10K_AMPDU_SUBFRM_NUM_10:
++		return "upto 10";
++	case ATH10K_AMPDU_SUBFRM_NUM_20:
++		return "11-20";
++	case ATH10K_AMPDU_SUBFRM_NUM_30:
++		return "21-30";
++	case ATH10K_AMPDU_SUBFRM_NUM_40:
++		return "31-40";
++	case ATH10K_AMPDU_SUBFRM_NUM_50:
++		return "41-50";
++	case ATH10K_AMPDU_SUBFRM_NUM_60:
++		return "51-60";
++	case ATH10K_AMPDU_SUBFRM_NUM_MORE:
++		return ">60";
++	case ATH10K_AMPDU_SUBFRM_NUM_MAX:
++		return "0";
++	}
++
++	return "0";
++}
++
++static char *get_num_amsdu_subfrm_str(enum ath10k_amsdu_subfrm_num i)
++{
++	switch (i) {
++	case ATH10K_AMSDU_SUBFRM_NUM_1:
++		return "1";
++	case ATH10K_AMSDU_SUBFRM_NUM_2:
++		return "2";
++	case ATH10K_AMSDU_SUBFRM_NUM_3:
++		return "3";
++	case ATH10K_AMSDU_SUBFRM_NUM_4:
++		return "4";
++	case ATH10K_AMSDU_SUBFRM_NUM_MORE:
++		return ">4";
++	case ATH10K_AMSDU_SUBFRM_NUM_MAX:
++		return "0";
++	}
++
++	return "0";
++}
++
++#define PRINT_TID_STATS(_field, _tabs) \
++	do { \
++		int k = 0; \
++		for (j = 0; j <= IEEE80211_NUM_TIDS; j++) { \
++			if (ar->sta_tid_stats_mask & BIT(j))  { \
++				len += scnprintf(buf + len, buf_len - len, \
++						 "[%02d] %-10lu  ", \
++						 j, stats[j]._field); \
++				k++; \
++				if (k % 8 == 0)  { \
++					len += scnprintf(buf + len, \
++							 buf_len - len, "\n"); \
++					len += scnprintf(buf + len, \
++							 buf_len - len, \
++							 _tabs); \
++				} \
++			} \
++		} \
++		len += scnprintf(buf + len, buf_len - len, "\n"); \
++	} while (0)
++
++static ssize_t ath10k_dbg_sta_read_tid_stats(struct file *file,
++					     char __user *user_buf,
++					     size_t count, loff_t *ppos)
++{
++	struct ieee80211_sta *sta = file->private_data;
++	struct ath10k_sta *arsta = (struct ath10k_sta *)sta->drv_priv;
++	struct ath10k *ar = arsta->arvif->ar;
++	struct ath10k_sta_tid_stats *stats = arsta->tid_stats;
++	size_t len = 0, buf_len = 1048 * IEEE80211_NUM_TIDS;
++	char *buf;
++	int i, j;
++	ssize_t ret;
++
++	buf = kzalloc(buf_len, GFP_KERNEL);
++	if (!buf)
++		return -ENOMEM;
++
++	mutex_lock(&ar->conf_mutex);
++
++	spin_lock_bh(&ar->data_lock);
++
++	len += scnprintf(buf + len, buf_len - len,
++			 "\n\t\tDriver Rx pkt stats per tid, ([tid] count)\n");
++	len += scnprintf(buf + len, buf_len - len,
++			 "\t\t------------------------------------------\n");
++	len += scnprintf(buf + len, buf_len - len, "MSDUs from FW\t\t\t");
++	PRINT_TID_STATS(rx_pkt_from_fw, "\t\t\t\t");
++
++	len += scnprintf(buf + len, buf_len - len, "MSDUs unchained\t\t\t");
++	PRINT_TID_STATS(rx_pkt_unchained, "\t\t\t\t");
++
++	len += scnprintf(buf + len, buf_len - len,
++			 "MSDUs locally dropped:chained\t");
++	PRINT_TID_STATS(rx_pkt_drop_chained, "\t\t\t\t");
++
++	len += scnprintf(buf + len, buf_len - len,
++			 "MSDUs locally dropped:filtered\t");
++	PRINT_TID_STATS(rx_pkt_drop_filter, "\t\t\t\t");
++
++	len += scnprintf(buf + len, buf_len - len,
++			 "MSDUs queued for mac80211\t");
++	PRINT_TID_STATS(rx_pkt_queued_for_mac, "\t\t\t\t");
++
++	for (i = 0; i < ATH10K_PKT_RX_ERR_MAX; i++) {
++		len += scnprintf(buf + len, buf_len - len,
++				 "MSDUs with error:%s\t", get_err_str(i));
++		PRINT_TID_STATS(rx_pkt_err[i], "\t\t\t\t");
++	}
++
++	len += scnprintf(buf + len, buf_len - len, "\n");
++	for (i = 0; i < ATH10K_AMPDU_SUBFRM_NUM_MAX; i++) {
++		len += scnprintf(buf + len, buf_len - len,
++				 "A-MPDU num subframes %s\t",
++				 get_num_ampdu_subfrm_str(i));
++		PRINT_TID_STATS(rx_pkt_ampdu[i], "\t\t\t\t");
++	}
++
++	len += scnprintf(buf + len, buf_len - len, "\n");
++	for (i = 0; i < ATH10K_AMSDU_SUBFRM_NUM_MAX; i++) {
++		len += scnprintf(buf + len, buf_len - len,
++				 "A-MSDU num subframes %s\t\t",
++				 get_num_amsdu_subfrm_str(i));
++		PRINT_TID_STATS(rx_pkt_amsdu[i], "\t\t\t\t");
++	}
++
++	spin_unlock_bh(&ar->data_lock);
++
++	ret = simple_read_from_buffer(user_buf, count, ppos, buf, len);
++
++	kfree(buf);
++
++	mutex_unlock(&ar->conf_mutex);
++
++	return ret;
++}
++
++static const struct file_operations fops_tid_stats_dump = {
++	.open = simple_open,
++	.read = ath10k_dbg_sta_read_tid_stats,
++	.owner = THIS_MODULE,
++	.llseek = default_llseek,
++};
++
+ void ath10k_sta_add_debugfs(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
+ 			    struct ieee80211_sta *sta, struct dentry *dir)
+ {
+@@ -378,4 +635,6 @@ void ath10k_sta_add_debugfs(struct ieee8
+ 	debugfs_create_file("delba", 0200, dir, sta, &fops_delba);
+ 	debugfs_create_file("peer_debug_trigger", 0600, dir, sta,
+ 			    &fops_peer_debug_trigger);
++	debugfs_create_file("dump_tid_stats", 0400, dir, sta,
++			    &fops_tid_stats_dump);
+ }
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/hif.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/hif.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/hif.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2015,2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -20,13 +20,14 @@
+ 
+ #include <linux/kernel.h>
+ #include "core.h"
++#include "bmi.h"
+ #include "debug.h"
+ 
+ struct ath10k_hif_sg_item {
+ 	u16 transfer_id;
+ 	void *transfer_context; /* NULL = tx completion callback not called */
+ 	void *vaddr; /* for debugging mostly */
+-	u32 paddr;
++	dma_addr_t paddr;
+ 	u16 len;
+ };
+ 
+@@ -93,6 +94,9 @@ struct ath10k_hif_ops {
+ 	/* fetch calibration data from target eeprom */
+ 	int (*fetch_cal_eeprom)(struct ath10k *ar, void **data,
+ 				size_t *data_len);
++
++	int (*get_target_info)(struct ath10k *ar,
++			       struct bmi_target_info *target_info);
+ };
+ 
+ static inline int ath10k_hif_tx_sg(struct ath10k *ar, u8 pipe_id,
+@@ -218,4 +222,13 @@ static inline int ath10k_hif_fetch_cal_e
+ 	return ar->hif.ops->fetch_cal_eeprom(ar, data, data_len);
+ }
+ 
++static inline int ath10k_hif_get_target_info(struct ath10k *ar,
++					     struct bmi_target_info *tgt_info)
++{
++	if (!ar->hif.ops->get_target_info)
++		return -EOPNOTSUPP;
++
++	return ar->hif.ops->get_target_info(ar, tgt_info);
++}
++
+ #endif /* _HIF_H_ */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htc.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htc.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htc.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -53,7 +53,8 @@ static inline void ath10k_htc_restore_tx
+ {
+ 	struct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(skb);
+ 
+-	dma_unmap_single(htc->ar->dev, skb_cb->paddr, skb->len, DMA_TO_DEVICE);
++	if (!htc->ar->is_high_latency)
++		dma_unmap_single(htc->ar->dev, skb_cb->paddr, skb->len, DMA_TO_DEVICE);
+ 	skb_pull(skb, sizeof(struct ath10k_htc_hdr));
+ }
+ 
+@@ -83,11 +84,13 @@ static void ath10k_htc_prepare_tx_skb(st
+ 	struct ath10k_htc_hdr *hdr;
+ 
+ 	hdr = (struct ath10k_htc_hdr *)skb->data;
++	memset(hdr, 0, sizeof(struct ath10k_htc_hdr));
+ 
+ 	hdr->eid = ep->eid;
+ 	hdr->len = __cpu_to_le16(skb->len - sizeof(*hdr));
+ 	hdr->flags = 0;
+-	hdr->flags |= ATH10K_HTC_FLAG_NEED_CREDIT_UPDATE;
++	if (ep->tx_credit_flow_enabled)
++		hdr->flags |= ATH10K_HTC_FLAG_NEED_CREDIT_UPDATE;
+ 
+ 	spin_lock_bh(&ep->htc->tx_lock);
+ 	hdr->seq_no = ep->seq_no++;
+@@ -137,11 +140,14 @@ int ath10k_htc_send(struct ath10k_htc *h
+ 	ath10k_htc_prepare_tx_skb(ep, skb);
+ 
+ 	skb_cb->eid = eid;
+-	skb_cb->paddr = dma_map_single(dev, skb->data, skb->len, DMA_TO_DEVICE);
+-	ret = dma_mapping_error(dev, skb_cb->paddr);
+-	if (ret) {
+-		ret = -EIO;
+-		goto err_credits;
++	if (!ar->is_high_latency) {
++		skb_cb->paddr = dma_map_single(dev, skb->data, skb->len,
++					       DMA_TO_DEVICE);
++		ret = dma_mapping_error(dev, skb_cb->paddr);
++		if (ret) {
++			ret = -EIO;
++			goto err_credits;
++		}
+ 	}
+ 
+ 	sg_item.transfer_id = ep->eid;
+@@ -157,7 +163,8 @@ int ath10k_htc_send(struct ath10k_htc *h
+ 	return 0;
+ 
+ err_unmap:
+-	dma_unmap_single(dev, skb_cb->paddr, skb->len, DMA_TO_DEVICE);
++	if (!ar->is_high_latency)
++		dma_unmap_single(dev, skb_cb->paddr, skb->len, DMA_TO_DEVICE);
+ err_credits:
+ 	if (ep->tx_credit_flow_enabled) {
+ 		spin_lock_bh(&htc->tx_lock);
+@@ -542,8 +549,14 @@ static const char *htc_service_name(enum
+ 		return "NMI Data";
+ 	case ATH10K_HTC_SVC_ID_HTT_DATA_MSG:
+ 		return "HTT Data";
++	case ATH10K_HTC_SVC_ID_HTT_DATA2_MSG:
++		return "HTT Data";
++	case ATH10K_HTC_SVC_ID_HTT_DATA3_MSG:
++		return "HTT Data";
+ 	case ATH10K_HTC_SVC_ID_TEST_RAW_STREAMS:
+ 		return "RAW";
++	case ATH10K_HTC_SVC_ID_HTT_LOG_MSG:
++		return "PKTLOG";
+ 	}
+ 
+ 	return "Unknown";
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htc.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htc.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htc.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -58,6 +58,7 @@ enum ath10k_htc_tx_flags {
+ };
+ 
+ enum ath10k_htc_rx_flags {
++	ATH10K_HTC_FLAGS_RECV_1MORE_BLOCK = 0x01,
+ 	ATH10K_HTC_FLAG_TRAILER_PRESENT = 0x02,
+ 	ATH10K_HTC_FLAG_BUNDLE_MASK     = 0xF0
+ };
+@@ -248,6 +249,7 @@ enum ath10k_htc_svc_gid {
+ 	ATH10K_HTC_SVC_GRP_WMI = 1,
+ 	ATH10K_HTC_SVC_GRP_NMI = 2,
+ 	ATH10K_HTC_SVC_GRP_HTT = 3,
++	ATH10K_LOG_SERVICE_GROUP = 6,
+ 
+ 	ATH10K_HTC_SVC_GRP_TEST = 254,
+ 	ATH10K_HTC_SVC_GRP_LAST = 255,
+@@ -273,6 +275,9 @@ enum ath10k_htc_svc_id {
+ 
+ 	ATH10K_HTC_SVC_ID_HTT_DATA_MSG	= SVC(ATH10K_HTC_SVC_GRP_HTT, 0),
+ 
++	ATH10K_HTC_SVC_ID_HTT_DATA2_MSG = SVC(ATH10K_HTC_SVC_GRP_HTT, 1),
++	ATH10K_HTC_SVC_ID_HTT_DATA3_MSG = SVC(ATH10K_HTC_SVC_GRP_HTT, 2),
++	ATH10K_HTC_SVC_ID_HTT_LOG_MSG = SVC(ATH10K_LOG_SERVICE_GROUP, 0),
+ 	/* raw stream service (i.e. flash, tcmd, calibration apps) */
+ 	ATH10K_HTC_SVC_ID_TEST_RAW_STREAMS = SVC(ATH10K_HTC_SVC_GRP_TEST, 0),
+ };
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htt.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htt.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htt.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -207,6 +207,9 @@ int ath10k_htt_init(struct ath10k *ar)
+ 		WARN_ON(1);
+ 		return -EINVAL;
+ 	}
++	ath10k_htt_set_tx_ops(htt);
++	ath10k_htt_set_rx_ops(htt);
++
+ 	return 0;
+ }
+ 
+@@ -258,7 +261,7 @@ int ath10k_htt_setup(struct ath10k_htt *
+ 	if (status)
+ 		return status;
+ 
+-	status = ath10k_htt_send_rx_ring_cfg_ll(htt);
++	status = ath10k_htt_send_rx_ring_cfg(htt);
+ 	if (status) {
+ 		ath10k_warn(ar, "failed to setup rx ring: %d\n",
+ 			    status);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htt.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htt.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htt.h
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -107,6 +108,14 @@ struct htt_msdu_ext_desc {
+ 	struct htt_data_tx_desc_frag frags[6];
+ };
+ 
++struct htt_msdu_ext_desc_64 {
++	__le32 tso_flag[5];
++	__le16 ip_identification;
++	u8 flags;
++	u8 reserved;
++	struct htt_data_tx_desc_frag frags[6];
++};
++
+ #define	HTT_MSDU_EXT_DESC_FLAG_IPV4_CSUM_ENABLE		BIT(0)
+ #define	HTT_MSDU_EXT_DESC_FLAG_UDP_IPV4_CSUM_ENABLE	BIT(1)
+ #define	HTT_MSDU_EXT_DESC_FLAG_UDP_IPV6_CSUM_ENABLE	BIT(2)
+@@ -179,6 +188,22 @@ struct htt_data_tx_desc {
+ 	u8 prefetch[0]; /* start of frame, for FW classification engine */
+ } __packed;
+ 
++struct htt_data_tx_desc_64 {
++	u8 flags0; /* %HTT_DATA_TX_DESC_FLAGS0_ */
++	__le16 flags1; /* %HTT_DATA_TX_DESC_FLAGS1_ */
++	__le16 len;
++	__le16 id;
++	__le64 frags_paddr;
++	union {
++		__le32 peerid;
++		struct {
++			__le16 peerid;
++			__le16 freq;
++		} __packed offchan_tx;
++	} __packed;
++	u8 prefetch[0]; /* start of frame, for FW classification engine */
++} __packed;
++
+ enum htt_rx_ring_flags {
+ 	HTT_RX_RING_FLAGS_MAC80211_HDR = 1 << 0,
+ 	HTT_RX_RING_FLAGS_MSDU_PAYLOAD = 1 << 1,
+@@ -200,8 +225,11 @@ enum htt_rx_ring_flags {
+ 
+ #define HTT_RX_RING_SIZE_MIN 128
+ #define HTT_RX_RING_SIZE_MAX 2048
++#define HTT_RX_RING_SIZE HTT_RX_RING_SIZE_MAX
++#define HTT_RX_RING_FILL_LEVEL (((HTT_RX_RING_SIZE) / 2) - 1)
++#define HTT_RX_RING_FILL_LEVEL_DUAL_MAC (HTT_RX_RING_SIZE - 1)
+ 
+-struct htt_rx_ring_setup_ring {
++struct htt_rx_ring_setup_ring32 {
+ 	__le32 fw_idx_shadow_reg_paddr;
+ 	__le32 rx_ring_base_paddr;
+ 	__le16 rx_ring_len; /* in 4-byte words */
+@@ -222,14 +250,40 @@ struct htt_rx_ring_setup_ring {
+ 	__le16 frag_info_offset;
+ } __packed;
+ 
++struct htt_rx_ring_setup_ring64 {
++	__le64 fw_idx_shadow_reg_paddr;
++	__le64 rx_ring_base_paddr;
++	__le16 rx_ring_len; /* in 4-byte words */
++	__le16 rx_ring_bufsize; /* rx skb size - in bytes */
++	__le16 flags; /* %HTT_RX_RING_FLAGS_ */
++	__le16 fw_idx_init_val;
++
++	/* the following offsets are in 4-byte units */
++	__le16 mac80211_hdr_offset;
++	__le16 msdu_payload_offset;
++	__le16 ppdu_start_offset;
++	__le16 ppdu_end_offset;
++	__le16 mpdu_start_offset;
++	__le16 mpdu_end_offset;
++	__le16 msdu_start_offset;
++	__le16 msdu_end_offset;
++	__le16 rx_attention_offset;
++	__le16 frag_info_offset;
++} __packed;
++
+ struct htt_rx_ring_setup_hdr {
+ 	u8 num_rings; /* supported values: 1, 2 */
+ 	__le16 rsvd0;
+ } __packed;
+ 
+-struct htt_rx_ring_setup {
++struct htt_rx_ring_setup_32 {
+ 	struct htt_rx_ring_setup_hdr hdr;
+-	struct htt_rx_ring_setup_ring rings[0];
++	struct htt_rx_ring_setup_ring32 rings[0];
++} __packed;
++
++struct htt_rx_ring_setup_64 {
++	struct htt_rx_ring_setup_hdr hdr;
++	struct htt_rx_ring_setup_ring64 rings[0];
+ } __packed;
+ 
+ /*
+@@ -480,12 +534,18 @@ struct htt_ver_resp {
+ 	u8 rsvd0;
+ } __packed;
+ 
++#define HTT_MGMT_TX_CMPL_FLAG_ACK_RSSI BIT(0)
++
++#define HTT_MGMT_TX_CMPL_INFO_ACK_RSSI_MASK	GENMASK(7, 0)
++
+ struct htt_mgmt_tx_completion {
+ 	u8 rsvd0;
+ 	u8 rsvd1;
+-	u8 rsvd2;
++	u8 flags;
+ 	__le32 desc_id;
+ 	__le32 status;
++	__le32 ppdu_id;
++	__le32 info;
+ } __packed;
+ 
+ #define HTT_RX_INDICATION_INFO0_EXT_TID_MASK  (0x1F)
+@@ -646,6 +706,15 @@ struct htt_rx_indication {
+ 	struct htt_rx_indication_mpdu_range mpdu_ranges[0];
+ } __packed;
+ 
++/* High latency version of the RX indication */
++struct htt_rx_indication_hl {
++	struct htt_rx_indication_hdr hdr;
++	struct htt_rx_indication_ppdu ppdu;
++	struct htt_rx_indication_prefix prefix;
++	struct fw_rx_desc_hl fw_desc;
++	struct htt_rx_indication_mpdu_range mpdu_ranges[0];
++} __packed;
++
+ static inline struct htt_rx_indication_mpdu_range *
+ 		htt_rx_ind_get_mpdu_ranges(struct htt_rx_indication *rx_ind)
+ {
+@@ -658,6 +727,18 @@ static inline struct htt_rx_indication_m
+ 	return ptr;
+ }
+ 
++static inline struct htt_rx_indication_mpdu_range *
++	htt_rx_ind_get_mpdu_ranges_hl(struct htt_rx_indication_hl *rx_ind)
++{
++	void *ptr = rx_ind;
++
++	ptr += sizeof(rx_ind->hdr)
++	     + sizeof(rx_ind->ppdu)
++	     + sizeof(rx_ind->prefix)
++	     + sizeof(rx_ind->fw_desc);
++	return ptr;
++}
++
+ enum htt_rx_flush_mpdu_status {
+ 	HTT_RX_FLUSH_MPDU_DISCARD = 0,
+ 	HTT_RX_FLUSH_MPDU_REORDER = 1,
+@@ -855,13 +936,23 @@ struct htt_rx_in_ord_msdu_desc {
+ 	u8 reserved;
+ } __packed;
+ 
++struct htt_rx_in_ord_msdu_desc_ext {
++	__le64 msdu_paddr;
++	__le16 msdu_len;
++	u8 fw_desc;
++	u8 reserved;
++} __packed;
++
+ struct htt_rx_in_ord_ind {
+ 	u8 info;
+ 	__le16 peer_id;
+ 	u8 vdev_id;
+ 	u8 reserved;
+ 	__le16 msdu_count;
+-	struct htt_rx_in_ord_msdu_desc msdu_descs[0];
++	union {
++		struct htt_rx_in_ord_msdu_desc msdu_descs32[0];
++		struct htt_rx_in_ord_msdu_desc_ext msdu_descs64[0];
++	} __packed;
+ } __packed;
+ 
+ #define HTT_RX_IN_ORD_IND_INFO_TID_MASK		0x0000001f
+@@ -1351,7 +1442,7 @@ struct htt_q_state_conf {
+ 	u8 pad[2];
+ } __packed;
+ 
+-struct htt_frag_desc_bank_cfg {
++struct htt_frag_desc_bank_cfg32 {
+ 	u8 info; /* HTT_FRAG_DESC_BANK_CFG_INFO_ */
+ 	u8 num_banks;
+ 	u8 desc_size;
+@@ -1360,6 +1451,15 @@ struct htt_frag_desc_bank_cfg {
+ 	struct htt_q_state_conf q_state;
+ } __packed;
+ 
++struct htt_frag_desc_bank_cfg64 {
++	u8 info; /* HTT_FRAG_DESC_BANK_CFG_INFO_ */
++	u8 num_banks;
++	u8 desc_size;
++	__le64 bank_base_addrs[HTT_FRAG_DESC_BANK_MAX];
++	struct htt_frag_desc_bank_id bank_id[HTT_FRAG_DESC_BANK_MAX];
++	struct htt_q_state_conf q_state;
++} __packed;
++
+ #define HTT_TX_Q_STATE_ENTRY_COEFFICIENT	128
+ #define HTT_TX_Q_STATE_ENTRY_FACTOR_MASK	0x3f
+ #define HTT_TX_Q_STATE_ENTRY_FACTOR_LSB		0
+@@ -1497,6 +1597,23 @@ struct htt_peer_tx_stats {
+ 	u8 payload[0];
+ } __packed;
+ 
++#define ATH10K_10_2_TX_STATS_OFFSET	136
++#define PEER_STATS_FOR_NO_OF_PPDUS	4
++
++struct ath10k_10_2_peer_tx_stats {
++	u8 ratecode[PEER_STATS_FOR_NO_OF_PPDUS];
++	u8 success_pkts[PEER_STATS_FOR_NO_OF_PPDUS];
++	__le16 success_bytes[PEER_STATS_FOR_NO_OF_PPDUS];
++	u8 retry_pkts[PEER_STATS_FOR_NO_OF_PPDUS];
++	__le16 retry_bytes[PEER_STATS_FOR_NO_OF_PPDUS];
++	u8 failed_pkts[PEER_STATS_FOR_NO_OF_PPDUS];
++	__le16 failed_bytes[PEER_STATS_FOR_NO_OF_PPDUS];
++	u8 flags[PEER_STATS_FOR_NO_OF_PPDUS];
++	__le32 tx_duration;
++	u8 tx_ppdu_cnt;
++	u8 peer_id;
++} __packed;
++
+ union htt_rx_pn_t {
+ 	/* WEP: 24-bit PN */
+ 	u32 pn24;
+@@ -1514,11 +1631,13 @@ struct htt_cmd {
+ 		struct htt_ver_req ver_req;
+ 		struct htt_mgmt_tx_desc mgmt_tx;
+ 		struct htt_data_tx_desc data_tx;
+-		struct htt_rx_ring_setup rx_setup;
++		struct htt_rx_ring_setup_32 rx_setup_32;
++		struct htt_rx_ring_setup_64 rx_setup_64;
+ 		struct htt_stats_req stats_req;
+ 		struct htt_oob_sync_req oob_sync_req;
+ 		struct htt_aggr_conf aggr_conf;
+-		struct htt_frag_desc_bank_cfg frag_desc_bank_cfg;
++		struct htt_frag_desc_bank_cfg32 frag_desc_bank_cfg32;
++		struct htt_frag_desc_bank_cfg64 frag_desc_bank_cfg64;
+ 		struct htt_tx_fetch_resp tx_fetch_resp;
+ 	};
+ } __packed;
+@@ -1530,6 +1649,7 @@ struct htt_resp {
+ 		struct htt_mgmt_tx_completion mgmt_tx_completion;
+ 		struct htt_data_tx_completion data_tx_completion;
+ 		struct htt_rx_indication rx_ind;
++		struct htt_rx_indication_hl rx_ind_hl;
+ 		struct htt_rx_fragment_indication rx_frag_ind;
+ 		struct htt_rx_peer_map peer_map;
+ 		struct htt_rx_peer_unmap peer_unmap;
+@@ -1557,6 +1677,7 @@ struct htt_resp {
+ struct htt_tx_done {
+ 	u16 msdu_id;
+ 	u16 status;
++	u8 ack_rssi;
+ };
+ 
+ enum htt_tx_compl_state {
+@@ -1576,13 +1697,20 @@ struct htt_peer_unmap_event {
+ 	u16 peer_id;
+ };
+ 
+-struct ath10k_htt_txbuf {
++struct ath10k_htt_txbuf_32 {
+ 	struct htt_data_tx_desc_frag frags[2];
+ 	struct ath10k_htc_hdr htc_hdr;
+ 	struct htt_cmd_hdr cmd_hdr;
+ 	struct htt_data_tx_desc cmd_tx;
+ } __packed;
+ 
++struct ath10k_htt_txbuf_64 {
++	struct htt_data_tx_desc_frag frags[2];
++	struct ath10k_htc_hdr htc_hdr;
++	struct htt_cmd_hdr cmd_hdr;
++	struct htt_data_tx_desc_64 cmd_tx;
++} __packed;
++
+ struct ath10k_htt {
+ 	struct ath10k *ar;
+ 	enum ath10k_htc_ep_id eid;
+@@ -1627,7 +1755,10 @@ struct ath10k_htt {
+ 		 * rx buffers the host SW provides for the MAC HW to
+ 		 * fill.
+ 		 */
+-		__le32 *paddrs_ring;
++		union {
++			__le64 *paddrs_ring_64;
++			__le32 *paddrs_ring_32;
++		};
+ 
+ 		/*
+ 		 * Base address of ring, as a "physical" device address
+@@ -1695,7 +1826,7 @@ struct ath10k_htt {
+ 	/* This is used to group tx/rx completions separately and process them
+ 	 * in batches to reduce cache stalls
+ 	 */
+-	struct sk_buff_head rx_compl_q;
++	struct sk_buff_head rx_msdus_q;
+ 	struct sk_buff_head rx_in_ord_compl_q;
+ 	struct sk_buff_head tx_fetch_ind_q;
+ 
+@@ -1704,12 +1835,20 @@ struct ath10k_htt {
+ 
+ 	struct {
+ 		dma_addr_t paddr;
+-		struct htt_msdu_ext_desc *vaddr;
++		union {
++			struct htt_msdu_ext_desc *vaddr_desc_32;
++			struct htt_msdu_ext_desc_64 *vaddr_desc_64;
++		};
++		size_t size;
+ 	} frag_desc;
+ 
+ 	struct {
+ 		dma_addr_t paddr;
+-		struct ath10k_htt_txbuf *vaddr;
++		union {
++			struct ath10k_htt_txbuf_32 *vaddr_txbuff_32;
++			struct ath10k_htt_txbuf_64 *vaddr_txbuff_64;
++		};
++		size_t size;
+ 	} txbuf;
+ 
+ 	struct {
+@@ -1724,8 +1863,118 @@ struct ath10k_htt {
+ 	} tx_q_state;
+ 
+ 	bool tx_mem_allocated;
++	const struct ath10k_htt_tx_ops *tx_ops;
++	const struct ath10k_htt_rx_ops *rx_ops;
++};
++
++struct ath10k_htt_tx_ops {
++	int (*htt_send_rx_ring_cfg)(struct ath10k_htt *htt);
++	int (*htt_send_frag_desc_bank_cfg)(struct ath10k_htt *htt);
++	int (*htt_alloc_frag_desc)(struct ath10k_htt *htt);
++	void (*htt_free_frag_desc)(struct ath10k_htt *htt);
++	int (*htt_tx)(struct ath10k_htt *htt, enum ath10k_hw_txrx_mode txmode,
++		      struct sk_buff *msdu);
++	int (*htt_alloc_txbuff)(struct ath10k_htt *htt);
++	void (*htt_free_txbuff)(struct ath10k_htt *htt);
++};
++
++static inline int ath10k_htt_send_rx_ring_cfg(struct ath10k_htt *htt)
++{
++	if (!htt->tx_ops->htt_send_rx_ring_cfg)
++		return -EOPNOTSUPP;
++
++	return htt->tx_ops->htt_send_rx_ring_cfg(htt);
++}
++
++static inline int ath10k_htt_send_frag_desc_bank_cfg(struct ath10k_htt *htt)
++{
++	if (!htt->tx_ops->htt_send_frag_desc_bank_cfg)
++		return -EOPNOTSUPP;
++
++	return htt->tx_ops->htt_send_frag_desc_bank_cfg(htt);
++}
++
++static inline int ath10k_htt_alloc_frag_desc(struct ath10k_htt *htt)
++{
++	if (!htt->tx_ops->htt_alloc_frag_desc)
++		return -EOPNOTSUPP;
++
++	return htt->tx_ops->htt_alloc_frag_desc(htt);
++}
++
++static inline void ath10k_htt_free_frag_desc(struct ath10k_htt *htt)
++{
++	if (htt->tx_ops->htt_free_frag_desc)
++		htt->tx_ops->htt_free_frag_desc(htt);
++}
++
++static inline int ath10k_htt_tx(struct ath10k_htt *htt,
++				enum ath10k_hw_txrx_mode txmode,
++				struct sk_buff *msdu)
++{
++	return htt->tx_ops->htt_tx(htt, txmode, msdu);
++}
++
++static inline int ath10k_htt_alloc_txbuff(struct ath10k_htt *htt)
++{
++	if (!htt->tx_ops->htt_alloc_txbuff)
++		return -EOPNOTSUPP;
++
++	return htt->tx_ops->htt_alloc_txbuff(htt);
++}
++
++static inline void ath10k_htt_free_txbuff(struct ath10k_htt *htt)
++{
++	if (htt->tx_ops->htt_free_txbuff)
++		htt->tx_ops->htt_free_txbuff(htt);
++}
++
++struct ath10k_htt_rx_ops {
++	size_t (*htt_get_rx_ring_size)(struct ath10k_htt *htt);
++	void (*htt_config_paddrs_ring)(struct ath10k_htt *htt, void *vaddr);
++	void (*htt_set_paddrs_ring)(struct ath10k_htt *htt, dma_addr_t paddr,
++				    int idx);
++	void* (*htt_get_vaddr_ring)(struct ath10k_htt *htt);
++	void (*htt_reset_paddrs_ring)(struct ath10k_htt *htt, int idx);
+ };
+ 
++static inline size_t ath10k_htt_get_rx_ring_size(struct ath10k_htt *htt)
++{
++	if (!htt->rx_ops->htt_get_rx_ring_size)
++		return 0;
++
++	return htt->rx_ops->htt_get_rx_ring_size(htt);
++}
++
++static inline void ath10k_htt_config_paddrs_ring(struct ath10k_htt *htt,
++						 void *vaddr)
++{
++	if (htt->rx_ops->htt_config_paddrs_ring)
++		htt->rx_ops->htt_config_paddrs_ring(htt, vaddr);
++}
++
++static inline void ath10k_htt_set_paddrs_ring(struct ath10k_htt *htt,
++					      dma_addr_t paddr,
++					      int idx)
++{
++	if (htt->rx_ops->htt_set_paddrs_ring)
++		htt->rx_ops->htt_set_paddrs_ring(htt, paddr, idx);
++}
++
++static inline void *ath10k_htt_get_vaddr_ring(struct ath10k_htt *htt)
++{
++	if (!htt->rx_ops->htt_get_vaddr_ring)
++		return NULL;
++
++	return htt->rx_ops->htt_get_vaddr_ring(htt);
++}
++
++static inline void ath10k_htt_reset_paddrs_ring(struct ath10k_htt *htt, int idx)
++{
++	if (htt->rx_ops->htt_reset_paddrs_ring)
++		htt->rx_ops->htt_reset_paddrs_ring(htt, idx);
++}
++
+ #define RX_HTT_HDR_STATUS_LEN 64
+ 
+ /* This structure layout is programmed via rx ring setup
+@@ -1754,6 +2003,31 @@ struct htt_rx_desc {
+ 	u8 msdu_payload[0];
+ };
+ 
++#define HTT_RX_DESC_HL_INFO_SEQ_NUM_MASK           0x00000fff
++#define HTT_RX_DESC_HL_INFO_SEQ_NUM_LSB            0
++#define HTT_RX_DESC_HL_INFO_ENCRYPTED_MASK         0x00001000
++#define HTT_RX_DESC_HL_INFO_ENCRYPTED_LSB          12
++#define HTT_RX_DESC_HL_INFO_CHAN_INFO_PRESENT_MASK 0x00002000
++#define HTT_RX_DESC_HL_INFO_CHAN_INFO_PRESENT_LSB  13
++#define HTT_RX_DESC_HL_INFO_MCAST_BCAST_MASK       0x00008000
++#define HTT_RX_DESC_HL_INFO_MCAST_BCAST_LSB        15
++#define HTT_RX_DESC_HL_INFO_FRAGMENT_MASK          0x00010000
++#define HTT_RX_DESC_HL_INFO_FRAGMENT_LSB           16
++#define HTT_RX_DESC_HL_INFO_KEY_ID_OCT_MASK        0x01fe0000
++#define HTT_RX_DESC_HL_INFO_KEY_ID_OCT_LSB         17
++
++struct htt_rx_desc_base_hl {
++	__le32 info; /* HTT_RX_DESC_HL_INFO_ */
++};
++
++struct htt_rx_chan_info {
++	__le16 primary_chan_center_freq_mhz;
++	__le16 contig_chan1_center_freq_mhz;
++	__le16 contig_chan2_center_freq_mhz;
++	u8 phy_mode;
++	u8 reserved;
++} __packed;
++
+ #define HTT_RX_DESC_ALIGN 8
+ 
+ #define HTT_MAC_ADDR_LEN 6
+@@ -1803,8 +2077,6 @@ void ath10k_htt_htc_t2h_msg_handler(stru
+ bool ath10k_htt_t2h_msg_handler(struct ath10k *ar, struct sk_buff *skb);
+ int ath10k_htt_h2t_ver_req_msg(struct ath10k_htt *htt);
+ int ath10k_htt_h2t_stats_req(struct ath10k_htt *htt, u8 mask, u64 cookie);
+-int ath10k_htt_send_frag_desc_bank_cfg(struct ath10k_htt *htt);
+-int ath10k_htt_send_rx_ring_cfg_ll(struct ath10k_htt *htt);
+ int ath10k_htt_h2t_aggr_cfg_msg(struct ath10k_htt *htt,
+ 				u8 max_subfrms_ampdu,
+ 				u8 max_subfrms_amsdu);
+@@ -1829,11 +2101,9 @@ int ath10k_htt_tx_mgmt_inc_pending(struc
+ int ath10k_htt_tx_alloc_msdu_id(struct ath10k_htt *htt, struct sk_buff *skb);
+ void ath10k_htt_tx_free_msdu_id(struct ath10k_htt *htt, u16 msdu_id);
+ int ath10k_htt_mgmt_tx(struct ath10k_htt *htt, struct sk_buff *msdu);
+-int ath10k_htt_tx(struct ath10k_htt *htt,
+-		  enum ath10k_hw_txrx_mode txmode,
+-		  struct sk_buff *msdu);
+ void ath10k_htt_rx_pktlog_completion_handler(struct ath10k *ar,
+ 					     struct sk_buff *skb);
+ int ath10k_htt_txrx_compl_task(struct ath10k *ar, int budget);
+-
++void ath10k_htt_set_tx_ops(struct ath10k_htt *htt);
++void ath10k_htt_set_rx_ops(struct ath10k_htt *htt);
+ #endif
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htt_rx.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htt_rx.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htt_rx.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -24,9 +25,7 @@
+ #include "mac.h"
+ 
+ #include <linux/log2.h>
+-
+-#define HTT_RX_RING_SIZE HTT_RX_RING_SIZE_MAX
+-#define HTT_RX_RING_FILL_LEVEL (((HTT_RX_RING_SIZE) / 2) - 1)
++#include <linux/bitfield.h>
+ 
+ /* when under memory pressure rx ring refill may fail and needs a retry */
+ #define HTT_RX_RING_REFILL_RETRY_MS 50
+@@ -36,7 +35,7 @@
+ static int ath10k_htt_rx_get_csum_state(struct sk_buff *skb);
+ 
+ static struct sk_buff *
+-ath10k_htt_rx_find_skb_paddr(struct ath10k *ar, u32 paddr)
++ath10k_htt_rx_find_skb_paddr(struct ath10k *ar, u64 paddr)
+ {
+ 	struct ath10k_skb_rxcb *rxcb;
+ 
+@@ -84,6 +83,60 @@ static void ath10k_htt_rx_ring_free(stru
+ 	       htt->rx_ring.size * sizeof(htt->rx_ring.netbufs_ring[0]));
+ }
+ 
++static size_t ath10k_htt_get_rx_ring_size_32(struct ath10k_htt *htt)
++{
++	return htt->rx_ring.size * sizeof(htt->rx_ring.paddrs_ring_32);
++}
++
++static size_t ath10k_htt_get_rx_ring_size_64(struct ath10k_htt *htt)
++{
++	return htt->rx_ring.size * sizeof(htt->rx_ring.paddrs_ring_64);
++}
++
++static void ath10k_htt_config_paddrs_ring_32(struct ath10k_htt *htt,
++					     void *vaddr)
++{
++	htt->rx_ring.paddrs_ring_32 = vaddr;
++}
++
++static void ath10k_htt_config_paddrs_ring_64(struct ath10k_htt *htt,
++					     void *vaddr)
++{
++	htt->rx_ring.paddrs_ring_64 = vaddr;
++}
++
++static void ath10k_htt_set_paddrs_ring_32(struct ath10k_htt *htt,
++					  dma_addr_t paddr, int idx)
++{
++	htt->rx_ring.paddrs_ring_32[idx] = __cpu_to_le32(paddr);
++}
++
++static void ath10k_htt_set_paddrs_ring_64(struct ath10k_htt *htt,
++					  dma_addr_t paddr, int idx)
++{
++	htt->rx_ring.paddrs_ring_64[idx] = __cpu_to_le64(paddr);
++}
++
++static void ath10k_htt_reset_paddrs_ring_32(struct ath10k_htt *htt, int idx)
++{
++	htt->rx_ring.paddrs_ring_32[idx] = 0;
++}
++
++static void ath10k_htt_reset_paddrs_ring_64(struct ath10k_htt *htt, int idx)
++{
++	htt->rx_ring.paddrs_ring_64[idx] = 0;
++}
++
++static void *ath10k_htt_get_vaddr_ring_32(struct ath10k_htt *htt)
++{
++	return (void *)htt->rx_ring.paddrs_ring_32;
++}
++
++static void *ath10k_htt_get_vaddr_ring_64(struct ath10k_htt *htt)
++{
++	return (void *)htt->rx_ring.paddrs_ring_64;
++}
++
+ static int __ath10k_htt_rx_ring_fill_n(struct ath10k_htt *htt, int num)
+ {
+ 	struct htt_rx_desc *rx_desc;
+@@ -129,13 +182,13 @@ static int __ath10k_htt_rx_ring_fill_n(s
+ 		rxcb = ATH10K_SKB_RXCB(skb);
+ 		rxcb->paddr = paddr;
+ 		htt->rx_ring.netbufs_ring[idx] = skb;
+-		htt->rx_ring.paddrs_ring[idx] = __cpu_to_le32(paddr);
++		ath10k_htt_set_paddrs_ring(htt, paddr, idx);
+ 		htt->rx_ring.fill_cnt++;
+ 
+ 		if (htt->rx_ring.in_ord_rx) {
+ 			hash_add(htt->rx_ring.skb_table,
+ 				 &ATH10K_SKB_RXCB(skb)->hlist,
+-				 (u32)paddr);
++				 paddr);
+ 		}
+ 
+ 		num--;
+@@ -200,9 +253,9 @@ static void ath10k_htt_rx_msdu_buff_repl
+ 	spin_unlock_bh(&htt->rx_ring.lock);
+ }
+ 
+-static void ath10k_htt_rx_ring_refill_retry(unsigned long arg)
++static void ath10k_htt_rx_ring_refill_retry(struct timer_list *t)
+ {
+-	struct ath10k_htt *htt = (struct ath10k_htt *)arg;
++	struct ath10k_htt *htt = from_timer(htt, t, rx_ring.refill_retry_timer);
+ 
+ 	ath10k_htt_rx_msdu_buff_replenish(htt);
+ }
+@@ -212,6 +265,9 @@ int ath10k_htt_rx_ring_refill(struct ath
+ 	struct ath10k_htt *htt = &ar->htt;
+ 	int ret;
+ 
++	if (ar->is_high_latency)
++		return 0;
++
+ 	spin_lock_bh(&htt->rx_ring.lock);
+ 	ret = ath10k_htt_rx_ring_fill_n(htt, (htt->rx_ring.fill_level -
+ 					      htt->rx_ring.fill_cnt));
+@@ -225,18 +281,20 @@ int ath10k_htt_rx_ring_refill(struct ath
+ 
+ void ath10k_htt_rx_free(struct ath10k_htt *htt)
+ {
++	if (htt->ar->is_high_latency)
++		return;
++
+ 	del_timer_sync(&htt->rx_ring.refill_retry_timer);
+ 
+-	skb_queue_purge(&htt->rx_compl_q);
++	skb_queue_purge(&htt->rx_msdus_q);
+ 	skb_queue_purge(&htt->rx_in_ord_compl_q);
+ 	skb_queue_purge(&htt->tx_fetch_ind_q);
+ 
+ 	ath10k_htt_rx_ring_free(htt);
+ 
+ 	dma_free_coherent(htt->ar->dev,
+-			  (htt->rx_ring.size *
+-			   sizeof(htt->rx_ring.paddrs_ring)),
+-			  htt->rx_ring.paddrs_ring,
++			  ath10k_htt_get_rx_ring_size(htt),
++			  ath10k_htt_get_vaddr_ring(htt),
+ 			  htt->rx_ring.base_paddr);
+ 
+ 	dma_free_coherent(htt->ar->dev,
+@@ -263,7 +321,7 @@ static inline struct sk_buff *ath10k_htt
+ 	idx = htt->rx_ring.sw_rd_idx.msdu_payld;
+ 	msdu = htt->rx_ring.netbufs_ring[idx];
+ 	htt->rx_ring.netbufs_ring[idx] = NULL;
+-	htt->rx_ring.paddrs_ring[idx] = 0;
++	ath10k_htt_reset_paddrs_ring(htt, idx);
+ 
+ 	idx++;
+ 	idx &= htt->rx_ring.size_mask;
+@@ -383,7 +441,7 @@ static int ath10k_htt_rx_amsdu_pop(struc
+ }
+ 
+ static struct sk_buff *ath10k_htt_rx_pop_paddr(struct ath10k_htt *htt,
+-					       u32 paddr)
++					       u64 paddr)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	struct ath10k_skb_rxcb *rxcb;
+@@ -408,12 +466,12 @@ static struct sk_buff *ath10k_htt_rx_pop
+ 	return msdu;
+ }
+ 
+-static int ath10k_htt_rx_pop_paddr_list(struct ath10k_htt *htt,
+-					struct htt_rx_in_ord_ind *ev,
+-					struct sk_buff_head *list)
++static int ath10k_htt_rx_pop_paddr32_list(struct ath10k_htt *htt,
++					  struct htt_rx_in_ord_ind *ev,
++					  struct sk_buff_head *list)
+ {
+ 	struct ath10k *ar = htt->ar;
+-	struct htt_rx_in_ord_msdu_desc *msdu_desc = ev->msdu_descs;
++	struct htt_rx_in_ord_msdu_desc *msdu_desc = ev->msdu_descs32;
+ 	struct htt_rx_desc *rxd;
+ 	struct sk_buff *msdu;
+ 	int msdu_count;
+@@ -458,14 +516,66 @@ static int ath10k_htt_rx_pop_paddr_list(
+ 	return 0;
+ }
+ 
++static int ath10k_htt_rx_pop_paddr64_list(struct ath10k_htt *htt,
++					  struct htt_rx_in_ord_ind *ev,
++					  struct sk_buff_head *list)
++{
++	struct ath10k *ar = htt->ar;
++	struct htt_rx_in_ord_msdu_desc_ext *msdu_desc = ev->msdu_descs64;
++	struct htt_rx_desc *rxd;
++	struct sk_buff *msdu;
++	int msdu_count;
++	bool is_offload;
++	u64 paddr;
++
++	lockdep_assert_held(&htt->rx_ring.lock);
++
++	msdu_count = __le16_to_cpu(ev->msdu_count);
++	is_offload = !!(ev->info & HTT_RX_IN_ORD_IND_INFO_OFFLOAD_MASK);
++
++	while (msdu_count--) {
++		paddr = __le64_to_cpu(msdu_desc->msdu_paddr);
++		msdu = ath10k_htt_rx_pop_paddr(htt, paddr);
++		if (!msdu) {
++			__skb_queue_purge(list);
++			return -ENOENT;
++		}
++
++		__skb_queue_tail(list, msdu);
++
++		if (!is_offload) {
++			rxd = (void *)msdu->data;
++
++			trace_ath10k_htt_rx_desc(ar, rxd, sizeof(*rxd));
++
++			skb_put(msdu, sizeof(*rxd));
++			skb_pull(msdu, sizeof(*rxd));
++			skb_put(msdu, __le16_to_cpu(msdu_desc->msdu_len));
++
++			if (!(__le32_to_cpu(rxd->attention.flags) &
++			      RX_ATTENTION_FLAGS_MSDU_DONE)) {
++				ath10k_warn(htt->ar, "tried to pop an incomplete frame, oops!\n");
++				return -EIO;
++			}
++		}
++
++		msdu_desc++;
++	}
++
++	return 0;
++}
++
+ int ath10k_htt_rx_alloc(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	dma_addr_t paddr;
+-	void *vaddr;
++	void *vaddr, *vaddr_ring;
+ 	size_t size;
+ 	struct timer_list *timer = &htt->rx_ring.refill_retry_timer;
+ 
++	if (ar->is_high_latency)
++		return 0;
++
+ 	htt->rx_confused = false;
+ 
+ 	/* XXX: The fill level could be changed during runtime in response to
+@@ -473,7 +583,7 @@ int ath10k_htt_rx_alloc(struct ath10k_ht
+ 	 */
+ 	htt->rx_ring.size = HTT_RX_RING_SIZE;
+ 	htt->rx_ring.size_mask = htt->rx_ring.size - 1;
+-	htt->rx_ring.fill_level = HTT_RX_RING_FILL_LEVEL;
++	htt->rx_ring.fill_level = ar->hw_params.rx_ring_fill_level;
+ 
+ 	if (!is_power_of_2(htt->rx_ring.size)) {
+ 		ath10k_warn(ar, "htt rx ring size is not power of 2\n");
+@@ -486,13 +596,13 @@ int ath10k_htt_rx_alloc(struct ath10k_ht
+ 	if (!htt->rx_ring.netbufs_ring)
+ 		goto err_netbuf;
+ 
+-	size = htt->rx_ring.size * sizeof(htt->rx_ring.paddrs_ring);
++	size = ath10k_htt_get_rx_ring_size(htt);
+ 
+-	vaddr = dma_alloc_coherent(htt->ar->dev, size, &paddr, GFP_KERNEL);
+-	if (!vaddr)
++	vaddr_ring = dma_alloc_coherent(htt->ar->dev, size, &paddr, GFP_KERNEL);
++	if (!vaddr_ring)
+ 		goto err_dma_ring;
+ 
+-	htt->rx_ring.paddrs_ring = vaddr;
++	ath10k_htt_config_paddrs_ring(htt, vaddr_ring);
+ 	htt->rx_ring.base_paddr = paddr;
+ 
+ 	vaddr = dma_alloc_coherent(htt->ar->dev,
+@@ -507,7 +617,7 @@ int ath10k_htt_rx_alloc(struct ath10k_ht
+ 	*htt->rx_ring.alloc_idx.vaddr = 0;
+ 
+ 	/* Initialize the Rx refill retry timer */
+-	setup_timer(timer, ath10k_htt_rx_ring_refill_retry, (unsigned long)htt);
++	timer_setup(timer, ath10k_htt_rx_ring_refill_retry, 0);
+ 
+ 	spin_lock_init(&htt->rx_ring.lock);
+ 
+@@ -515,7 +625,7 @@ int ath10k_htt_rx_alloc(struct ath10k_ht
+ 	htt->rx_ring.sw_rd_idx.msdu_payld = 0;
+ 	hash_init(htt->rx_ring.skb_table);
+ 
+-	skb_queue_head_init(&htt->rx_compl_q);
++	skb_queue_head_init(&htt->rx_msdus_q);
+ 	skb_queue_head_init(&htt->rx_in_ord_compl_q);
+ 	skb_queue_head_init(&htt->tx_fetch_ind_q);
+ 	atomic_set(&htt->num_mpdus_ready, 0);
+@@ -526,9 +636,8 @@ int ath10k_htt_rx_alloc(struct ath10k_ht
+ 
+ err_dma_idx:
+ 	dma_free_coherent(htt->ar->dev,
+-			  (htt->rx_ring.size *
+-			   sizeof(htt->rx_ring.paddrs_ring)),
+-			  htt->rx_ring.paddrs_ring,
++			  ath10k_htt_get_rx_ring_size(htt),
++			  vaddr_ring,
+ 			  htt->rx_ring.base_paddr);
+ err_dma_ring:
+ 	kfree(htt->rx_ring.netbufs_ring);
+@@ -566,18 +675,16 @@ static int ath10k_htt_rx_crypto_param_le
+ 
+ #define MICHAEL_MIC_LEN 8
+ 
+-static int ath10k_htt_rx_crypto_tail_len(struct ath10k *ar,
+-					 enum htt_rx_mpdu_encrypt_type type)
++static int ath10k_htt_rx_crypto_mic_len(struct ath10k *ar,
++					enum htt_rx_mpdu_encrypt_type type)
+ {
+ 	switch (type) {
+ 	case HTT_RX_MPDU_ENCRYPT_NONE:
+-		return 0;
+ 	case HTT_RX_MPDU_ENCRYPT_WEP40:
+ 	case HTT_RX_MPDU_ENCRYPT_WEP104:
+-		return IEEE80211_WEP_ICV_LEN;
+ 	case HTT_RX_MPDU_ENCRYPT_TKIP_WITHOUT_MIC:
+ 	case HTT_RX_MPDU_ENCRYPT_TKIP_WPA:
+-		return IEEE80211_TKIP_ICV_LEN;
++		return 0;
+ 	case HTT_RX_MPDU_ENCRYPT_AES_CCM_WPA2:
+ 		return IEEE80211_CCMP_MIC_LEN;
+ 	case HTT_RX_MPDU_ENCRYPT_AES_CCM256_WPA2:
+@@ -594,6 +701,31 @@ static int ath10k_htt_rx_crypto_tail_len
+ 	return 0;
+ }
+ 
++static int ath10k_htt_rx_crypto_icv_len(struct ath10k *ar,
++					enum htt_rx_mpdu_encrypt_type type)
++{
++	switch (type) {
++	case HTT_RX_MPDU_ENCRYPT_NONE:
++	case HTT_RX_MPDU_ENCRYPT_AES_CCM_WPA2:
++	case HTT_RX_MPDU_ENCRYPT_AES_CCM256_WPA2:
++	case HTT_RX_MPDU_ENCRYPT_AES_GCMP_WPA2:
++	case HTT_RX_MPDU_ENCRYPT_AES_GCMP256_WPA2:
++		return 0;
++	case HTT_RX_MPDU_ENCRYPT_WEP40:
++	case HTT_RX_MPDU_ENCRYPT_WEP104:
++		return IEEE80211_WEP_ICV_LEN;
++	case HTT_RX_MPDU_ENCRYPT_TKIP_WITHOUT_MIC:
++	case HTT_RX_MPDU_ENCRYPT_TKIP_WPA:
++		return IEEE80211_TKIP_ICV_LEN;
++	case HTT_RX_MPDU_ENCRYPT_WEP128:
++	case HTT_RX_MPDU_ENCRYPT_WAPI:
++		break;
++	}
++
++	ath10k_warn(ar, "unsupported encryption type %d\n", type);
++	return 0;
++}
++
+ struct amsdu_subframe_hdr {
+ 	u8 dst[ETH_ALEN];
+ 	u8 src[ETH_ALEN];
+@@ -602,6 +734,28 @@ struct amsdu_subframe_hdr {
+ 
+ #define GROUP_ID_IS_SU_MIMO(x) ((x) == 0 || (x) == 63)
+ 
++static inline u8 ath10k_bw_to_mac80211_bw(u8 bw)
++{
++	u8 ret = 0;
++
++	switch (bw) {
++	case 0:
++		ret = RATE_INFO_BW_20;
++		break;
++	case 1:
++		ret = RATE_INFO_BW_40;
++		break;
++	case 2:
++		ret = RATE_INFO_BW_80;
++		break;
++	case 3:
++		ret = RATE_INFO_BW_160;
++		break;
++	}
++
++	return ret;
++}
++
+ static void ath10k_htt_rx_h_rates(struct ath10k *ar,
+ 				  struct ieee80211_rx_status *status,
+ 				  struct htt_rx_desc *rxd)
+@@ -704,23 +858,7 @@ static void ath10k_htt_rx_h_rates(struct
+ 		if (sgi)
+ 			status->enc_flags |= RX_ENC_FLAG_SHORT_GI;
+ 
+-		switch (bw) {
+-		/* 20MHZ */
+-		case 0:
+-			break;
+-		/* 40MHZ */
+-		case 1:
+-			status->bw = RATE_INFO_BW_40;
+-			break;
+-		/* 80MHZ */
+-		case 2:
+-			status->bw = RATE_INFO_BW_80;
+-			break;
+-		case 3:
+-			status->bw = RATE_INFO_BW_160;
+-			break;
+-		}
+-
++		status->bw = ath10k_bw_to_mac80211_bw(bw);
+ 		status->encoding = RX_ENC_VHT;
+ 		break;
+ 	default:
+@@ -951,16 +1089,25 @@ static char *ath10k_get_tid(struct ieee8
+ 	return out;
+ }
+ 
+-static void ath10k_process_rx(struct ath10k *ar,
+-			      struct ieee80211_rx_status *rx_status,
+-			      struct sk_buff *skb)
++static void ath10k_htt_rx_h_queue_msdu(struct ath10k *ar,
++				       struct ieee80211_rx_status *rx_status,
++				       struct sk_buff *skb)
++{
++	struct ieee80211_rx_status *status;
++
++	status = IEEE80211_SKB_RXCB(skb);
++	*status = *rx_status;
++
++	__skb_queue_tail(&ar->htt.rx_msdus_q, skb);
++}
++
++static void ath10k_process_rx(struct ath10k *ar, struct sk_buff *skb)
+ {
+ 	struct ieee80211_rx_status *status;
+ 	struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;
+ 	char tid[32];
+ 
+ 	status = IEEE80211_SKB_RXCB(skb);
+-	*status = *rx_status;
+ 
+ 	ath10k_dbg(ar, ATH10K_DBG_DATA,
+ 		   "rx skb %pK len %u peer %pM %s %s sn %u %s%s%s%s%s%s %srate_idx %u vht_nss %u freq %u band %u flag 0x%x fcs-err %i mic-err %i amsdu-more %i\n",
+@@ -1063,25 +1210,27 @@ static void ath10k_htt_rx_h_undecap_raw(
+ 	/* Tail */
+ 	if (status->flag & RX_FLAG_IV_STRIPPED) {
+ 		skb_trim(msdu, msdu->len -
+-			 ath10k_htt_rx_crypto_tail_len(ar, enctype));
++			 ath10k_htt_rx_crypto_mic_len(ar, enctype));
++
++		skb_trim(msdu, msdu->len -
++			 ath10k_htt_rx_crypto_icv_len(ar, enctype));
+ 	} else {
+ 		/* MIC */
+-		if ((status->flag & RX_FLAG_MIC_STRIPPED) &&
+-		    enctype == HTT_RX_MPDU_ENCRYPT_AES_CCM_WPA2)
+-			skb_trim(msdu, msdu->len - 8);
++		if (status->flag & RX_FLAG_MIC_STRIPPED)
++			skb_trim(msdu, msdu->len -
++				 ath10k_htt_rx_crypto_mic_len(ar, enctype));
+ 
+ 		/* ICV */
+-		if (status->flag & RX_FLAG_ICV_STRIPPED &&
+-		    enctype != HTT_RX_MPDU_ENCRYPT_AES_CCM_WPA2)
++		if (status->flag & RX_FLAG_ICV_STRIPPED)
+ 			skb_trim(msdu, msdu->len -
+-				 ath10k_htt_rx_crypto_tail_len(ar, enctype));
++				 ath10k_htt_rx_crypto_icv_len(ar, enctype));
+ 	}
+ 
+ 	/* MMIC */
+ 	if ((status->flag & RX_FLAG_MMIC_STRIPPED) &&
+ 	    !ieee80211_has_morefrags(hdr->frame_control) &&
+ 	    enctype == HTT_RX_MPDU_ENCRYPT_TKIP_WPA)
+-		skb_trim(msdu, msdu->len - 8);
++		skb_trim(msdu, msdu->len - MICHAEL_MIC_LEN);
+ 
+ 	/* Head */
+ 	if (status->flag & RX_FLAG_IV_STRIPPED) {
+@@ -1370,7 +1519,9 @@ static void ath10k_htt_rx_h_csum_offload
+ static void ath10k_htt_rx_h_mpdu(struct ath10k *ar,
+ 				 struct sk_buff_head *amsdu,
+ 				 struct ieee80211_rx_status *status,
+-				 bool fill_crypt_header)
++				 bool fill_crypt_header,
++				 u8 *rx_hdr,
++				 enum ath10k_pkt_rx_err *err)
+ {
+ 	struct sk_buff *first;
+ 	struct sk_buff *last;
+@@ -1406,6 +1557,9 @@ static void ath10k_htt_rx_h_mpdu(struct
+ 	hdr = (void *)rxd->rx_hdr_status;
+ 	memcpy(first_hdr, hdr, RX_HTT_HDR_STATUS_LEN);
+ 
++	if (rx_hdr)
++		memcpy(rx_hdr, hdr, RX_HTT_HDR_STATUS_LEN);
++
+ 	/* Each A-MSDU subframe will use the original header as the base and be
+ 	 * reported as a separate MSDU so strip the A-MSDU bit from QoS Ctl.
+ 	 */
+@@ -1449,6 +1603,17 @@ static void ath10k_htt_rx_h_mpdu(struct
+ 	if (has_tkip_err)
+ 		status->flag |= RX_FLAG_MMIC_ERROR;
+ 
++	if (err) {
++		if (has_fcs_err)
++			*err = ATH10K_PKT_RX_ERR_FCS;
++		else if (has_tkip_err)
++			*err = ATH10K_PKT_RX_ERR_TKIP;
++		else if (has_crypto_err)
++			*err = ATH10K_PKT_RX_ERR_CRYPT;
++		else if (has_peer_idx_invalid)
++			*err = ATH10K_PKT_RX_ERR_PEER_IDX_INVAL;
++	}
++
+ 	/* Firmware reports all necessary management frames via WMI already.
+ 	 * They are not reported to monitor interfaces at all so pass the ones
+ 	 * coming via HTT to monitor interfaces instead. This simplifies
+@@ -1492,7 +1657,7 @@ static void ath10k_htt_rx_h_mpdu(struct
+ 	}
+ }
+ 
+-static void ath10k_htt_rx_h_deliver(struct ath10k *ar,
++static void ath10k_htt_rx_h_enqueue(struct ath10k *ar,
+ 				    struct sk_buff_head *amsdu,
+ 				    struct ieee80211_rx_status *status)
+ {
+@@ -1515,15 +1680,17 @@ static void ath10k_htt_rx_h_deliver(stru
+ 			status->flag |= RX_FLAG_ALLOW_SAME_PN;
+ 		}
+ 
+-		ath10k_process_rx(ar, status, msdu);
++		ath10k_htt_rx_h_queue_msdu(ar, status, msdu);
+ 	}
+ }
+ 
+-static int ath10k_unchain_msdu(struct sk_buff_head *amsdu)
++static int ath10k_unchain_msdu(struct sk_buff_head *amsdu,
++			       unsigned long int *unchain_cnt)
+ {
+ 	struct sk_buff *skb, *first;
+ 	int space;
+ 	int total_len = 0;
++	int amsdu_len = skb_queue_len(amsdu);
+ 
+ 	/* TODO:  Might could optimize this by using
+ 	 * skb_try_coalesce or similar method to
+@@ -1559,11 +1726,16 @@ static int ath10k_unchain_msdu(struct sk
+ 	}
+ 
+ 	__skb_queue_head(amsdu, first);
++
++	*unchain_cnt += amsdu_len - 1;
++
+ 	return 0;
+ }
+ 
+ static void ath10k_htt_rx_h_unchain(struct ath10k *ar,
+-				    struct sk_buff_head *amsdu)
++				    struct sk_buff_head *amsdu,
++				    unsigned long int *drop_cnt,
++				    unsigned long int *unchain_cnt)
+ {
+ 	struct sk_buff *first;
+ 	struct htt_rx_desc *rxd;
+@@ -1581,11 +1753,12 @@ static void ath10k_htt_rx_h_unchain(stru
+ 	 */
+ 	if (decap != RX_MSDU_DECAP_RAW ||
+ 	    skb_queue_len(amsdu) != 1 + rxd->frag_info.ring2_more_count) {
++		*drop_cnt += skb_queue_len(amsdu);
+ 		__skb_queue_purge(amsdu);
+ 		return;
+ 	}
+ 
+-	ath10k_unchain_msdu(amsdu);
++	ath10k_unchain_msdu(amsdu, unchain_cnt);
+ }
+ 
+ static bool ath10k_htt_rx_amsdu_allowed(struct ath10k *ar,
+@@ -1611,7 +1784,8 @@ static bool ath10k_htt_rx_amsdu_allowed(
+ 
+ static void ath10k_htt_rx_h_filter(struct ath10k *ar,
+ 				   struct sk_buff_head *amsdu,
+-				   struct ieee80211_rx_status *rx_status)
++				   struct ieee80211_rx_status *rx_status,
++				   unsigned long int *drop_cnt)
+ {
+ 	if (skb_queue_empty(amsdu))
+ 		return;
+@@ -1619,6 +1793,9 @@ static void ath10k_htt_rx_h_filter(struc
+ 	if (ath10k_htt_rx_amsdu_allowed(ar, amsdu, rx_status))
+ 		return;
+ 
++	if (drop_cnt)
++		*drop_cnt += skb_queue_len(amsdu);
++
+ 	__skb_queue_purge(amsdu);
+ }
+ 
+@@ -1627,7 +1804,13 @@ static int ath10k_htt_rx_handle_amsdu(st
+ 	struct ath10k *ar = htt->ar;
+ 	struct ieee80211_rx_status *rx_status = &htt->rx_status;
+ 	struct sk_buff_head amsdu;
+-	int ret, num_msdus;
++	int ret;
++	unsigned long int drop_cnt = 0;
++	unsigned long int unchain_cnt = 0;
++	unsigned long int drop_cnt_filter = 0;
++	unsigned long int msdus_to_queue, num_msdus;
++	enum ath10k_pkt_rx_err err = ATH10K_PKT_RX_ERR_MAX;
++	u8 first_hdr[RX_HTT_HDR_STATUS_LEN];
+ 
+ 	__skb_queue_head_init(&amsdu);
+ 
+@@ -1650,29 +1833,153 @@ static int ath10k_htt_rx_handle_amsdu(st
+ 	}
+ 
+ 	num_msdus = skb_queue_len(&amsdu);
++
+ 	ath10k_htt_rx_h_ppdu(ar, &amsdu, rx_status, 0xffff);
+ 
+ 	/* only for ret = 1 indicates chained msdus */
+ 	if (ret > 0)
+-		ath10k_htt_rx_h_unchain(ar, &amsdu);
++		ath10k_htt_rx_h_unchain(ar, &amsdu, &drop_cnt, &unchain_cnt);
++
++	ath10k_htt_rx_h_filter(ar, &amsdu, rx_status, &drop_cnt_filter);
++	ath10k_htt_rx_h_mpdu(ar, &amsdu, rx_status, true, first_hdr, &err);
++	msdus_to_queue = skb_queue_len(&amsdu);
++	ath10k_htt_rx_h_enqueue(ar, &amsdu, rx_status);
++
++	ath10k_sta_update_rx_tid_stats(ar, first_hdr, num_msdus, err,
++				       unchain_cnt, drop_cnt, drop_cnt_filter,
++				       msdus_to_queue);
++
++	return 0;
++}
++
++static bool ath10k_htt_rx_proc_rx_ind_hl(struct ath10k_htt *htt,
++					 struct htt_rx_indication_hl *rx,
++					 struct sk_buff *skb)
++{
++	struct ath10k *ar = htt->ar;
++	struct ath10k_peer *peer;
++	struct htt_rx_indication_mpdu_range *mpdu_ranges;
++	struct fw_rx_desc_hl *fw_desc;
++	struct ieee80211_hdr *hdr;
++	struct ieee80211_rx_status *rx_status;
++	u16 peer_id;
++	u8 rx_desc_len;
++	int num_mpdu_ranges;
++	size_t tot_hdr_len;
++	struct ieee80211_channel *ch;
++
++	peer_id = __le16_to_cpu(rx->hdr.peer_id);
++
++	peer = ath10k_peer_find_by_id(ar, peer_id);
++	if (!peer)
++		ath10k_warn(ar, "Got RX ind from invalid peer: %u\n", peer_id);
++
++	num_mpdu_ranges = MS(__le32_to_cpu(rx->hdr.info1),
++			     HTT_RX_INDICATION_INFO1_NUM_MPDU_RANGES);
++	mpdu_ranges = htt_rx_ind_get_mpdu_ranges_hl(rx);
++	fw_desc = &rx->fw_desc;
++	rx_desc_len = fw_desc->len;
++
++	/* I have not yet seen any case where num_mpdu_ranges > 1.
++	 * qcacld does not seem handle that case either, so we introduce the
++	 * same limitiation here as well.
++	 */
++	if (num_mpdu_ranges > 1)
++		ath10k_warn(ar,
++			    "Unsupported number of MPDU ranges: %d, ignoring all but the first\n",
++			    num_mpdu_ranges);
++
++	if (mpdu_ranges->mpdu_range_status !=
++	    HTT_RX_IND_MPDU_STATUS_OK) {
++		ath10k_warn(ar, "MPDU range status: %d\n",
++			    mpdu_ranges->mpdu_range_status);
++		goto err;
++	}
++
++	/* Strip off all headers before the MAC header before delivery to
++	 * mac80211
++	 */
++	tot_hdr_len = sizeof(struct htt_resp_hdr) + sizeof(rx->hdr) +
++		      sizeof(rx->ppdu) + sizeof(rx->prefix) +
++		      sizeof(rx->fw_desc) +
++		      sizeof(*mpdu_ranges) * num_mpdu_ranges + rx_desc_len;
++	skb_pull(skb, tot_hdr_len);
++
++	hdr = (struct ieee80211_hdr *)skb->data;
++	rx_status = IEEE80211_SKB_RXCB(skb);
++	rx_status->chains |= BIT(0);
++
++	if (ar->hif.bus == ATH10K_BUS_SDIO) {
++		/* SDIO firmware does not provide signal */
++		rx_status->signal = 0;
++		rx_status->flag |= RX_FLAG_NO_SIGNAL_VAL;
++	} else {
++		rx_status->signal = ATH10K_DEFAULT_NOISE_FLOOR +
++			rx->ppdu.combined_rssi;
++		rx_status->flag &= ~RX_FLAG_NO_SIGNAL_VAL;
++	}
++
++	spin_lock_bh(&ar->data_lock);
++	ch = ar->scan_channel;
++	if (!ch)
++		ch = ar->rx_channel;
++	if (!ch)
++		ch = ath10k_htt_rx_h_any_channel(ar);
++	if (!ch)
++		ch = ar->tgt_oper_chan;
++	spin_unlock_bh(&ar->data_lock);
++
++	if (ch) {
++		rx_status->band = ch->band;
++		rx_status->freq = ch->center_freq;
++	}
++	if (rx->fw_desc.flags & FW_RX_DESC_FLAGS_LAST_MSDU)
++		rx_status->flag &= ~RX_FLAG_AMSDU_MORE;
++	else
++		rx_status->flag |= RX_FLAG_AMSDU_MORE;
+ 
+-	ath10k_htt_rx_h_filter(ar, &amsdu, rx_status);
+-	ath10k_htt_rx_h_mpdu(ar, &amsdu, rx_status, true);
+-	ath10k_htt_rx_h_deliver(ar, &amsdu, rx_status);
++	/* Not entirely sure about this, but all frames from the chipset has
++	 * the protected flag set even though they have already been decrypted.
++	 * Unmasking this flag is necessary in order for mac80211 not to drop
++	 * the frame.
++	 * TODO: Verify this is always the case or find out a way to check
++	 * if there has been hw decryption.
++	 */
++	if (ieee80211_has_protected(hdr->frame_control)) {
++		hdr->frame_control &= ~__cpu_to_le16(IEEE80211_FCTL_PROTECTED);
++		rx_status->flag |= RX_FLAG_DECRYPTED |
++				   RX_FLAG_IV_STRIPPED |
++				   RX_FLAG_MMIC_STRIPPED;
++	}
+ 
+-	return num_msdus;
++	ieee80211_rx_ni(ar->hw, skb);
++
++	/* We have delivered the skb to the upper layers (mac80211) so we
++	 * must not free it.
++	 */
++	return false;
++err:
++	/* Tell the caller that it must free the skb since we have not
++	 * consumed it
++	 */
++	return true;
+ }
+ 
+-static void ath10k_htt_rx_proc_rx_ind(struct ath10k_htt *htt,
+-				      struct htt_rx_indication *rx)
++static void ath10k_htt_rx_proc_rx_ind_ll(struct ath10k_htt *htt,
++					 struct htt_rx_indication *rx)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	struct htt_rx_indication_mpdu_range *mpdu_ranges;
+ 	int num_mpdu_ranges;
+ 	int i, mpdu_count = 0;
++	u16 peer_id;
++	u8 tid;
+ 
+ 	num_mpdu_ranges = MS(__le32_to_cpu(rx->hdr.info1),
+ 			     HTT_RX_INDICATION_INFO1_NUM_MPDU_RANGES);
++	peer_id = __le16_to_cpu(rx->hdr.peer_id);
++	tid =  MS(rx->hdr.info0, HTT_RX_INDICATION_INFO0_EXT_TID);
++
+ 	mpdu_ranges = htt_rx_ind_get_mpdu_ranges(rx);
+ 
+ 	ath10k_dbg_dump(ar, ATH10K_DBG_HTT_DUMP, NULL, "htt rx ind: ",
+@@ -1684,6 +1991,9 @@ static void ath10k_htt_rx_proc_rx_ind(st
+ 		mpdu_count += mpdu_ranges[i].mpdu_count;
+ 
+ 	atomic_add(mpdu_count, &htt->num_mpdus_ready);
++
++	ath10k_sta_update_rx_tid_stats_ampdu(ar, peer_id, tid, mpdu_ranges,
++					     num_mpdu_ranges);
+ }
+ 
+ static void ath10k_htt_rx_tx_compl_ind(struct ath10k *ar,
+@@ -1729,7 +2039,9 @@ static void ath10k_htt_rx_tx_compl_ind(s
+ 		 *  Note that with only one concurrent reader and one concurrent
+ 		 *  writer, you don't need extra locking to use these macro.
+ 		 */
+-		if (!kfifo_put(&htt->txdone_fifo, tx_done)) {
++		if (ar->hif.bus == ATH10K_BUS_SDIO) {
++			ath10k_txrx_tx_unref(htt, &tx_done);
++		} else if (!kfifo_put(&htt->txdone_fifo, tx_done)) {
+ 			ath10k_warn(ar, "txdone fifo overrun, msdu_id %d status %d\n",
+ 				    tx_done.msdu_id, tx_done.status);
+ 			ath10k_txrx_tx_unref(htt, &tx_done);
+@@ -1868,15 +2180,14 @@ static void ath10k_htt_rx_h_rx_offload_p
+ 			RX_FLAG_MMIC_STRIPPED;
+ }
+ 
+-static int ath10k_htt_rx_h_rx_offload(struct ath10k *ar,
+-				      struct sk_buff_head *list)
++static void ath10k_htt_rx_h_rx_offload(struct ath10k *ar,
++				       struct sk_buff_head *list)
+ {
+ 	struct ath10k_htt *htt = &ar->htt;
+ 	struct ieee80211_rx_status *status = &htt->rx_status;
+ 	struct htt_rx_offload_msdu *rx;
+ 	struct sk_buff *msdu;
+ 	size_t offset;
+-	int num_msdu = 0;
+ 
+ 	while ((msdu = __skb_dequeue(list))) {
+ 		/* Offloaded frames don't have Rx descriptor. Instead they have
+@@ -1915,10 +2226,8 @@ static int ath10k_htt_rx_h_rx_offload(st
+ 
+ 		ath10k_htt_rx_h_rx_offload_prot(status, msdu);
+ 		ath10k_htt_rx_h_channel(ar, status, NULL, rx->vdev_id);
+-		ath10k_process_rx(ar, status, msdu);
+-		num_msdu++;
++		ath10k_htt_rx_h_queue_msdu(ar, status, msdu);
+ 	}
+-	return num_msdu;
+ }
+ 
+ static int ath10k_htt_rx_in_ord_ind(struct ath10k *ar, struct sk_buff *skb)
+@@ -1934,7 +2243,7 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 	u8 tid;
+ 	bool offload;
+ 	bool frag;
+-	int ret, num_msdus = 0;
++	int ret;
+ 
+ 	lockdep_assert_held(&htt->rx_ring.lock);
+ 
+@@ -1956,7 +2265,7 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 		   "htt rx in ord vdev %i peer %i tid %i offload %i frag %i msdu count %i\n",
+ 		   vdev_id, peer_id, tid, offload, frag, msdu_count);
+ 
+-	if (skb->len < msdu_count * sizeof(*resp->rx_in_ord_ind.msdu_descs)) {
++	if (skb->len < msdu_count * sizeof(*resp->rx_in_ord_ind.msdu_descs32)) {
+ 		ath10k_warn(ar, "dropping invalid in order rx indication\n");
+ 		return -EINVAL;
+ 	}
+@@ -1965,7 +2274,13 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 	 * extracted and processed.
+ 	 */
+ 	__skb_queue_head_init(&list);
+-	ret = ath10k_htt_rx_pop_paddr_list(htt, &resp->rx_in_ord_ind, &list);
++	if (ar->hw_params.target_64bit)
++		ret = ath10k_htt_rx_pop_paddr64_list(htt, &resp->rx_in_ord_ind,
++						     &list);
++	else
++		ret = ath10k_htt_rx_pop_paddr32_list(htt, &resp->rx_in_ord_ind,
++						     &list);
++
+ 	if (ret < 0) {
+ 		ath10k_warn(ar, "failed to pop paddr list: %d\n", ret);
+ 		htt->rx_confused = true;
+@@ -1976,7 +2291,7 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 	 * separately.
+ 	 */
+ 	if (offload)
+-		num_msdus = ath10k_htt_rx_h_rx_offload(ar, &list);
++		ath10k_htt_rx_h_rx_offload(ar, &list);
+ 
+ 	while (!skb_queue_empty(&list)) {
+ 		__skb_queue_head_init(&amsdu);
+@@ -1989,11 +2304,11 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 			 * better to report something than nothing though. This
+ 			 * should still give an idea about rx rate to the user.
+ 			 */
+-			num_msdus += skb_queue_len(&amsdu);
+ 			ath10k_htt_rx_h_ppdu(ar, &amsdu, status, vdev_id);
+-			ath10k_htt_rx_h_filter(ar, &amsdu, status);
+-			ath10k_htt_rx_h_mpdu(ar, &amsdu, status, false);
+-			ath10k_htt_rx_h_deliver(ar, &amsdu, status);
++			ath10k_htt_rx_h_filter(ar, &amsdu, status, NULL);
++			ath10k_htt_rx_h_mpdu(ar, &amsdu, status, false, NULL,
++					     NULL);
++			ath10k_htt_rx_h_enqueue(ar, &amsdu, status);
+ 			break;
+ 		case -EAGAIN:
+ 			/* fall through */
+@@ -2005,7 +2320,7 @@ static int ath10k_htt_rx_in_ord_ind(stru
+ 			return -EIO;
+ 		}
+ 	}
+-	return num_msdus;
++	return ret;
+ }
+ 
+ static void ath10k_htt_rx_tx_fetch_resp_id_confirm(struct ath10k *ar,
+@@ -2366,7 +2681,7 @@ ath10k_update_per_peer_tx_stats(struct a
+ 		arsta->txrate.flags |= RATE_INFO_FLAGS_SHORT_GI;
+ 
+ 	arsta->txrate.nss = txrate.nss;
+-	arsta->txrate.bw = txrate.bw + RATE_INFO_BW_20;
++	arsta->txrate.bw = ath10k_bw_to_mac80211_bw(txrate.bw);
+ }
+ 
+ static void ath10k_htt_fetch_peer_stats(struct ath10k *ar,
+@@ -2424,6 +2739,62 @@ out:
+ 	rcu_read_unlock();
+ }
+ 
++static void ath10k_fetch_10_2_tx_stats(struct ath10k *ar, u8 *data)
++{
++	struct ath10k_pktlog_hdr *hdr = (struct ath10k_pktlog_hdr *)data;
++	struct ath10k_per_peer_tx_stats *p_tx_stats = &ar->peer_tx_stats;
++	struct ath10k_10_2_peer_tx_stats *tx_stats;
++	struct ieee80211_sta *sta;
++	struct ath10k_peer *peer;
++	u16 log_type = __le16_to_cpu(hdr->log_type);
++	u32 peer_id = 0, i;
++
++	if (log_type != ATH_PKTLOG_TYPE_TX_STAT)
++		return;
++
++	tx_stats = (struct ath10k_10_2_peer_tx_stats *)((hdr->payload) +
++		    ATH10K_10_2_TX_STATS_OFFSET);
++
++	if (!tx_stats->tx_ppdu_cnt)
++		return;
++
++	peer_id = tx_stats->peer_id;
++
++	rcu_read_lock();
++	spin_lock_bh(&ar->data_lock);
++	peer = ath10k_peer_find_by_id(ar, peer_id);
++	if (!peer) {
++		ath10k_warn(ar, "Invalid peer id %d in peer stats buffer\n",
++			    peer_id);
++		goto out;
++	}
++
++	sta = peer->sta;
++	for (i = 0; i < tx_stats->tx_ppdu_cnt; i++) {
++		p_tx_stats->succ_bytes =
++			__le16_to_cpu(tx_stats->success_bytes[i]);
++		p_tx_stats->retry_bytes =
++			__le16_to_cpu(tx_stats->retry_bytes[i]);
++		p_tx_stats->failed_bytes =
++			__le16_to_cpu(tx_stats->failed_bytes[i]);
++		p_tx_stats->ratecode = tx_stats->ratecode[i];
++		p_tx_stats->flags = tx_stats->flags[i];
++		p_tx_stats->succ_pkts = tx_stats->success_pkts[i];
++		p_tx_stats->retry_pkts = tx_stats->retry_pkts[i];
++		p_tx_stats->failed_pkts = tx_stats->failed_pkts[i];
++
++		ath10k_update_per_peer_tx_stats(ar, sta, p_tx_stats);
++	}
++	spin_unlock_bh(&ar->data_lock);
++	rcu_read_unlock();
++
++	return;
++
++out:
++	spin_unlock_bh(&ar->data_lock);
++	rcu_read_unlock();
++}
++
+ bool ath10k_htt_t2h_msg_handler(struct ath10k *ar, struct sk_buff *skb)
+ {
+ 	struct ath10k_htt *htt = &ar->htt;
+@@ -2452,7 +2823,12 @@ bool ath10k_htt_t2h_msg_handler(struct a
+ 		break;
+ 	}
+ 	case HTT_T2H_MSG_TYPE_RX_IND:
+-		ath10k_htt_rx_proc_rx_ind(htt, &resp->rx_ind);
++		if (ar->is_high_latency)
++			return ath10k_htt_rx_proc_rx_ind_hl(htt,
++							    &resp->rx_ind_hl,
++							    skb);
++		else
++			ath10k_htt_rx_proc_rx_ind_ll(htt, &resp->rx_ind);
+ 		break;
+ 	case HTT_T2H_MSG_TYPE_PEER_MAP: {
+ 		struct htt_peer_map_event ev = {
+@@ -2473,12 +2849,21 @@ bool ath10k_htt_t2h_msg_handler(struct a
+ 	case HTT_T2H_MSG_TYPE_MGMT_TX_COMPLETION: {
+ 		struct htt_tx_done tx_done = {};
+ 		int status = __le32_to_cpu(resp->mgmt_tx_completion.status);
++		int info = __le32_to_cpu(resp->mgmt_tx_completion.info);
+ 
+ 		tx_done.msdu_id = __le32_to_cpu(resp->mgmt_tx_completion.desc_id);
+ 
+ 		switch (status) {
+ 		case HTT_MGMT_TX_STATUS_OK:
+ 			tx_done.status = HTT_TX_COMPL_STATE_ACK;
++			if (test_bit(WMI_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
++				     ar->wmi.svc_map) &&
++			    (resp->mgmt_tx_completion.flags &
++			     HTT_MGMT_TX_CMPL_FLAG_ACK_RSSI)) {
++				tx_done.ack_rssi =
++				FIELD_GET(HTT_MGMT_TX_CMPL_INFO_ACK_RSSI_MASK,
++					  info);
++			}
+ 			break;
+ 		case HTT_MGMT_TX_STATUS_RETRY:
+ 			tx_done.status = HTT_TX_COMPL_STATE_NOACK;
+@@ -2497,7 +2882,8 @@ bool ath10k_htt_t2h_msg_handler(struct a
+ 		break;
+ 	}
+ 	case HTT_T2H_MSG_TYPE_TX_COMPL_IND:
+-		ath10k_htt_rx_tx_compl_ind(htt->ar, skb);
++		if (!(ar->hif.bus == ATH10K_BUS_USB))
++			ath10k_htt_rx_tx_compl_ind(htt->ar, skb);
+ 		break;
+ 	case HTT_T2H_MSG_TYPE_SEC_IND: {
+ 		struct ath10k *ar = htt->ar;
+@@ -2541,6 +2927,10 @@ bool ath10k_htt_t2h_msg_handler(struct a
+ 					skb->len -
+ 					offsetof(struct htt_resp,
+ 						 pktlog_msg.payload));
++
++		if (ath10k_peer_stats_enabled(ar))
++			ath10k_fetch_10_2_tx_stats(ar,
++						   resp->pktlog_msg.payload);
+ 		break;
+ 	}
+ 	case HTT_T2H_MSG_TYPE_RX_FLUSH: {
+@@ -2606,6 +2996,24 @@ void ath10k_htt_rx_pktlog_completion_han
+ }
+ EXPORT_SYMBOL(ath10k_htt_rx_pktlog_completion_handler);
+ 
++static int ath10k_htt_rx_deliver_msdu(struct ath10k *ar, int quota, int budget)
++{
++	struct sk_buff *skb;
++
++	while (quota < budget) {
++		if (skb_queue_empty(&ar->htt.rx_msdus_q))
++			break;
++
++		skb = __skb_dequeue(&ar->htt.rx_msdus_q);
++		if (!skb)
++			break;
++		ath10k_process_rx(ar, skb);
++		quota++;
++	}
++
++	return quota;
++}
++
+ int ath10k_htt_txrx_compl_task(struct ath10k *ar, int budget)
+ {
+ 	struct ath10k_htt *htt = &ar->htt;
+@@ -2613,63 +3021,44 @@ int ath10k_htt_txrx_compl_task(struct at
+ 	struct sk_buff_head tx_ind_q;
+ 	struct sk_buff *skb;
+ 	unsigned long flags;
+-	int quota = 0, done, num_rx_msdus;
++	int quota = 0, done, ret;
+ 	bool resched_napi = false;
+ 
+ 	__skb_queue_head_init(&tx_ind_q);
+ 
+-	/* Since in-ord-ind can deliver more than 1 A-MSDU in single event,
+-	 * process it first to utilize full available quota.
++	/* Process pending frames before dequeuing more data
++	 * from hardware.
+ 	 */
+-	while (quota < budget) {
+-		if (skb_queue_empty(&htt->rx_in_ord_compl_q))
+-			break;
+-
+-		skb = __skb_dequeue(&htt->rx_in_ord_compl_q);
+-		if (!skb) {
+-			resched_napi = true;
+-			goto exit;
+-		}
++	quota = ath10k_htt_rx_deliver_msdu(ar, quota, budget);
++	if (quota == budget) {
++		resched_napi = true;
++		goto exit;
++	}
+ 
++	while ((skb = __skb_dequeue(&htt->rx_in_ord_compl_q))) {
+ 		spin_lock_bh(&htt->rx_ring.lock);
+-		num_rx_msdus = ath10k_htt_rx_in_ord_ind(ar, skb);
++		ret = ath10k_htt_rx_in_ord_ind(ar, skb);
+ 		spin_unlock_bh(&htt->rx_ring.lock);
+-		if (num_rx_msdus < 0) {
+-			resched_napi = true;
+-			goto exit;
+-		}
+ 
+ 		dev_kfree_skb_any(skb);
+-		if (num_rx_msdus > 0)
+-			quota += num_rx_msdus;
+-
+-		if ((quota > ATH10K_NAPI_QUOTA_LIMIT) &&
+-		    !skb_queue_empty(&htt->rx_in_ord_compl_q)) {
++		if (ret == -EIO) {
+ 			resched_napi = true;
+ 			goto exit;
+ 		}
+ 	}
+ 
+-	while (quota < budget) {
+-		/* no more data to receive */
+-		if (!atomic_read(&htt->num_mpdus_ready))
+-			break;
+-
+-		num_rx_msdus = ath10k_htt_rx_handle_amsdu(htt);
+-		if (num_rx_msdus < 0) {
++	while (atomic_read(&htt->num_mpdus_ready)) {
++		ret = ath10k_htt_rx_handle_amsdu(htt);
++		if (ret == -EIO) {
+ 			resched_napi = true;
+ 			goto exit;
+ 		}
+-
+-		quota += num_rx_msdus;
+ 		atomic_dec(&htt->num_mpdus_ready);
+-		if ((quota > ATH10K_NAPI_QUOTA_LIMIT) &&
+-		    atomic_read(&htt->num_mpdus_ready)) {
+-			resched_napi = true;
+-			goto exit;
+-		}
+ 	}
+ 
++	/* Deliver received data after processing data from hardware */
++	quota = ath10k_htt_rx_deliver_msdu(ar, quota, budget);
++
+ 	/* From NAPI documentation:
+ 	 *  The napi poll() function may also process TX completions, in which
+ 	 *  case if it processes the entire TX ring then it should count that
+@@ -2707,3 +3096,34 @@ exit:
+ 	return done;
+ }
+ EXPORT_SYMBOL(ath10k_htt_txrx_compl_task);
++
++static const struct ath10k_htt_rx_ops htt_rx_ops_32 = {
++	.htt_get_rx_ring_size = ath10k_htt_get_rx_ring_size_32,
++	.htt_config_paddrs_ring = ath10k_htt_config_paddrs_ring_32,
++	.htt_set_paddrs_ring = ath10k_htt_set_paddrs_ring_32,
++	.htt_get_vaddr_ring = ath10k_htt_get_vaddr_ring_32,
++	.htt_reset_paddrs_ring = ath10k_htt_reset_paddrs_ring_32,
++};
++
++static const struct ath10k_htt_rx_ops htt_rx_ops_64 = {
++	.htt_get_rx_ring_size = ath10k_htt_get_rx_ring_size_64,
++	.htt_config_paddrs_ring = ath10k_htt_config_paddrs_ring_64,
++	.htt_set_paddrs_ring = ath10k_htt_set_paddrs_ring_64,
++	.htt_get_vaddr_ring = ath10k_htt_get_vaddr_ring_64,
++	.htt_reset_paddrs_ring = ath10k_htt_reset_paddrs_ring_64,
++};
++
++static const struct ath10k_htt_rx_ops htt_rx_ops_hl = {
++};
++
++void ath10k_htt_set_rx_ops(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++
++	if (ar->is_high_latency)
++		htt->rx_ops = &htt_rx_ops_hl;
++	else if (ar->hw_params.target_64bit)
++		htt->rx_ops = &htt_rx_ops_64;
++	else
++		htt->rx_ops = &htt_rx_ops_32;
++}
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/htt_tx.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/htt_tx.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/htt_tx.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -153,6 +153,9 @@ void ath10k_htt_tx_txq_update(struct iee
+ 
+ void ath10k_htt_tx_dec_pending(struct ath10k_htt *htt)
+ {
++	if (htt->ar->hif.bus == ATH10K_BUS_USB)
++		return;
++
+ 	lockdep_assert_held(&htt->tx_lock);
+ 
+ 	htt->num_pending_tx--;
+@@ -162,6 +165,9 @@ void ath10k_htt_tx_dec_pending(struct at
+ 
+ int ath10k_htt_tx_inc_pending(struct ath10k_htt *htt)
+ {
++	if (htt->ar->hif.bus == ATH10K_BUS_USB)
++		return 0;
++
+ 	lockdep_assert_held(&htt->tx_lock);
+ 
+ 	if (htt->num_pending_tx >= htt->max_num_pending_tx)
+@@ -229,50 +235,131 @@ void ath10k_htt_tx_free_msdu_id(struct a
+ 	idr_remove(&htt->pending_tx, msdu_id);
+ }
+ 
+-static void ath10k_htt_tx_free_cont_txbuf(struct ath10k_htt *htt)
++static void ath10k_htt_tx_free_cont_txbuf_32(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	size_t size;
++
++	if (!htt->txbuf.vaddr_txbuff_32)
++		return;
++
++	size = htt->txbuf.size;
++	dma_free_coherent(ar->dev, size, htt->txbuf.vaddr_txbuff_32,
++			  htt->txbuf.paddr);
++	htt->txbuf.vaddr_txbuff_32 = NULL;
++}
++
++static int ath10k_htt_tx_alloc_cont_txbuf_32(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	size_t size;
++
++	size = htt->max_num_pending_tx *
++			sizeof(struct ath10k_htt_txbuf_32);
++
++	htt->txbuf.vaddr_txbuff_32 = dma_alloc_coherent(ar->dev, size,
++							&htt->txbuf.paddr,
++							GFP_KERNEL);
++	if (!htt->txbuf.vaddr_txbuff_32)
++		return -ENOMEM;
++
++	htt->txbuf.size = size;
++
++	return 0;
++}
++
++static void ath10k_htt_tx_free_cont_txbuf_64(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	size_t size;
++
++	if (!htt->txbuf.vaddr_txbuff_64)
++		return;
++
++	size = htt->txbuf.size;
++	dma_free_coherent(ar->dev, size, htt->txbuf.vaddr_txbuff_64,
++			  htt->txbuf.paddr);
++	htt->txbuf.vaddr_txbuff_64 = NULL;
++}
++
++static int ath10k_htt_tx_alloc_cont_txbuf_64(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	size_t size;
+ 
+-	if (!htt->txbuf.vaddr)
++	size = htt->max_num_pending_tx *
++			sizeof(struct ath10k_htt_txbuf_64);
++
++	htt->txbuf.vaddr_txbuff_64 = dma_alloc_coherent(ar->dev, size,
++							&htt->txbuf.paddr,
++							GFP_KERNEL);
++	if (!htt->txbuf.vaddr_txbuff_64)
++		return -ENOMEM;
++
++	htt->txbuf.size = size;
++
++	return 0;
++}
++
++static void ath10k_htt_tx_free_cont_frag_desc_32(struct ath10k_htt *htt)
++{
++	size_t size;
++
++	if (!htt->frag_desc.vaddr_desc_32)
+ 		return;
+ 
+-	size = htt->max_num_pending_tx * sizeof(struct ath10k_htt_txbuf);
+-	dma_free_coherent(ar->dev, size, htt->txbuf.vaddr, htt->txbuf.paddr);
+-	htt->txbuf.vaddr = NULL;
++	size = htt->max_num_pending_tx *
++			sizeof(struct htt_msdu_ext_desc);
++
++	dma_free_coherent(htt->ar->dev,
++			  size,
++			  htt->frag_desc.vaddr_desc_32,
++			  htt->frag_desc.paddr);
++
++	htt->frag_desc.vaddr_desc_32 = NULL;
+ }
+ 
+-static int ath10k_htt_tx_alloc_cont_txbuf(struct ath10k_htt *htt)
++static int ath10k_htt_tx_alloc_cont_frag_desc_32(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	size_t size;
+ 
+-	size = htt->max_num_pending_tx * sizeof(struct ath10k_htt_txbuf);
+-	htt->txbuf.vaddr = dma_alloc_coherent(ar->dev, size, &htt->txbuf.paddr,
+-					      GFP_KERNEL);
+-	if (!htt->txbuf.vaddr)
++	if (!ar->hw_params.continuous_frag_desc)
++		return 0;
++
++	size = htt->max_num_pending_tx *
++			sizeof(struct htt_msdu_ext_desc);
++	htt->frag_desc.vaddr_desc_32 = dma_alloc_coherent(ar->dev, size,
++							  &htt->frag_desc.paddr,
++							  GFP_KERNEL);
++	if (!htt->frag_desc.vaddr_desc_32) {
++		ath10k_err(ar, "failed to alloc fragment desc memory\n");
+ 		return -ENOMEM;
++	}
++	htt->frag_desc.size = size;
+ 
+ 	return 0;
+ }
+ 
+-static void ath10k_htt_tx_free_cont_frag_desc(struct ath10k_htt *htt)
++static void ath10k_htt_tx_free_cont_frag_desc_64(struct ath10k_htt *htt)
+ {
+ 	size_t size;
+ 
+-	if (!htt->frag_desc.vaddr)
++	if (!htt->frag_desc.vaddr_desc_64)
+ 		return;
+ 
+-	size = htt->max_num_pending_tx * sizeof(struct htt_msdu_ext_desc);
++	size = htt->max_num_pending_tx *
++			sizeof(struct htt_msdu_ext_desc_64);
+ 
+ 	dma_free_coherent(htt->ar->dev,
+ 			  size,
+-			  htt->frag_desc.vaddr,
++			  htt->frag_desc.vaddr_desc_64,
+ 			  htt->frag_desc.paddr);
+-	htt->frag_desc.vaddr = NULL;
++
++	htt->frag_desc.vaddr_desc_64 = NULL;
+ }
+ 
+-static int ath10k_htt_tx_alloc_cont_frag_desc(struct ath10k_htt *htt)
++static int ath10k_htt_tx_alloc_cont_frag_desc_64(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	size_t size;
+@@ -280,12 +367,17 @@ static int ath10k_htt_tx_alloc_cont_frag
+ 	if (!ar->hw_params.continuous_frag_desc)
+ 		return 0;
+ 
+-	size = htt->max_num_pending_tx * sizeof(struct htt_msdu_ext_desc);
+-	htt->frag_desc.vaddr = dma_alloc_coherent(ar->dev, size,
+-						  &htt->frag_desc.paddr,
+-						  GFP_KERNEL);
+-	if (!htt->frag_desc.vaddr)
++	size = htt->max_num_pending_tx *
++			sizeof(struct htt_msdu_ext_desc_64);
++
++	htt->frag_desc.vaddr_desc_64 = dma_alloc_coherent(ar->dev, size,
++							  &htt->frag_desc.paddr,
++							  GFP_KERNEL);
++	if (!htt->frag_desc.vaddr_desc_64) {
++		ath10k_err(ar, "failed to alloc fragment desc memory\n");
+ 		return -ENOMEM;
++	}
++	htt->frag_desc.size = size;
+ 
+ 	return 0;
+ }
+@@ -357,13 +449,13 @@ static int ath10k_htt_tx_alloc_buf(struc
+ 	struct ath10k *ar = htt->ar;
+ 	int ret;
+ 
+-	ret = ath10k_htt_tx_alloc_cont_txbuf(htt);
++	ret = ath10k_htt_alloc_txbuff(htt);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to alloc cont tx buffer: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+-	ret = ath10k_htt_tx_alloc_cont_frag_desc(htt);
++	ret = ath10k_htt_alloc_frag_desc(htt);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to alloc cont frag desc: %d\n", ret);
+ 		goto free_txbuf;
+@@ -387,10 +479,10 @@ free_txq:
+ 	ath10k_htt_tx_free_txq(htt);
+ 
+ free_frag_desc:
+-	ath10k_htt_tx_free_cont_frag_desc(htt);
++	ath10k_htt_free_frag_desc(htt);
+ 
+ free_txbuf:
+-	ath10k_htt_tx_free_cont_txbuf(htt);
++	ath10k_htt_free_txbuff(htt);
+ 
+ 	return ret;
+ }
+@@ -409,6 +501,9 @@ int ath10k_htt_tx_start(struct ath10k_ht
+ 	if (htt->tx_mem_allocated)
+ 		return 0;
+ 
++	if (ar->is_high_latency)
++		return 0;
++
+ 	ret = ath10k_htt_tx_alloc_buf(htt);
+ 	if (ret)
+ 		goto free_idr_pending_tx;
+@@ -444,9 +539,9 @@ void ath10k_htt_tx_destroy(struct ath10k
+ 	if (!htt->tx_mem_allocated)
+ 		return;
+ 
+-	ath10k_htt_tx_free_cont_txbuf(htt);
++	ath10k_htt_free_txbuff(htt);
+ 	ath10k_htt_tx_free_txq(htt);
+-	ath10k_htt_tx_free_cont_frag_desc(htt);
++	ath10k_htt_free_frag_desc(htt);
+ 	ath10k_htt_tx_free_txdone_fifo(htt);
+ 	htt->tx_mem_allocated = false;
+ }
+@@ -465,7 +560,8 @@ void ath10k_htt_tx_free(struct ath10k_ht
+ 
+ void ath10k_htt_htc_tx_complete(struct ath10k *ar, struct sk_buff *skb)
+ {
+-	dev_kfree_skb_any(skb);
++	if (!(ar->hif.bus == ATH10K_BUS_SDIO))
++		dev_kfree_skb_any(skb);
+ }
+ 
+ void ath10k_htt_hif_tx_complete(struct ath10k *ar, struct sk_buff *skb)
+@@ -545,12 +641,12 @@ int ath10k_htt_h2t_stats_req(struct ath1
+ 	return 0;
+ }
+ 
+-int ath10k_htt_send_frag_desc_bank_cfg(struct ath10k_htt *htt)
++static int ath10k_htt_send_frag_desc_bank_cfg_32(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	struct sk_buff *skb;
+ 	struct htt_cmd *cmd;
+-	struct htt_frag_desc_bank_cfg *cfg;
++	struct htt_frag_desc_bank_cfg32 *cfg;
+ 	int ret, size;
+ 	u8 info;
+ 
+@@ -562,7 +658,7 @@ int ath10k_htt_send_frag_desc_bank_cfg(s
+ 		return -EINVAL;
+ 	}
+ 
+-	size = sizeof(cmd->hdr) + sizeof(cmd->frag_desc_bank_cfg);
++	size = sizeof(cmd->hdr) + sizeof(cmd->frag_desc_bank_cfg32);
+ 	skb = ath10k_htc_alloc_skb(ar, size);
+ 	if (!skb)
+ 		return -ENOMEM;
+@@ -579,7 +675,7 @@ int ath10k_htt_send_frag_desc_bank_cfg(s
+ 		     ar->running_fw->fw_file.fw_features))
+ 		info |= HTT_FRAG_DESC_BANK_CFG_INFO_Q_STATE_VALID;
+ 
+-	cfg = &cmd->frag_desc_bank_cfg;
++	cfg = &cmd->frag_desc_bank_cfg32;
+ 	cfg->info = info;
+ 	cfg->num_banks = 1;
+ 	cfg->desc_size = sizeof(struct htt_msdu_ext_desc);
+@@ -607,12 +703,112 @@ int ath10k_htt_send_frag_desc_bank_cfg(s
+ 	return 0;
+ }
+ 
+-int ath10k_htt_send_rx_ring_cfg_ll(struct ath10k_htt *htt)
++static int ath10k_htt_send_frag_desc_bank_cfg_64(struct ath10k_htt *htt)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	struct sk_buff *skb;
+ 	struct htt_cmd *cmd;
+-	struct htt_rx_ring_setup_ring *ring;
++	struct htt_frag_desc_bank_cfg64 *cfg;
++	int ret, size;
++	u8 info;
++
++	if (!ar->hw_params.continuous_frag_desc)
++		return 0;
++
++	if (!htt->frag_desc.paddr) {
++		ath10k_warn(ar, "invalid frag desc memory\n");
++		return -EINVAL;
++	}
++
++	size = sizeof(cmd->hdr) + sizeof(cmd->frag_desc_bank_cfg64);
++	skb = ath10k_htc_alloc_skb(ar, size);
++	if (!skb)
++		return -ENOMEM;
++
++	skb_put(skb, size);
++	cmd = (struct htt_cmd *)skb->data;
++	cmd->hdr.msg_type = HTT_H2T_MSG_TYPE_FRAG_DESC_BANK_CFG;
++
++	info = 0;
++	info |= SM(htt->tx_q_state.type,
++		   HTT_FRAG_DESC_BANK_CFG_INFO_Q_STATE_DEPTH_TYPE);
++
++	if (test_bit(ATH10K_FW_FEATURE_PEER_FLOW_CONTROL,
++		     ar->running_fw->fw_file.fw_features))
++		info |= HTT_FRAG_DESC_BANK_CFG_INFO_Q_STATE_VALID;
++
++	cfg = &cmd->frag_desc_bank_cfg64;
++	cfg->info = info;
++	cfg->num_banks = 1;
++	cfg->desc_size = sizeof(struct htt_msdu_ext_desc_64);
++	cfg->bank_base_addrs[0] =  __cpu_to_le64(htt->frag_desc.paddr);
++	cfg->bank_id[0].bank_min_id = 0;
++	cfg->bank_id[0].bank_max_id = __cpu_to_le16(htt->max_num_pending_tx -
++						    1);
++
++	cfg->q_state.paddr = cpu_to_le32(htt->tx_q_state.paddr);
++	cfg->q_state.num_peers = cpu_to_le16(htt->tx_q_state.num_peers);
++	cfg->q_state.num_tids = cpu_to_le16(htt->tx_q_state.num_tids);
++	cfg->q_state.record_size = HTT_TX_Q_STATE_ENTRY_SIZE;
++	cfg->q_state.record_multiplier = HTT_TX_Q_STATE_ENTRY_MULTIPLIER;
++
++	ath10k_dbg(ar, ATH10K_DBG_HTT, "htt frag desc bank cmd\n");
++
++	ret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);
++	if (ret) {
++		ath10k_warn(ar, "failed to send frag desc bank cfg request: %d\n",
++			    ret);
++		dev_kfree_skb_any(skb);
++		return ret;
++	}
++
++	return 0;
++}
++
++static void ath10k_htt_fill_rx_desc_offset_32(void *rx_ring)
++{
++	struct htt_rx_ring_setup_ring32 *ring =
++			(struct htt_rx_ring_setup_ring32 *)rx_ring;
++
++#define desc_offset(x) (offsetof(struct htt_rx_desc, x) / 4)
++	ring->mac80211_hdr_offset = __cpu_to_le16(desc_offset(rx_hdr_status));
++	ring->msdu_payload_offset = __cpu_to_le16(desc_offset(msdu_payload));
++	ring->ppdu_start_offset = __cpu_to_le16(desc_offset(ppdu_start));
++	ring->ppdu_end_offset = __cpu_to_le16(desc_offset(ppdu_end));
++	ring->mpdu_start_offset = __cpu_to_le16(desc_offset(mpdu_start));
++	ring->mpdu_end_offset = __cpu_to_le16(desc_offset(mpdu_end));
++	ring->msdu_start_offset = __cpu_to_le16(desc_offset(msdu_start));
++	ring->msdu_end_offset = __cpu_to_le16(desc_offset(msdu_end));
++	ring->rx_attention_offset = __cpu_to_le16(desc_offset(attention));
++	ring->frag_info_offset = __cpu_to_le16(desc_offset(frag_info));
++#undef desc_offset
++}
++
++static void ath10k_htt_fill_rx_desc_offset_64(void *rx_ring)
++{
++	struct htt_rx_ring_setup_ring64 *ring =
++			(struct htt_rx_ring_setup_ring64 *)rx_ring;
++
++#define desc_offset(x) (offsetof(struct htt_rx_desc, x) / 4)
++	ring->mac80211_hdr_offset = __cpu_to_le16(desc_offset(rx_hdr_status));
++	ring->msdu_payload_offset = __cpu_to_le16(desc_offset(msdu_payload));
++	ring->ppdu_start_offset = __cpu_to_le16(desc_offset(ppdu_start));
++	ring->ppdu_end_offset = __cpu_to_le16(desc_offset(ppdu_end));
++	ring->mpdu_start_offset = __cpu_to_le16(desc_offset(mpdu_start));
++	ring->mpdu_end_offset = __cpu_to_le16(desc_offset(mpdu_end));
++	ring->msdu_start_offset = __cpu_to_le16(desc_offset(msdu_start));
++	ring->msdu_end_offset = __cpu_to_le16(desc_offset(msdu_end));
++	ring->rx_attention_offset = __cpu_to_le16(desc_offset(attention));
++	ring->frag_info_offset = __cpu_to_le16(desc_offset(frag_info));
++#undef desc_offset
++}
++
++static int ath10k_htt_send_rx_ring_cfg_32(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	struct sk_buff *skb;
++	struct htt_cmd *cmd;
++	struct htt_rx_ring_setup_ring32 *ring;
+ 	const int num_rx_ring = 1;
+ 	u16 flags;
+ 	u32 fw_idx;
+@@ -626,7 +822,7 @@ int ath10k_htt_send_rx_ring_cfg_ll(struc
+ 	BUILD_BUG_ON(!IS_ALIGNED(HTT_RX_BUF_SIZE, 4));
+ 	BUILD_BUG_ON((HTT_RX_BUF_SIZE & HTT_MAX_CACHE_LINE_SIZE_MASK) != 0);
+ 
+-	len = sizeof(cmd->hdr) + sizeof(cmd->rx_setup.hdr)
++	len = sizeof(cmd->hdr) + sizeof(cmd->rx_setup_32.hdr)
+ 	    + (sizeof(*ring) * num_rx_ring);
+ 	skb = ath10k_htc_alloc_skb(ar, len);
+ 	if (!skb)
+@@ -635,10 +831,10 @@ int ath10k_htt_send_rx_ring_cfg_ll(struc
+ 	skb_put(skb, len);
+ 
+ 	cmd = (struct htt_cmd *)skb->data;
+-	ring = &cmd->rx_setup.rings[0];
++	ring = &cmd->rx_setup_32.rings[0];
+ 
+ 	cmd->hdr.msg_type = HTT_H2T_MSG_TYPE_RX_RING_CFG;
+-	cmd->rx_setup.hdr.num_rings = 1;
++	cmd->rx_setup_32.hdr.num_rings = 1;
+ 
+ 	/* FIXME: do we need all of this? */
+ 	flags = 0;
+@@ -669,20 +865,126 @@ int ath10k_htt_send_rx_ring_cfg_ll(struc
+ 	ring->flags = __cpu_to_le16(flags);
+ 	ring->fw_idx_init_val = __cpu_to_le16(fw_idx);
+ 
+-#define desc_offset(x) (offsetof(struct htt_rx_desc, x) / 4)
++	ath10k_htt_fill_rx_desc_offset_32(ring);
++	ret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);
++	if (ret) {
++		dev_kfree_skb_any(skb);
++		return ret;
++	}
+ 
+-	ring->mac80211_hdr_offset = __cpu_to_le16(desc_offset(rx_hdr_status));
+-	ring->msdu_payload_offset = __cpu_to_le16(desc_offset(msdu_payload));
+-	ring->ppdu_start_offset = __cpu_to_le16(desc_offset(ppdu_start));
+-	ring->ppdu_end_offset = __cpu_to_le16(desc_offset(ppdu_end));
+-	ring->mpdu_start_offset = __cpu_to_le16(desc_offset(mpdu_start));
+-	ring->mpdu_end_offset = __cpu_to_le16(desc_offset(mpdu_end));
+-	ring->msdu_start_offset = __cpu_to_le16(desc_offset(msdu_start));
+-	ring->msdu_end_offset = __cpu_to_le16(desc_offset(msdu_end));
+-	ring->rx_attention_offset = __cpu_to_le16(desc_offset(attention));
+-	ring->frag_info_offset = __cpu_to_le16(desc_offset(frag_info));
++	return 0;
++}
+ 
+-#undef desc_offset
++static int ath10k_htt_send_rx_ring_cfg_64(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	struct sk_buff *skb;
++	struct htt_cmd *cmd;
++	struct htt_rx_ring_setup_ring64 *ring;
++	const int num_rx_ring = 1;
++	u16 flags;
++	u32 fw_idx;
++	int len;
++	int ret;
++
++	/* HW expects the buffer to be an integral number of 4-byte
++	 * "words"
++	 */
++	BUILD_BUG_ON(!IS_ALIGNED(HTT_RX_BUF_SIZE, 4));
++	BUILD_BUG_ON((HTT_RX_BUF_SIZE & HTT_MAX_CACHE_LINE_SIZE_MASK) != 0);
++
++	len = sizeof(cmd->hdr) + sizeof(cmd->rx_setup_64.hdr)
++	    + (sizeof(*ring) * num_rx_ring);
++	skb = ath10k_htc_alloc_skb(ar, len);
++	if (!skb)
++		return -ENOMEM;
++
++	skb_put(skb, len);
++
++	cmd = (struct htt_cmd *)skb->data;
++	ring = &cmd->rx_setup_64.rings[0];
++
++	cmd->hdr.msg_type = HTT_H2T_MSG_TYPE_RX_RING_CFG;
++	cmd->rx_setup_64.hdr.num_rings = 1;
++
++	flags = 0;
++	flags |= HTT_RX_RING_FLAGS_MAC80211_HDR;
++	flags |= HTT_RX_RING_FLAGS_MSDU_PAYLOAD;
++	flags |= HTT_RX_RING_FLAGS_PPDU_START;
++	flags |= HTT_RX_RING_FLAGS_PPDU_END;
++	flags |= HTT_RX_RING_FLAGS_MPDU_START;
++	flags |= HTT_RX_RING_FLAGS_MPDU_END;
++	flags |= HTT_RX_RING_FLAGS_MSDU_START;
++	flags |= HTT_RX_RING_FLAGS_MSDU_END;
++	flags |= HTT_RX_RING_FLAGS_RX_ATTENTION;
++	flags |= HTT_RX_RING_FLAGS_FRAG_INFO;
++	flags |= HTT_RX_RING_FLAGS_UNICAST_RX;
++	flags |= HTT_RX_RING_FLAGS_MULTICAST_RX;
++	flags |= HTT_RX_RING_FLAGS_CTRL_RX;
++	flags |= HTT_RX_RING_FLAGS_MGMT_RX;
++	flags |= HTT_RX_RING_FLAGS_NULL_RX;
++	flags |= HTT_RX_RING_FLAGS_PHY_DATA_RX;
++
++	fw_idx = __le32_to_cpu(*htt->rx_ring.alloc_idx.vaddr);
++
++	ring->fw_idx_shadow_reg_paddr = __cpu_to_le64(htt->rx_ring.alloc_idx.paddr);
++	ring->rx_ring_base_paddr = __cpu_to_le64(htt->rx_ring.base_paddr);
++	ring->rx_ring_len = __cpu_to_le16(htt->rx_ring.size);
++	ring->rx_ring_bufsize = __cpu_to_le16(HTT_RX_BUF_SIZE);
++	ring->flags = __cpu_to_le16(flags);
++	ring->fw_idx_init_val = __cpu_to_le16(fw_idx);
++
++	ath10k_htt_fill_rx_desc_offset_64(ring);
++	ret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);
++	if (ret) {
++		dev_kfree_skb_any(skb);
++		return ret;
++	}
++
++	return 0;
++}
++
++static int ath10k_htt_send_rx_ring_cfg_hl(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++	struct sk_buff *skb;
++	struct htt_cmd *cmd;
++	struct htt_rx_ring_setup_ring32 *ring;
++	const int num_rx_ring = 1;
++	u16 flags;
++	int len;
++	int ret;
++
++	/*
++	 * the HW expects the buffer to be an integral number of 4-byte
++	 * "words"
++	 */
++	BUILD_BUG_ON(!IS_ALIGNED(HTT_RX_BUF_SIZE, 4));
++	BUILD_BUG_ON((HTT_RX_BUF_SIZE & HTT_MAX_CACHE_LINE_SIZE_MASK) != 0);
++
++	len = sizeof(cmd->hdr) + sizeof(cmd->rx_setup_32.hdr)
++	    + (sizeof(*ring) * num_rx_ring);
++	skb = ath10k_htc_alloc_skb(ar, len);
++	if (!skb)
++		return -ENOMEM;
++
++	skb_put(skb, len);
++
++	cmd = (struct htt_cmd *)skb->data;
++	ring = &cmd->rx_setup_32.rings[0];
++
++	cmd->hdr.msg_type = HTT_H2T_MSG_TYPE_RX_RING_CFG;
++	cmd->rx_setup_32.hdr.num_rings = 1;
++
++	flags = 0;
++	flags |= HTT_RX_RING_FLAGS_MSDU_PAYLOAD;
++	flags |= HTT_RX_RING_FLAGS_UNICAST_RX;
++	flags |= HTT_RX_RING_FLAGS_MULTICAST_RX;
++
++	memset(ring, 0, sizeof(*ring));
++	ring->rx_ring_len = __cpu_to_le16(HTT_RX_RING_SIZE_MIN);
++	ring->rx_ring_bufsize = __cpu_to_le16(HTT_RX_BUF_SIZE);
++	ring->flags = __cpu_to_le16(flags);
+ 
+ 	ret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);
+ 	if (ret) {
+@@ -884,7 +1186,8 @@ int ath10k_htt_mgmt_tx(struct ath10k_htt
+ 	return 0;
+ 
+ err_unmap_msdu:
+-	dma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
++	if (!ar->is_high_latency)
++		dma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
+ err_free_txdesc:
+ 	dev_kfree_skb_any(txdesc);
+ err_free_msdu_id:
+@@ -895,8 +1198,110 @@ err:
+ 	return res;
+ }
+ 
+-int ath10k_htt_tx(struct ath10k_htt *htt, enum ath10k_hw_txrx_mode txmode,
+-		  struct sk_buff *msdu)
++#define HTT_TX_HL_NEEDED_HEADROOM \
++	(unsigned int)(sizeof(struct htt_cmd_hdr) + \
++	sizeof(struct htt_data_tx_desc) + \
++	sizeof(struct ath10k_htc_hdr))
++
++static int ath10k_htt_tx_hl(struct ath10k_htt *htt, enum ath10k_hw_txrx_mode txmode,
++			    struct sk_buff *msdu)
++{
++	struct ath10k *ar = htt->ar;
++	int res, data_len;
++	struct htt_cmd_hdr *cmd_hdr;
++	struct htt_data_tx_desc *tx_desc;
++	struct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(msdu);
++	struct sk_buff *tmp_skb;
++	bool is_eth = (txmode == ATH10K_HW_TXRX_ETHERNET);
++	u8 vdev_id = ath10k_htt_tx_get_vdev_id(ar, msdu);
++	u8 tid = ath10k_htt_tx_get_tid(msdu, is_eth);
++	u8 flags0 = 0;
++	u16 flags1 = 0;
++	u16 msdu_id = 0;
++
++	data_len = msdu->len;
++
++	switch (txmode) {
++	case ATH10K_HW_TXRX_RAW:
++	case ATH10K_HW_TXRX_NATIVE_WIFI:
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;
++		/* fall through */
++	case ATH10K_HW_TXRX_ETHERNET:
++		flags0 |= SM(txmode, HTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);
++		break;
++	case ATH10K_HW_TXRX_MGMT:
++		flags0 |= SM(ATH10K_HW_TXRX_MGMT,
++			     HTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;
++		break;
++	}
++
++	if (skb_cb->flags & ATH10K_SKB_F_NO_HWCRYPT)
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_NO_ENCRYPT;
++
++	flags1 |= SM((u16)vdev_id, HTT_DATA_TX_DESC_FLAGS1_VDEV_ID);
++	flags1 |= SM((u16)tid, HTT_DATA_TX_DESC_FLAGS1_EXT_TID);
++	if (msdu->ip_summed == CHECKSUM_PARTIAL &&
++	    !test_bit(ATH10K_FLAG_RAW_MODE, &ar->dev_flags)) {
++		flags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L3_OFFLOAD;
++		flags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L4_OFFLOAD;
++	}
++
++	/* Prepend the HTT header and TX desc struct to the data message
++	 * and realloc the skb if it does not have enough headroom.
++	 */
++	if (skb_headroom(msdu) < HTT_TX_HL_NEEDED_HEADROOM) {
++		tmp_skb = msdu;
++
++		ath10k_dbg(htt->ar, ATH10K_DBG_HTT,
++			   "Not enough headroom in skb. Current headroom: %u, needed: %u. Reallocating...\n",
++			   skb_headroom(msdu), HTT_TX_HL_NEEDED_HEADROOM);
++		msdu = skb_realloc_headroom(msdu, HTT_TX_HL_NEEDED_HEADROOM);
++		kfree_skb(tmp_skb);
++		if (!msdu) {
++			ath10k_warn(htt->ar, "htt hl tx: Unable to realloc skb!\n");
++			res = -ENOMEM;
++			goto out;
++		}
++	}
++
++	if (ar->hif.bus == ATH10K_BUS_SDIO) {
++		flags1 |= HTT_DATA_TX_DESC_FLAGS1_POSTPONED;
++		spin_lock_bh(&htt->tx_lock);
++		res = ath10k_htt_tx_alloc_msdu_id(htt, msdu);
++		spin_unlock_bh(&htt->tx_lock);
++		if (res < 0) {
++			ath10k_err(ar, "msdu_id allocation failed %d\n", res);
++			goto out;
++		}
++		msdu_id = res;
++	}
++
++	skb_push(msdu, sizeof(*cmd_hdr));
++	skb_push(msdu, sizeof(*tx_desc));
++	cmd_hdr = (struct htt_cmd_hdr *)msdu->data;
++	tx_desc = (struct htt_data_tx_desc *)(msdu->data + sizeof(*cmd_hdr));
++
++	cmd_hdr->msg_type = HTT_H2T_MSG_TYPE_TX_FRM;
++	tx_desc->flags0 = flags0;
++	tx_desc->flags1 = __cpu_to_le16(flags1);
++	tx_desc->len = __cpu_to_le16(data_len);
++	tx_desc->id = __cpu_to_le16(msdu_id);
++	tx_desc->frags_paddr = 0; /* always zero */
++	/* Initialize peer_id to INVALID_PEER because this is NOT
++	 * Reinjection path
++	 */
++	tx_desc->peerid = __cpu_to_le32(HTT_INVALID_PEERID);
++
++	res = ath10k_htc_send(&htt->ar->htc, htt->eid, msdu);
++
++out:
++	return res;
++}
++
++static int ath10k_htt_tx_32(struct ath10k_htt *htt,
++			    enum ath10k_hw_txrx_mode txmode,
++			    struct sk_buff *msdu)
+ {
+ 	struct ath10k *ar = htt->ar;
+ 	struct device *dev = ar->dev;
+@@ -904,7 +1309,7 @@ int ath10k_htt_tx(struct ath10k_htt *htt
+ 	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(msdu);
+ 	struct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(msdu);
+ 	struct ath10k_hif_sg_item sg_items[2];
+-	struct ath10k_htt_txbuf *txbuf;
++	struct ath10k_htt_txbuf_32 *txbuf;
+ 	struct htt_data_tx_desc_frag *frags;
+ 	bool is_eth = (txmode == ATH10K_HW_TXRX_ETHERNET);
+ 	u8 vdev_id = ath10k_htt_tx_get_vdev_id(ar, msdu);
+@@ -917,6 +1322,7 @@ int ath10k_htt_tx(struct ath10k_htt *htt
+ 	u32 frags_paddr = 0;
+ 	u32 txbuf_paddr;
+ 	struct htt_msdu_ext_desc *ext_desc = NULL;
++	struct htt_msdu_ext_desc *ext_desc_t = NULL;
+ 
+ 	spin_lock_bh(&htt->tx_lock);
+ 	res = ath10k_htt_tx_alloc_msdu_id(htt, msdu);
+@@ -929,9 +1335,9 @@ int ath10k_htt_tx(struct ath10k_htt *htt
+ 	prefetch_len = min(htt->prefetch_len, msdu->len);
+ 	prefetch_len = roundup(prefetch_len, 4);
+ 
+-	txbuf = &htt->txbuf.vaddr[msdu_id];
++	txbuf = htt->txbuf.vaddr_txbuff_32 + msdu_id;
+ 	txbuf_paddr = htt->txbuf.paddr +
+-		      (sizeof(struct ath10k_htt_txbuf) * msdu_id);
++		      (sizeof(struct ath10k_htt_txbuf_32) * msdu_id);
+ 
+ 	if ((ieee80211_is_action(hdr->frame_control) ||
+ 	     ieee80211_is_deauth(hdr->frame_control) ||
+@@ -962,11 +1368,12 @@ int ath10k_htt_tx(struct ath10k_htt *htt
+ 		/* pass through */
+ 	case ATH10K_HW_TXRX_ETHERNET:
+ 		if (ar->hw_params.continuous_frag_desc) {
+-			memset(&htt->frag_desc.vaddr[msdu_id], 0,
++			ext_desc_t = htt->frag_desc.vaddr_desc_32;
++			memset(&ext_desc_t[msdu_id], 0,
+ 			       sizeof(struct htt_msdu_ext_desc));
+ 			frags = (struct htt_data_tx_desc_frag *)
+-				&htt->frag_desc.vaddr[msdu_id].frags;
+-			ext_desc = &htt->frag_desc.vaddr[msdu_id];
++				&ext_desc_t[msdu_id].frags;
++			ext_desc = &ext_desc_t[msdu_id];
+ 			frags[0].tword_addr.paddr_lo =
+ 				__cpu_to_le32(skb_cb->paddr);
+ 			frags[0].tword_addr.paddr_hi = 0;
+@@ -1055,9 +1462,215 @@ int ath10k_htt_tx(struct ath10k_htt *htt
+ 
+ 	trace_ath10k_htt_tx(ar, msdu_id, msdu->len, vdev_id, tid);
+ 	ath10k_dbg(ar, ATH10K_DBG_HTT,
+-		   "htt tx flags0 %hhu flags1 %hu len %d id %hu frags_paddr %08x, msdu_paddr %08x vdev %hhu tid %hhu freq %hu\n",
+-		   flags0, flags1, msdu->len, msdu_id, frags_paddr,
+-		   (u32)skb_cb->paddr, vdev_id, tid, freq);
++		   "htt tx flags0 %hhu flags1 %hu len %d id %hu frags_paddr %pad, msdu_paddr %pad vdev %hhu tid %hhu freq %hu\n",
++		   flags0, flags1, msdu->len, msdu_id, &frags_paddr,
++		   &skb_cb->paddr, vdev_id, tid, freq);
++	ath10k_dbg_dump(ar, ATH10K_DBG_HTT_DUMP, NULL, "htt tx msdu: ",
++			msdu->data, msdu->len);
++	trace_ath10k_tx_hdr(ar, msdu->data, msdu->len);
++	trace_ath10k_tx_payload(ar, msdu->data, msdu->len);
++
++	sg_items[0].transfer_id = 0;
++	sg_items[0].transfer_context = NULL;
++	sg_items[0].vaddr = &txbuf->htc_hdr;
++	sg_items[0].paddr = txbuf_paddr +
++			    sizeof(txbuf->frags);
++	sg_items[0].len = sizeof(txbuf->htc_hdr) +
++			  sizeof(txbuf->cmd_hdr) +
++			  sizeof(txbuf->cmd_tx);
++
++	sg_items[1].transfer_id = 0;
++	sg_items[1].transfer_context = NULL;
++	sg_items[1].vaddr = msdu->data;
++	sg_items[1].paddr = skb_cb->paddr;
++	sg_items[1].len = prefetch_len;
++
++	res = ath10k_hif_tx_sg(htt->ar,
++			       htt->ar->htc.endpoint[htt->eid].ul_pipe_id,
++			       sg_items, ARRAY_SIZE(sg_items));
++	if (res)
++		goto err_unmap_msdu;
++
++	return 0;
++
++err_unmap_msdu:
++	dma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
++err_free_msdu_id:
++	ath10k_htt_tx_free_msdu_id(htt, msdu_id);
++err:
++	return res;
++}
++
++static int ath10k_htt_tx_64(struct ath10k_htt *htt,
++			    enum ath10k_hw_txrx_mode txmode,
++			    struct sk_buff *msdu)
++{
++	struct ath10k *ar = htt->ar;
++	struct device *dev = ar->dev;
++	struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)msdu->data;
++	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(msdu);
++	struct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(msdu);
++	struct ath10k_hif_sg_item sg_items[2];
++	struct ath10k_htt_txbuf_64 *txbuf;
++	struct htt_data_tx_desc_frag *frags;
++	bool is_eth = (txmode == ATH10K_HW_TXRX_ETHERNET);
++	u8 vdev_id = ath10k_htt_tx_get_vdev_id(ar, msdu);
++	u8 tid = ath10k_htt_tx_get_tid(msdu, is_eth);
++	int prefetch_len;
++	int res;
++	u8 flags0 = 0;
++	u16 msdu_id, flags1 = 0;
++	u16 freq = 0;
++	dma_addr_t frags_paddr = 0;
++	u32 txbuf_paddr;
++	struct htt_msdu_ext_desc_64 *ext_desc = NULL;
++	struct htt_msdu_ext_desc_64 *ext_desc_t = NULL;
++
++	spin_lock_bh(&htt->tx_lock);
++	res = ath10k_htt_tx_alloc_msdu_id(htt, msdu);
++	spin_unlock_bh(&htt->tx_lock);
++	if (res < 0)
++		goto err;
++
++	msdu_id = res;
++
++	prefetch_len = min(htt->prefetch_len, msdu->len);
++	prefetch_len = roundup(prefetch_len, 4);
++
++	txbuf = htt->txbuf.vaddr_txbuff_64 + msdu_id;
++	txbuf_paddr = htt->txbuf.paddr +
++		      (sizeof(struct ath10k_htt_txbuf_64) * msdu_id);
++
++	if ((ieee80211_is_action(hdr->frame_control) ||
++	     ieee80211_is_deauth(hdr->frame_control) ||
++	     ieee80211_is_disassoc(hdr->frame_control)) &&
++	     ieee80211_has_protected(hdr->frame_control)) {
++		skb_put(msdu, IEEE80211_CCMP_MIC_LEN);
++	} else if (!(skb_cb->flags & ATH10K_SKB_F_NO_HWCRYPT) &&
++		   txmode == ATH10K_HW_TXRX_RAW &&
++		   ieee80211_has_protected(hdr->frame_control)) {
++		skb_put(msdu, IEEE80211_CCMP_MIC_LEN);
++	}
++
++	skb_cb->paddr = dma_map_single(dev, msdu->data, msdu->len,
++				       DMA_TO_DEVICE);
++	res = dma_mapping_error(dev, skb_cb->paddr);
++	if (res) {
++		res = -EIO;
++		goto err_free_msdu_id;
++	}
++
++	if (unlikely(info->flags & IEEE80211_TX_CTL_TX_OFFCHAN))
++		freq = ar->scan.roc_freq;
++
++	switch (txmode) {
++	case ATH10K_HW_TXRX_RAW:
++	case ATH10K_HW_TXRX_NATIVE_WIFI:
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;
++		/* pass through */
++	case ATH10K_HW_TXRX_ETHERNET:
++		if (ar->hw_params.continuous_frag_desc) {
++			ext_desc_t = htt->frag_desc.vaddr_desc_64;
++			memset(&ext_desc_t[msdu_id], 0,
++			       sizeof(struct htt_msdu_ext_desc_64));
++			frags = (struct htt_data_tx_desc_frag *)
++				&ext_desc_t[msdu_id].frags;
++			ext_desc = &ext_desc_t[msdu_id];
++			frags[0].tword_addr.paddr_lo =
++				__cpu_to_le32(skb_cb->paddr);
++			frags[0].tword_addr.paddr_hi =
++				__cpu_to_le16(upper_32_bits(skb_cb->paddr));
++			frags[0].tword_addr.len_16 = __cpu_to_le16(msdu->len);
++
++			frags_paddr =  htt->frag_desc.paddr +
++			   (sizeof(struct htt_msdu_ext_desc_64) * msdu_id);
++		} else {
++			frags = txbuf->frags;
++			frags[0].tword_addr.paddr_lo =
++						__cpu_to_le32(skb_cb->paddr);
++			frags[0].tword_addr.paddr_hi =
++				__cpu_to_le16(upper_32_bits(skb_cb->paddr));
++			frags[0].tword_addr.len_16 = __cpu_to_le16(msdu->len);
++			frags[1].tword_addr.paddr_lo = 0;
++			frags[1].tword_addr.paddr_hi = 0;
++			frags[1].tword_addr.len_16 = 0;
++		}
++		flags0 |= SM(txmode, HTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);
++		break;
++	case ATH10K_HW_TXRX_MGMT:
++		flags0 |= SM(ATH10K_HW_TXRX_MGMT,
++			     HTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;
++
++		frags_paddr = skb_cb->paddr;
++		break;
++	}
++
++	/* Normally all commands go through HTC which manages tx credits for
++	 * each endpoint and notifies when tx is completed.
++	 *
++	 * HTT endpoint is creditless so there's no need to care about HTC
++	 * flags. In that case it is trivial to fill the HTC header here.
++	 *
++	 * MSDU transmission is considered completed upon HTT event. This
++	 * implies no relevant resources can be freed until after the event is
++	 * received. That's why HTC tx completion handler itself is ignored by
++	 * setting NULL to transfer_context for all sg items.
++	 *
++	 * There is simply no point in pushing HTT TX_FRM through HTC tx path
++	 * as it's a waste of resources. By bypassing HTC it is possible to
++	 * avoid extra memory allocations, compress data structures and thus
++	 * improve performance.
++	 */
++
++	txbuf->htc_hdr.eid = htt->eid;
++	txbuf->htc_hdr.len = __cpu_to_le16(sizeof(txbuf->cmd_hdr) +
++					   sizeof(txbuf->cmd_tx) +
++					   prefetch_len);
++	txbuf->htc_hdr.flags = 0;
++
++	if (skb_cb->flags & ATH10K_SKB_F_NO_HWCRYPT)
++		flags0 |= HTT_DATA_TX_DESC_FLAGS0_NO_ENCRYPT;
++
++	flags1 |= SM((u16)vdev_id, HTT_DATA_TX_DESC_FLAGS1_VDEV_ID);
++	flags1 |= SM((u16)tid, HTT_DATA_TX_DESC_FLAGS1_EXT_TID);
++	if (msdu->ip_summed == CHECKSUM_PARTIAL &&
++	    !test_bit(ATH10K_FLAG_RAW_MODE, &ar->dev_flags)) {
++		flags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L3_OFFLOAD;
++		flags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L4_OFFLOAD;
++		if (ar->hw_params.continuous_frag_desc)
++			ext_desc->flags |= HTT_MSDU_CHECKSUM_ENABLE;
++	}
++
++	/* Prevent firmware from sending up tx inspection requests. There's
++	 * nothing ath10k can do with frames requested for inspection so force
++	 * it to simply rely a regular tx completion with discard status.
++	 */
++	flags1 |= HTT_DATA_TX_DESC_FLAGS1_POSTPONED;
++
++	txbuf->cmd_hdr.msg_type = HTT_H2T_MSG_TYPE_TX_FRM;
++	txbuf->cmd_tx.flags0 = flags0;
++	txbuf->cmd_tx.flags1 = __cpu_to_le16(flags1);
++	txbuf->cmd_tx.len = __cpu_to_le16(msdu->len);
++	txbuf->cmd_tx.id = __cpu_to_le16(msdu_id);
++
++	/* fill fragment descriptor */
++	txbuf->cmd_tx.frags_paddr = __cpu_to_le64(frags_paddr);
++	if (ath10k_mac_tx_frm_has_freq(ar)) {
++		txbuf->cmd_tx.offchan_tx.peerid =
++				__cpu_to_le16(HTT_INVALID_PEERID);
++		txbuf->cmd_tx.offchan_tx.freq =
++				__cpu_to_le16(freq);
++	} else {
++		txbuf->cmd_tx.peerid =
++				__cpu_to_le32(HTT_INVALID_PEERID);
++	}
++
++	trace_ath10k_htt_tx(ar, msdu_id, msdu->len, vdev_id, tid);
++	ath10k_dbg(ar, ATH10K_DBG_HTT,
++		   "htt tx flags0 %hhu flags1 %hu len %d id %hu frags_paddr %pad, msdu_paddr %pad vdev %hhu tid %hhu freq %hu\n",
++		   flags0, flags1, msdu->len, msdu_id, &frags_paddr,
++		   &skb_cb->paddr, vdev_id, tid, freq);
+ 	ath10k_dbg_dump(ar, ATH10K_DBG_HTT_DUMP, NULL, "htt tx msdu: ",
+ 			msdu->data, msdu->len);
+ 	trace_ath10k_tx_hdr(ar, msdu->data, msdu->len);
+@@ -1093,3 +1706,41 @@ err_free_msdu_id:
+ err:
+ 	return res;
+ }
++
++static const struct ath10k_htt_tx_ops htt_tx_ops_32 = {
++	.htt_send_rx_ring_cfg = ath10k_htt_send_rx_ring_cfg_32,
++	.htt_send_frag_desc_bank_cfg = ath10k_htt_send_frag_desc_bank_cfg_32,
++	.htt_alloc_frag_desc = ath10k_htt_tx_alloc_cont_frag_desc_32,
++	.htt_free_frag_desc = ath10k_htt_tx_free_cont_frag_desc_32,
++	.htt_tx = ath10k_htt_tx_32,
++	.htt_alloc_txbuff = ath10k_htt_tx_alloc_cont_txbuf_32,
++	.htt_free_txbuff = ath10k_htt_tx_free_cont_txbuf_32,
++};
++
++static const struct ath10k_htt_tx_ops htt_tx_ops_64 = {
++	.htt_send_rx_ring_cfg = ath10k_htt_send_rx_ring_cfg_64,
++	.htt_send_frag_desc_bank_cfg = ath10k_htt_send_frag_desc_bank_cfg_64,
++	.htt_alloc_frag_desc = ath10k_htt_tx_alloc_cont_frag_desc_64,
++	.htt_free_frag_desc = ath10k_htt_tx_free_cont_frag_desc_64,
++	.htt_tx = ath10k_htt_tx_64,
++	.htt_alloc_txbuff = ath10k_htt_tx_alloc_cont_txbuf_64,
++	.htt_free_txbuff = ath10k_htt_tx_free_cont_txbuf_64,
++};
++
++static const struct ath10k_htt_tx_ops htt_tx_ops_hl = {
++	.htt_send_rx_ring_cfg = ath10k_htt_send_rx_ring_cfg_hl,
++	.htt_send_frag_desc_bank_cfg = ath10k_htt_send_frag_desc_bank_cfg_32,
++	.htt_tx = ath10k_htt_tx_hl,
++};
++
++void ath10k_htt_set_tx_ops(struct ath10k_htt *htt)
++{
++	struct ath10k *ar = htt->ar;
++
++	if (ar->is_high_latency)
++		htt->tx_ops = &htt_tx_ops_hl;
++	else if (ar->hw_params.target_64bit)
++		htt->tx_ops = &htt_tx_ops_64;
++	else
++		htt->tx_ops = &htt_tx_ops_32;
++}
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/hw.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/hw.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/hw.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2014-2015 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -310,7 +310,7 @@ static struct ath10k_hw_ce_dst_src_wm_re
+ 	.wm_high	= &wcn3990_dst_wm_high,
+ };
+ 
+-struct ath10k_hw_ce_regs wcn3990_ce_regs = {
++const struct ath10k_hw_ce_regs wcn3990_ce_regs = {
+ 	.sr_base_addr		= 0x00000000,
+ 	.sr_size_addr		= 0x00000008,
+ 	.dr_base_addr		= 0x0000000c,
+@@ -457,7 +457,7 @@ static struct ath10k_hw_ce_dst_src_wm_re
+ 	.wm_high	= &qcax_dst_wm_high,
+ };
+ 
+-struct ath10k_hw_ce_regs qcax_ce_regs = {
++const struct ath10k_hw_ce_regs qcax_ce_regs = {
+ 	.sr_base_addr		= 0x00000000,
+ 	.sr_size_addr		= 0x00000004,
+ 	.dr_base_addr		= 0x00000008,
+@@ -604,8 +604,13 @@ static void ath10k_hw_qca988x_set_covera
+ 
+ 	/* Only modify registers if the core is started. */
+ 	if ((ar->state != ATH10K_STATE_ON) &&
+-	    (ar->state != ATH10K_STATE_RESTARTED))
++	    (ar->state != ATH10K_STATE_RESTARTED)) {
++		spin_lock_bh(&ar->data_lock);
++		/* Store config value for when radio boots up */
++		ar->fw_coverage.coverage_class = value;
++		spin_unlock_bh(&ar->data_lock);
+ 		goto unlock;
++	}
+ 
+ 	/* Retrieve the current values of the two registers that need to be
+ 	 * adjusted.
+@@ -637,7 +642,7 @@ static void ath10k_hw_qca988x_set_covera
+ 		ar->fw_coverage.reg_ack_cts_timeout_orig = timeout_reg;
+ 	ar->fw_coverage.reg_phyclk = phyclk_reg;
+ 
+-	/* Calculat new value based on the (original) firmware calculation. */
++	/* Calculate new value based on the (original) firmware calculation. */
+ 	slottime_reg = ar->fw_coverage.reg_slottime_orig;
+ 	timeout_reg = ar->fw_coverage.reg_ack_cts_timeout_orig;
+ 
+@@ -812,6 +817,8 @@ static int ath10k_hw_qca6174_enable_pll_
+ 	if (ret)
+ 		return -EINVAL;
+ 
++	reg_val &= ~(WLAN_PLL_CONTROL_REFDIV_MASK | WLAN_PLL_CONTROL_DIV_MASK |
++		     WLAN_PLL_CONTROL_NOPWD_MASK);
+ 	reg_val |= (SM(hw_clk->refdiv, WLAN_PLL_CONTROL_REFDIV) |
+ 		    SM(hw_clk->div, WLAN_PLL_CONTROL_DIV) |
+ 		    SM(1, WLAN_PLL_CONTROL_NOPWD));
+@@ -926,3 +933,5 @@ const struct ath10k_hw_ops qca6174_ops =
+ 	.set_coverage_class = ath10k_hw_qca988x_set_coverage_class,
+ 	.enable_pll_clk = ath10k_hw_qca6174_enable_pll_clock,
+ };
++
++const struct ath10k_hw_ops wcn3990_ops = {};
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/hw.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/hw.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/hw.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -20,16 +20,26 @@
+ 
+ #include "targaddrs.h"
+ 
++enum ath10k_bus {
++	ATH10K_BUS_PCI,
++	ATH10K_BUS_AHB,
++	ATH10K_BUS_SDIO,
++	ATH10K_BUS_USB,
++	ATH10K_BUS_SNOC,
++};
++
+ #define ATH10K_FW_DIR			"ath10k"
+ 
+-#define QCA988X_2_0_DEVICE_ID   (0x003c)
+-#define QCA6164_2_1_DEVICE_ID   (0x0041)
+-#define QCA6174_2_1_DEVICE_ID   (0x003e)
+-#define QCA99X0_2_0_DEVICE_ID   (0x0040)
+-#define QCA9888_2_0_DEVICE_ID	(0x0056)
+-#define QCA9984_1_0_DEVICE_ID	(0x0046)
+-#define QCA9377_1_0_DEVICE_ID   (0x0042)
+-#define QCA9887_1_0_DEVICE_ID   (0x0050)
++#define QCA988X_2_0_DEVICE_ID_UBNT	(0x11ac)
++#define QCA988X_2_0_DEVICE_ID		(0x003c)
++#define QCA6164_2_1_DEVICE_ID		(0x0041)
++#define QCA6174_2_1_DEVICE_ID		(0x003e)
++#define QCA99X0_2_0_DEVICE_ID		(0x0040)
++#define QCA9888_2_0_DEVICE_ID		(0x0056)
++#define QCA9984_1_0_DEVICE_ID		(0x0046)
++#define QCA9377_1_0_DEVICE_ID		(0x0042)
++#define QCA9887_1_0_DEVICE_ID		(0x0050)
++#define SPARKLAN_WPEQ_160N_DEVICE_ID	(0x9378)
+ 
+ /* QCA988X 1.0 definitions (unsupported) */
+ #define QCA988X_HW_1_0_CHIP_ID_REV	0x0
+@@ -89,6 +99,7 @@ enum qca9377_chip_id_rev {
+ 
+ #define QCA6174_HW_3_0_FW_DIR		"ath10k/QCA6174/hw3.0"
+ #define QCA6174_HW_3_0_BOARD_DATA_FILE	"board.bin"
++#define QCA6174_HW_3_0_BOARD_DATA_FILE_SDIO	"board-sdio.bin"
+ #define QCA6174_HW_3_0_PATCH_LOAD_ADDR	0x1234
+ 
+ /* QCA99X0 1.0 definitions (unsupported) */
+@@ -120,6 +131,8 @@ enum qca9377_chip_id_rev {
+ /* QCA9377 1.0 definitions */
+ #define QCA9377_HW_1_0_FW_DIR          ATH10K_FW_DIR "/QCA9377/hw1.0"
+ #define QCA9377_HW_1_0_BOARD_DATA_FILE "board.bin"
++#define QCA9377_HW_1_0_BOARD_DATA_FILE_USB "board-usb.bin"
++#define QCA9377_HW_1_0_BOARD_DATA_FILE_SDIO "board-sdio.bin"
+ #define QCA9377_HW_1_0_PATCH_LOAD_ADDR	0x1234
+ 
+ /* QCA4019 1.0 definitions */
+@@ -128,6 +141,10 @@ enum qca9377_chip_id_rev {
+ #define QCA4019_HW_1_0_BOARD_DATA_FILE "board.bin"
+ #define QCA4019_HW_1_0_PATCH_LOAD_ADDR  0x1234
+ 
++/* WCN3990 1.0 definitions */
++#define WCN3990_HW_1_0_DEV_VERSION	ATH10K_HW_WCN3990
++#define WCN3990_HW_1_0_FW_DIR		ATH10K_FW_DIR "/WCN3990/hw1.0"
++
+ #define ATH10K_FW_FILE_BASE		"firmware"
+ #define ATH10K_FW_API_MAX		6
+ #define ATH10K_FW_API_MIN		2
+@@ -369,8 +386,8 @@ extern const struct ath10k_hw_values qca
+ extern const struct ath10k_hw_values qca9888_values;
+ extern const struct ath10k_hw_values qca4019_values;
+ extern const struct ath10k_hw_values wcn3990_values;
+-extern struct ath10k_hw_ce_regs wcn3990_ce_regs;
+-extern struct ath10k_hw_ce_regs qcax_ce_regs;
++extern const struct ath10k_hw_ce_regs wcn3990_ce_regs;
++extern const struct ath10k_hw_ce_regs qcax_ce_regs;
+ 
+ void ath10k_hw_fill_survey_time(struct ath10k *ar, struct survey_info *survey,
+ 				u32 cc, u32 rcc, u32 cc_prev, u32 rcc_prev);
+@@ -487,6 +504,7 @@ struct ath10k_hw_clk_params {
+ struct ath10k_hw_params {
+ 	u32 id;
+ 	u16 dev_id;
++	enum ath10k_bus bus;
+ 	const char *name;
+ 	u32 patch_load_addr;
+ 	int uart_pin;
+@@ -550,6 +568,28 @@ struct ath10k_hw_params {
+ 	 */
+ 	int vht160_mcs_rx_highest;
+ 	int vht160_mcs_tx_highest;
++
++	/* Number of ciphers supported (i.e First N) in cipher_suites array */
++	int n_cipher_suites;
++
++	u32 num_peers;
++	u32 ast_skid_limit;
++	u32 num_wds_entries;
++
++	/* Targets supporting physical addressing capability above 32-bits */
++	bool target_64bit;
++
++	/* Target rx ring fill level */
++	u32 rx_ring_fill_level;
++
++	/* target supporting per ce IRQ */
++	bool per_ce_irq;
++
++	/* Specifies whether or not the device should be started once.
++	 * If set, the device will be started once by the early fw probe
++	 * and it will not be terminated afterwards.
++	 */
++	bool start_once;
+ };
+ 
+ struct htt_rx_desc;
+@@ -564,6 +604,7 @@ struct ath10k_hw_ops {
+ extern const struct ath10k_hw_ops qca988x_ops;
+ extern const struct ath10k_hw_ops qca99x0_ops;
+ extern const struct ath10k_hw_ops qca6174_ops;
++extern const struct ath10k_hw_ops wcn3990_ops;
+ 
+ extern const struct ath10k_hw_clk_params qca6174_clk[];
+ 
+@@ -658,8 +699,18 @@ ath10k_rx_desc_get_l3_pad_bytes(struct a
+ #define TARGET_TLV_NUM_TDLS_VDEVS		1
+ #define TARGET_TLV_NUM_TIDS			((TARGET_TLV_NUM_PEERS) * 2)
+ #define TARGET_TLV_NUM_MSDU_DESC		(1024 + 32)
++#define TARGET_TLV_NUM_MSDU_DESC_HL		64
+ #define TARGET_TLV_NUM_WOW_PATTERNS		22
+ 
++/* Target specific defines for WMI-HL-1.0 firmware */
++#define TARGET_HL_10_TLV_NUM_PEERS		14
++#define TARGET_HL_10_TLV_AST_SKID_LIMIT		6
++#define TARGET_HL_10_TLV_NUM_WDS_ENTRIES	2
++
++/* Target specific defines for QCA6174 and QCA9377 high latency firmware */
++#define TARGET_QCA6174_HL_NUM_PEERS		15
++#define TARGET_QCA9377_HL_NUM_PEERS		15
++
+ /* Diagnostic Window */
+ #define CE_DIAG_PIPE	7
+ 
+@@ -865,6 +916,7 @@ ath10k_rx_desc_get_l3_pad_bytes(struct a
+ #define PCIE_INTR_CLR_ADDRESS			ar->regs->pcie_intr_clr_address
+ #define SCRATCH_3_ADDRESS			ar->regs->scratch_3_address
+ #define CPU_INTR_ADDRESS			0x0010
++#define FW_RAM_CONFIG_ADDRESS			0x0018
+ 
+ #define CCNT_TO_MSEC(ar, x) ((x) / ar->hw_params.channel_counters_freq_hz)
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/mac.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/mac.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/mac.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -242,6 +243,16 @@ static int ath10k_send_key(struct ath10k
+ 	case WLAN_CIPHER_SUITE_WEP104:
+ 		arg.key_cipher = WMI_CIPHER_WEP;
+ 		break;
++	case WLAN_CIPHER_SUITE_CCMP_256:
++		arg.key_cipher = WMI_CIPHER_AES_CCM;
++		break;
++	case WLAN_CIPHER_SUITE_GCMP:
++	case WLAN_CIPHER_SUITE_GCMP_256:
++		arg.key_cipher = WMI_CIPHER_AES_GCM;
++		break;
++	case WLAN_CIPHER_SUITE_BIP_GMAC_128:
++	case WLAN_CIPHER_SUITE_BIP_GMAC_256:
++	case WLAN_CIPHER_SUITE_BIP_CMAC_256:
+ 	case WLAN_CIPHER_SUITE_AES_CMAC:
+ 		WARN_ON(1);
+ 		return -EINVAL;
+@@ -934,8 +945,12 @@ static void ath10k_mac_vif_beacon_cleanu
+ 	ath10k_mac_vif_beacon_free(arvif);
+ 
+ 	if (arvif->beacon_buf) {
+-		dma_free_coherent(ar->dev, IEEE80211_MAX_FRAME_LEN,
+-				  arvif->beacon_buf, arvif->beacon_paddr);
++		if (ar->is_high_latency)
++			kfree(arvif->beacon_buf);
++		else
++			dma_free_coherent(ar->dev, IEEE80211_MAX_FRAME_LEN,
++					  arvif->beacon_buf,
++					  arvif->beacon_paddr);
+ 		arvif->beacon_buf = NULL;
+ 	}
+ }
+@@ -2966,7 +2981,7 @@ static int ath10k_station_assoc(struct a
+ 		}
+ 
+ 		/* Plumb cached keys only for static WEP */
+-		if (arvif->def_wep_key_idx != -1) {
++		if ((arvif->def_wep_key_idx != -1) && (!sta->tdls)) {
+ 			ret = ath10k_install_peer_wep_keys(arvif, sta->addr);
+ 			if (ret) {
+ 				ath10k_warn(ar, "failed to install peer wep keys for vdev %i: %d\n",
+@@ -3564,7 +3579,9 @@ ath10k_mac_tx_h_get_txpath(struct ath10k
+ 		return ATH10K_MAC_TX_HTT;
+ 	case ATH10K_HW_TXRX_MGMT:
+ 		if (test_bit(ATH10K_FW_FEATURE_HAS_WMI_MGMT_TX,
+-			     ar->running_fw->fw_file.fw_features))
++			     ar->running_fw->fw_file.fw_features) ||
++			     test_bit(WMI_SERVICE_MGMT_TX_WMI,
++				      ar->wmi.svc_map))
+ 			return ATH10K_MAC_TX_WMI_MGMT;
+ 		else if (ar->htt.target_version_major >= 3)
+ 			return ATH10K_MAC_TX_HTT;
+@@ -3796,6 +3813,7 @@ void ath10k_mgmt_over_wmi_tx_work(struct
+ {
+ 	struct ath10k *ar = container_of(work, struct ath10k, wmi_mgmt_tx_work);
+ 	struct sk_buff *skb;
++	dma_addr_t paddr;
+ 	int ret;
+ 
+ 	for (;;) {
+@@ -3803,11 +3821,27 @@ void ath10k_mgmt_over_wmi_tx_work(struct
+ 		if (!skb)
+ 			break;
+ 
+-		ret = ath10k_wmi_mgmt_tx(ar, skb);
+-		if (ret) {
+-			ath10k_warn(ar, "failed to transmit management frame via WMI: %d\n",
+-				    ret);
+-			ieee80211_free_txskb(ar->hw, skb);
++		if (test_bit(ATH10K_FW_FEATURE_MGMT_TX_BY_REF,
++			     ar->running_fw->fw_file.fw_features)) {
++			paddr = dma_map_single(ar->dev, skb->data,
++					       skb->len, DMA_TO_DEVICE);
++			if (!paddr)
++				continue;
++			ret = ath10k_wmi_mgmt_tx_send(ar, skb, paddr);
++			if (ret) {
++				ath10k_warn(ar, "failed to transmit management frame by ref via WMI: %d\n",
++					    ret);
++				dma_unmap_single(ar->dev, paddr, skb->len,
++						 DMA_FROM_DEVICE);
++				ieee80211_free_txskb(ar->hw, skb);
++			}
++		} else {
++			ret = ath10k_wmi_mgmt_tx(ar, skb);
++			if (ret) {
++				ath10k_warn(ar, "failed to transmit management frame via WMI: %d\n",
++					    ret);
++				ieee80211_free_txskb(ar->hw, skb);
++			}
+ 		}
+ 	}
+ }
+@@ -5039,10 +5073,17 @@ static int ath10k_add_interface(struct i
+ 	if (vif->type == NL80211_IFTYPE_ADHOC ||
+ 	    vif->type == NL80211_IFTYPE_MESH_POINT ||
+ 	    vif->type == NL80211_IFTYPE_AP) {
+-		arvif->beacon_buf = dma_zalloc_coherent(ar->dev,
+-							IEEE80211_MAX_FRAME_LEN,
+-							&arvif->beacon_paddr,
+-							GFP_ATOMIC);
++		if (ar->is_high_latency) {
++			arvif->beacon_buf = kmalloc(IEEE80211_MAX_FRAME_LEN,
++						    GFP_KERNEL);
++			arvif->beacon_paddr = (dma_addr_t)arvif->beacon_buf;
++		} else {
++			arvif->beacon_buf =
++				dma_zalloc_coherent(ar->dev,
++						    IEEE80211_MAX_FRAME_LEN,
++						    &arvif->beacon_paddr,
++						    GFP_ATOMIC);
++		}
+ 		if (!arvif->beacon_buf) {
+ 			ret = -ENOMEM;
+ 			ath10k_warn(ar, "failed to allocate beacon buffer: %d\n",
+@@ -5231,8 +5272,12 @@ err_vdev_delete:
+ 
+ err:
+ 	if (arvif->beacon_buf) {
+-		dma_free_coherent(ar->dev, IEEE80211_MAX_FRAME_LEN,
+-				  arvif->beacon_buf, arvif->beacon_paddr);
++		if (ar->is_high_latency)
++			kfree(arvif->beacon_buf);
++		else
++			dma_free_coherent(ar->dev, IEEE80211_MAX_FRAME_LEN,
++					  arvif->beacon_buf,
++					  arvif->beacon_paddr);
+ 		arvif->beacon_buf = NULL;
+ 	}
+ 
+@@ -5575,6 +5620,59 @@ static void ath10k_mac_op_set_coverage_c
+ 	ar->hw_params.hw_ops->set_coverage_class(ar, value);
+ }
+ 
++struct ath10k_mac_tdls_iter_data {
++	u32 num_tdls_stations;
++	struct ieee80211_vif *curr_vif;
++};
++
++static void ath10k_mac_tdls_vif_stations_count_iter(void *data,
++						    struct ieee80211_sta *sta)
++{
++	struct ath10k_mac_tdls_iter_data *iter_data = data;
++	struct ath10k_sta *arsta = (struct ath10k_sta *)sta->drv_priv;
++	struct ieee80211_vif *sta_vif = arsta->arvif->vif;
++
++	if (sta->tdls && sta_vif == iter_data->curr_vif)
++		iter_data->num_tdls_stations++;
++}
++
++static int ath10k_mac_tdls_vif_stations_count(struct ieee80211_hw *hw,
++					      struct ieee80211_vif *vif)
++{
++	struct ath10k_mac_tdls_iter_data data = {};
++
++	data.curr_vif = vif;
++
++	ieee80211_iterate_stations_atomic(hw,
++					  ath10k_mac_tdls_vif_stations_count_iter,
++					  &data);
++	return data.num_tdls_stations;
++}
++
++static void ath10k_mac_tdls_vifs_count_iter(void *data, u8 *mac,
++					    struct ieee80211_vif *vif)
++{
++	struct ath10k_vif *arvif = (void *)vif->drv_priv;
++	int *num_tdls_vifs = data;
++
++	if (vif->type != NL80211_IFTYPE_STATION)
++		return;
++
++	if (ath10k_mac_tdls_vif_stations_count(arvif->ar->hw, vif) > 0)
++		(*num_tdls_vifs)++;
++}
++
++static int ath10k_mac_tdls_vifs_count(struct ieee80211_hw *hw)
++{
++	int num_tdls_vifs = 0;
++
++	ieee80211_iterate_active_interfaces_atomic(hw,
++						   IEEE80211_IFACE_ITER_NORMAL,
++						   ath10k_mac_tdls_vifs_count_iter,
++						   &num_tdls_vifs);
++	return num_tdls_vifs;
++}
++
+ static int ath10k_hw_scan(struct ieee80211_hw *hw,
+ 			  struct ieee80211_vif *vif,
+ 			  struct ieee80211_scan_request *hw_req)
+@@ -5588,6 +5686,11 @@ static int ath10k_hw_scan(struct ieee802
+ 
+ 	mutex_lock(&ar->conf_mutex);
+ 
++	if (ath10k_mac_tdls_vif_stations_count(hw, vif) > 0) {
++		ret = -EBUSY;
++		goto exit;
++	}
++
+ 	spin_lock_bh(&ar->data_lock);
+ 	switch (ar->scan.state) {
+ 	case ATH10K_SCAN_IDLE:
+@@ -5723,7 +5826,10 @@ static int ath10k_set_key(struct ieee802
+ 	u32 flags2;
+ 
+ 	/* this one needs to be done in software */
+-	if (key->cipher == WLAN_CIPHER_SUITE_AES_CMAC)
++	if (key->cipher == WLAN_CIPHER_SUITE_AES_CMAC ||
++	    key->cipher == WLAN_CIPHER_SUITE_BIP_GMAC_128 ||
++	    key->cipher == WLAN_CIPHER_SUITE_BIP_GMAC_256 ||
++	    key->cipher == WLAN_CIPHER_SUITE_BIP_CMAC_256)
+ 		return 1;
+ 
+ 	if (arvif->nohwcrypt)
+@@ -5841,6 +5947,10 @@ static int ath10k_set_key(struct ieee802
+ 		ath10k_warn(ar, "Peer %pM disappeared!\n", peer_addr);
+ 	spin_unlock_bh(&ar->data_lock);
+ 
++	if (sta && sta->tdls)
++		ath10k_wmi_peer_set_param(ar, arvif->vdev_id, sta->addr,
++					  WMI_PEER_AUTHORIZE, 1);
++
+ exit:
+ 	mutex_unlock(&ar->conf_mutex);
+ 	return ret;
+@@ -5999,59 +6109,6 @@ static void ath10k_mac_dec_num_stations(
+ 	ar->num_stations--;
+ }
+ 
+-struct ath10k_mac_tdls_iter_data {
+-	u32 num_tdls_stations;
+-	struct ieee80211_vif *curr_vif;
+-};
+-
+-static void ath10k_mac_tdls_vif_stations_count_iter(void *data,
+-						    struct ieee80211_sta *sta)
+-{
+-	struct ath10k_mac_tdls_iter_data *iter_data = data;
+-	struct ath10k_sta *arsta = (struct ath10k_sta *)sta->drv_priv;
+-	struct ieee80211_vif *sta_vif = arsta->arvif->vif;
+-
+-	if (sta->tdls && sta_vif == iter_data->curr_vif)
+-		iter_data->num_tdls_stations++;
+-}
+-
+-static int ath10k_mac_tdls_vif_stations_count(struct ieee80211_hw *hw,
+-					      struct ieee80211_vif *vif)
+-{
+-	struct ath10k_mac_tdls_iter_data data = {};
+-
+-	data.curr_vif = vif;
+-
+-	ieee80211_iterate_stations_atomic(hw,
+-					  ath10k_mac_tdls_vif_stations_count_iter,
+-					  &data);
+-	return data.num_tdls_stations;
+-}
+-
+-static void ath10k_mac_tdls_vifs_count_iter(void *data, u8 *mac,
+-					    struct ieee80211_vif *vif)
+-{
+-	struct ath10k_vif *arvif = (void *)vif->drv_priv;
+-	int *num_tdls_vifs = data;
+-
+-	if (vif->type != NL80211_IFTYPE_STATION)
+-		return;
+-
+-	if (ath10k_mac_tdls_vif_stations_count(arvif->ar->hw, vif) > 0)
+-		(*num_tdls_vifs)++;
+-}
+-
+-static int ath10k_mac_tdls_vifs_count(struct ieee80211_hw *hw)
+-{
+-	int num_tdls_vifs = 0;
+-
+-	ieee80211_iterate_active_interfaces_atomic(hw,
+-						   IEEE80211_IFACE_ITER_NORMAL,
+-						   ath10k_mac_tdls_vifs_count_iter,
+-						   &num_tdls_vifs);
+-	return num_tdls_vifs;
+-}
+-
+ static int ath10k_sta_state(struct ieee80211_hw *hw,
+ 			    struct ieee80211_vif *vif,
+ 			    struct ieee80211_sta *sta,
+@@ -6486,6 +6543,11 @@ static int ath10k_remain_on_channel(stru
+ 
+ 	mutex_lock(&ar->conf_mutex);
+ 
++	if (ath10k_mac_tdls_vif_stations_count(hw, vif) > 0) {
++		ret = -EBUSY;
++		goto exit;
++	}
++
+ 	spin_lock_bh(&ar->data_lock);
+ 	switch (ar->scan.state) {
+ 	case ATH10K_SCAN_IDLE:
+@@ -7532,6 +7594,16 @@ ath10k_mac_op_assign_vif_chanctx(struct
+ 				    arvif->vdev_id, ret);
+ 	}
+ 
++	if (ath10k_peer_stats_enabled(ar)) {
++		ar->pktlog_filter |= ATH10K_PKTLOG_PEER_STATS;
++		ret = ath10k_wmi_pdev_pktlog_enable(ar,
++						    ar->pktlog_filter);
++		if (ret) {
++			ath10k_warn(ar, "failed to enable pktlog %d\n", ret);
++			goto err_stop;
++		}
++	}
++
+ 	mutex_unlock(&ar->conf_mutex);
+ 	return 0;
+ 
+@@ -7616,6 +7688,34 @@ static void ath10k_mac_op_sta_pre_rcu_re
+ 			peer->removed = true;
+ }
+ 
++static void ath10k_sta_statistics(struct ieee80211_hw *hw,
++				  struct ieee80211_vif *vif,
++				  struct ieee80211_sta *sta,
++				  struct station_info *sinfo)
++{
++	struct ath10k_sta *arsta = (struct ath10k_sta *)sta->drv_priv;
++	struct ath10k *ar = arsta->arvif->ar;
++
++	if (!ath10k_peer_stats_enabled(ar))
++		return;
++
++	sinfo->rx_duration = arsta->rx_duration;
++	sinfo->filled |= 1ULL << NL80211_STA_INFO_RX_DURATION;
++
++	if (!arsta->txrate.legacy && !arsta->txrate.nss)
++		return;
++
++	if (arsta->txrate.legacy) {
++		sinfo->txrate.legacy = arsta->txrate.legacy;
++	} else {
++		sinfo->txrate.mcs = arsta->txrate.mcs;
++		sinfo->txrate.nss = arsta->txrate.nss;
++		sinfo->txrate.bw = arsta->txrate.bw;
++	}
++	sinfo->txrate.flags = arsta->txrate.flags;
++	sinfo->filled |= 1ULL << NL80211_STA_INFO_TX_BITRATE;
++}
++
+ static const struct ieee80211_ops ath10k_ops = {
+ 	.tx				= ath10k_mac_op_tx,
+ 	.wake_tx_queue			= ath10k_mac_op_wake_tx_queue,
+@@ -7657,6 +7757,7 @@ static const struct ieee80211_ops ath10k
+ 	.unassign_vif_chanctx		= ath10k_mac_op_unassign_vif_chanctx,
+ 	.switch_vif_chanctx		= ath10k_mac_op_switch_vif_chanctx,
+ 	.sta_pre_rcu_remove		= ath10k_mac_op_sta_pre_rcu_remove,
++	.sta_statistics			= ath10k_sta_statistics,
+ 
+ 	CFG80211_TESTMODE_CMD(ath10k_tm_cmd)
+ 
+@@ -7667,7 +7768,6 @@ static const struct ieee80211_ops ath10k
+ #endif
+ #ifdef CONFIG_MAC80211_DEBUGFS
+ 	.sta_add_debugfs		= ath10k_sta_add_debugfs,
+-	.sta_statistics			= ath10k_sta_statistics,
+ #endif
+ };
+ 
+@@ -8095,7 +8195,22 @@ int ath10k_mac_register(struct ath10k *a
+ 		WLAN_CIPHER_SUITE_WEP104,
+ 		WLAN_CIPHER_SUITE_TKIP,
+ 		WLAN_CIPHER_SUITE_CCMP,
++
++		/* Do not add hardware supported ciphers before this line.
++		 * Allow software encryption for all chips. Don't forget to
++		 * update n_cipher_suites below.
++		 */
+ 		WLAN_CIPHER_SUITE_AES_CMAC,
++		WLAN_CIPHER_SUITE_BIP_CMAC_256,
++		WLAN_CIPHER_SUITE_BIP_GMAC_128,
++		WLAN_CIPHER_SUITE_BIP_GMAC_256,
++
++		/* Only QCA99x0 and QCA4019 varients support GCMP-128, GCMP-256
++		 * and CCMP-256 in hardware.
++		 */
++		WLAN_CIPHER_SUITE_GCMP,
++		WLAN_CIPHER_SUITE_GCMP_256,
++		WLAN_CIPHER_SUITE_CCMP_256,
+ 	};
+ 	struct ieee80211_supported_band *band;
+ 	void *channels;
+@@ -8167,8 +8282,13 @@ int ath10k_mac_register(struct ath10k *a
+ 			BIT(NL80211_IFTYPE_P2P_GO);
+ 
+ 	ieee80211_hw_set(ar->hw, SIGNAL_DBM);
+-	ieee80211_hw_set(ar->hw, SUPPORTS_PS);
+-	ieee80211_hw_set(ar->hw, SUPPORTS_DYNAMIC_PS);
++
++	if (!test_bit(ATH10K_FW_FEATURE_NO_PS,
++		      ar->running_fw->fw_file.fw_features)) {
++		ieee80211_hw_set(ar->hw, SUPPORTS_PS);
++		ieee80211_hw_set(ar->hw, SUPPORTS_DYNAMIC_PS);
++	}
++
+ 	ieee80211_hw_set(ar->hw, MFP_CAPABLE);
+ 	ieee80211_hw_set(ar->hw, REPORTS_TX_ACK_STATUS);
+ 	ieee80211_hw_set(ar->hw, HAS_RATE_CONTROL);
+@@ -8222,9 +8342,13 @@ int ath10k_mac_register(struct ath10k *a
+ 	if (test_bit(WMI_SERVICE_TDLS, ar->wmi.svc_map) ||
+ 	    test_bit(WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY, ar->wmi.svc_map)) {
+ 		ar->hw->wiphy->flags |= WIPHY_FLAG_SUPPORTS_TDLS;
+-		ieee80211_hw_set(ar->hw, TDLS_WIDER_BW);
++		if (test_bit(WMI_SERVICE_TDLS_WIDER_BANDWIDTH, ar->wmi.svc_map))
++			ieee80211_hw_set(ar->hw, TDLS_WIDER_BW);
+ 	}
+ 
++	if (test_bit(WMI_SERVICE_TDLS_UAPSD_BUFFER_STA, ar->wmi.svc_map))
++		ieee80211_hw_set(ar->hw, SUPPORTS_TDLS_BUFFER_STA);
++
+ 	ar->hw->wiphy->flags |= WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL;
+ 	ar->hw->wiphy->flags |= WIPHY_FLAG_HAS_CHANNEL_SWITCH;
+ 	ar->hw->wiphy->max_remain_on_channel_duration = 5000;
+@@ -8307,15 +8431,6 @@ int ath10k_mac_register(struct ath10k *a
+ 			ath10k_warn(ar, "failed to initialise DFS pattern detector\n");
+ 	}
+ 
+-	/* Current wake_tx_queue implementation imposes a significant
+-	 * performance penalty in some setups. The tx scheduling code needs
+-	 * more work anyway so disable the wake_tx_queue unless firmware
+-	 * supports the pull-push mechanism.
+-	 */
+-	if (!test_bit(ATH10K_FW_FEATURE_PEER_FLOW_CONTROL,
+-		      ar->running_fw->fw_file.fw_features))
+-		ar->ops->wake_tx_queue = NULL;
+-
+ 	ret = ath10k_mac_init_rd(ar);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to derive regdom: %d\n", ret);
+@@ -8334,7 +8449,18 @@ int ath10k_mac_register(struct ath10k *a
+ 	}
+ 
+ 	ar->hw->wiphy->cipher_suites = cipher_suites;
+-	ar->hw->wiphy->n_cipher_suites = ARRAY_SIZE(cipher_suites);
++
++	/* QCA988x and QCA6174 family chips do not support CCMP-256, GCMP-128
++	 * and GCMP-256 ciphers in hardware. Fetch number of ciphers supported
++	 * from chip specific hw_param table.
++	 */
++	if (!ar->hw_params.n_cipher_suites ||
++	    ar->hw_params.n_cipher_suites > ARRAY_SIZE(cipher_suites)) {
++		ath10k_err(ar, "invalid hw_params.n_cipher_suites %d\n",
++			   ar->hw_params.n_cipher_suites);
++		ar->hw_params.n_cipher_suites = 8;
++	}
++	ar->hw->wiphy->n_cipher_suites = ar->hw_params.n_cipher_suites;
+ 
+ 	wiphy_ext_feature_set(ar->hw->wiphy, NL80211_EXT_FEATURE_CQM_RSSI_LIST);
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/mac.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/mac.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/mac.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/pci.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/pci.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/pci.c
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -23,6 +23,7 @@
+ 
+ #include "core.h"
+ #include "debug.h"
++#include "coredump.h"
+ 
+ #include "targaddrs.h"
+ #include "bmi.h"
+@@ -51,7 +52,19 @@ MODULE_PARM_DESC(reset_mode, "0: auto, 1
+ #define ATH10K_PCI_TARGET_WAIT 3000
+ #define ATH10K_PCI_NUM_WARM_RESET_ATTEMPTS 3
+ 
++/* Maximum number of bytes that can be handled atomically by
++ * diag read and write.
++ */
++#define ATH10K_DIAG_TRANSFER_LIMIT	0x5000
++
++#define QCA99X0_PCIE_BAR0_START_REG    0x81030
++#define QCA99X0_CPU_MEM_ADDR_REG       0x4d00c
++#define QCA99X0_CPU_MEM_DATA_REG       0x4d010
++
+ static const struct pci_device_id ath10k_pci_id_table[] = {
++	/* PCI-E QCA988X V2 (Ubiquiti branded) */
++	{ PCI_VDEVICE(UBIQUITI, QCA988X_2_0_DEVICE_ID_UBNT) },
++
+ 	{ PCI_VDEVICE(ATHEROS, QCA988X_2_0_DEVICE_ID) }, /* PCI-E QCA988X V2 */
+ 	{ PCI_VDEVICE(ATHEROS, QCA6164_2_1_DEVICE_ID) }, /* PCI-E QCA6164 V2.1 */
+ 	{ PCI_VDEVICE(ATHEROS, QCA6174_2_1_DEVICE_ID) }, /* PCI-E QCA6174 V2.1 */
+@@ -68,6 +81,7 @@ static const struct ath10k_pci_supp_chip
+ 	 * hacks. ath10k doesn't have them and these devices crash horribly
+ 	 * because of that.
+ 	 */
++	{ QCA988X_2_0_DEVICE_ID_UBNT, QCA988X_HW_2_0_CHIP_ID_REV },
+ 	{ QCA988X_2_0_DEVICE_ID, QCA988X_HW_2_0_CHIP_ID_REV },
+ 
+ 	{ QCA6164_2_1_DEVICE_ID, QCA6174_HW_2_1_CHIP_ID_REV },
+@@ -585,10 +599,10 @@ skip:
+ 	spin_unlock_irqrestore(&ar_pci->ps_lock, flags);
+ }
+ 
+-static void ath10k_pci_ps_timer(unsigned long ptr)
++static void ath10k_pci_ps_timer(struct timer_list *t)
+ {
+-	struct ath10k *ar = (void *)ptr;
+-	struct ath10k_pci *ar_pci = ath10k_pci_priv(ar);
++	struct ath10k_pci *ar_pci = from_timer(ar_pci, t, ps_timer);
++	struct ath10k *ar = ar_pci->ar;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ar_pci->ps_lock, flags);
+@@ -785,7 +799,7 @@ static int __ath10k_pci_rx_post_buf(stru
+ 	ATH10K_SKB_RXCB(skb)->paddr = paddr;
+ 
+ 	spin_lock_bh(&ce->ce_lock);
+-	ret = __ath10k_ce_rx_post_buf(ce_pipe, skb, paddr);
++	ret = ce_pipe->ops->ce_rx_post_buf(ce_pipe, skb, paddr);
+ 	spin_unlock_bh(&ce->ce_lock);
+ 	if (ret) {
+ 		dma_unmap_single(ar->dev, paddr, skb->len + skb_tailroom(skb),
+@@ -838,9 +852,10 @@ void ath10k_pci_rx_post(struct ath10k *a
+ 		ath10k_pci_rx_post_pipe(&ar_pci->pipe_info[i]);
+ }
+ 
+-void ath10k_pci_rx_replenish_retry(unsigned long ptr)
++void ath10k_pci_rx_replenish_retry(struct timer_list *t)
+ {
+-	struct ath10k *ar = (void *)ptr;
++	struct ath10k_pci *ar_pci = from_timer(ar_pci, t, rx_post_retry);
++	struct ath10k *ar = ar_pci->ar;
+ 
+ 	ath10k_pci_rx_post(ar);
+ }
+@@ -922,7 +937,7 @@ static int ath10k_pci_diag_read_mem(stru
+ 		nbytes = min_t(unsigned int, remaining_bytes,
+ 			       DIAG_TRANSFER_LIMIT);
+ 
+-		ret = __ath10k_ce_rx_post_buf(ce_diag, &ce_data, ce_data);
++		ret = ce_diag->ops->ce_rx_post_buf(ce_diag, &ce_data, ce_data);
+ 		if (ret != 0)
+ 			goto done;
+ 
+@@ -1088,7 +1103,7 @@ int ath10k_pci_diag_write_mem(struct ath
+ 		nbytes = min_t(int, remaining_bytes, DIAG_TRANSFER_LIMIT);
+ 
+ 		/* Set up to receive directly into Target(!) address */
+-		ret = __ath10k_ce_rx_post_buf(ce_diag, &address, address);
++		ret = ce_diag->ops->ce_rx_post_buf(ce_diag, &address, address);
+ 		if (ret != 0)
+ 			goto done;
+ 
+@@ -1368,8 +1383,8 @@ int ath10k_pci_hif_tx_sg(struct ath10k *
+ 
+ 	for (i = 0; i < n_items - 1; i++) {
+ 		ath10k_dbg(ar, ATH10K_DBG_PCI,
+-			   "pci tx item %d paddr 0x%08x len %d n_items %d\n",
+-			   i, items[i].paddr, items[i].len, n_items);
++			   "pci tx item %d paddr %pad len %d n_items %d\n",
++			   i, &items[i].paddr, items[i].len, n_items);
+ 		ath10k_dbg_dump(ar, ATH10K_DBG_PCI_DUMP, NULL, "pci tx data: ",
+ 				items[i].vaddr, items[i].len);
+ 
+@@ -1386,8 +1401,8 @@ int ath10k_pci_hif_tx_sg(struct ath10k *
+ 	/* `i` is equal to `n_items -1` after for() */
+ 
+ 	ath10k_dbg(ar, ATH10K_DBG_PCI,
+-		   "pci tx item %d paddr 0x%08x len %d n_items %d\n",
+-		   i, items[i].paddr, items[i].len, n_items);
++		   "pci tx item %d paddr %pad len %d n_items %d\n",
++		   i, &items[i].paddr, items[i].len, n_items);
+ 	ath10k_dbg_dump(ar, ATH10K_DBG_PCI_DUMP, NULL, "pci tx data: ",
+ 			items[i].vaddr, items[i].len);
+ 
+@@ -1460,6 +1475,271 @@ static void ath10k_pci_dump_registers(st
+ 		crash_data->registers[i] = reg_dump_values[i];
+ }
+ 
++static int ath10k_pci_dump_memory_section(struct ath10k *ar,
++					  const struct ath10k_mem_region *mem_region,
++					  u8 *buf, size_t buf_len)
++{
++	const struct ath10k_mem_section *cur_section, *next_section;
++	unsigned int count, section_size, skip_size;
++	int ret, i, j;
++
++	if (!mem_region || !buf)
++		return 0;
++
++	cur_section = &mem_region->section_table.sections[0];
++
++	if (mem_region->start > cur_section->start) {
++		ath10k_warn(ar, "incorrect memdump region 0x%x with section start address 0x%x.\n",
++			    mem_region->start, cur_section->start);
++		return 0;
++	}
++
++	skip_size = cur_section->start - mem_region->start;
++
++	/* fill the gap between the first register section and register
++	 * start address
++	 */
++	for (i = 0; i < skip_size; i++) {
++		*buf = ATH10K_MAGIC_NOT_COPIED;
++		buf++;
++	}
++
++	count = 0;
++
++	for (i = 0; cur_section != NULL; i++) {
++		section_size = cur_section->end - cur_section->start;
++
++		if (section_size <= 0) {
++			ath10k_warn(ar, "incorrect ramdump format with start address 0x%x and stop address 0x%x\n",
++				    cur_section->start,
++				    cur_section->end);
++			break;
++		}
++
++		if ((i + 1) == mem_region->section_table.size) {
++			/* last section */
++			next_section = NULL;
++			skip_size = 0;
++		} else {
++			next_section = cur_section + 1;
++
++			if (cur_section->end > next_section->start) {
++				ath10k_warn(ar, "next ramdump section 0x%x is smaller than current end address 0x%x\n",
++					    next_section->start,
++					    cur_section->end);
++				break;
++			}
++
++			skip_size = next_section->start - cur_section->end;
++		}
++
++		if (buf_len < (skip_size + section_size)) {
++			ath10k_warn(ar, "ramdump buffer is too small: %zu\n", buf_len);
++			break;
++		}
++
++		buf_len -= skip_size + section_size;
++
++		/* read section to dest memory */
++		ret = ath10k_pci_diag_read_mem(ar, cur_section->start,
++					       buf, section_size);
++		if (ret) {
++			ath10k_warn(ar, "failed to read ramdump from section 0x%x: %d\n",
++				    cur_section->start, ret);
++			break;
++		}
++
++		buf += section_size;
++		count += section_size;
++
++		/* fill in the gap between this section and the next */
++		for (j = 0; j < skip_size; j++) {
++			*buf = ATH10K_MAGIC_NOT_COPIED;
++			buf++;
++		}
++
++		count += skip_size;
++
++		if (!next_section)
++			/* this was the last section */
++			break;
++
++		cur_section = next_section;
++	}
++
++	return count;
++}
++
++static int ath10k_pci_set_ram_config(struct ath10k *ar, u32 config)
++{
++	u32 val;
++
++	ath10k_pci_write32(ar, SOC_CORE_BASE_ADDRESS +
++			   FW_RAM_CONFIG_ADDRESS, config);
++
++	val = ath10k_pci_read32(ar, SOC_CORE_BASE_ADDRESS +
++				FW_RAM_CONFIG_ADDRESS);
++	if (val != config) {
++		ath10k_warn(ar, "failed to set RAM config from 0x%x to 0x%x\n",
++			    val, config);
++		return -EIO;
++	}
++
++	return 0;
++}
++
++/* if an error happened returns < 0, otherwise the length */
++static int ath10k_pci_dump_memory_sram(struct ath10k *ar,
++				       const struct ath10k_mem_region *region,
++				       u8 *buf)
++{
++	struct ath10k_pci *ar_pci = ath10k_pci_priv(ar);
++	u32 base_addr, i;
++
++	base_addr = ioread32(ar_pci->mem + QCA99X0_PCIE_BAR0_START_REG);
++	base_addr += region->start;
++
++	for (i = 0; i < region->len; i += 4) {
++		iowrite32(base_addr + i, ar_pci->mem + QCA99X0_CPU_MEM_ADDR_REG);
++		*(u32 *)(buf + i) = ioread32(ar_pci->mem + QCA99X0_CPU_MEM_DATA_REG);
++	}
++
++	return region->len;
++}
++
++/* if an error happened returns < 0, otherwise the length */
++static int ath10k_pci_dump_memory_reg(struct ath10k *ar,
++				      const struct ath10k_mem_region *region,
++				      u8 *buf)
++{
++	struct ath10k_pci *ar_pci = ath10k_pci_priv(ar);
++	u32 i;
++
++	for (i = 0; i < region->len; i += 4)
++		*(u32 *)(buf + i) = ioread32(ar_pci->mem + region->start + i);
++
++	return region->len;
++}
++
++/* if an error happened returns < 0, otherwise the length */
++static int ath10k_pci_dump_memory_generic(struct ath10k *ar,
++					  const struct ath10k_mem_region *current_region,
++					  u8 *buf)
++{
++	int ret;
++
++	if (current_region->section_table.size > 0)
++		/* Copy each section individually. */
++		return ath10k_pci_dump_memory_section(ar,
++						      current_region,
++						      buf,
++						      current_region->len);
++
++	/* No individiual memory sections defined so we can
++	 * copy the entire memory region.
++	 */
++	ret = ath10k_pci_diag_read_mem(ar,
++				       current_region->start,
++				       buf,
++				       current_region->len);
++	if (ret) {
++		ath10k_warn(ar, "failed to copy ramdump region %s: %d\n",
++			    current_region->name, ret);
++		return ret;
++	}
++
++	return current_region->len;
++}
++
++static void ath10k_pci_dump_memory(struct ath10k *ar,
++				   struct ath10k_fw_crash_data *crash_data)
++{
++	const struct ath10k_hw_mem_layout *mem_layout;
++	const struct ath10k_mem_region *current_region;
++	struct ath10k_dump_ram_data_hdr *hdr;
++	u32 count, shift;
++	size_t buf_len;
++	int ret, i;
++	u8 *buf;
++
++	lockdep_assert_held(&ar->data_lock);
++
++	if (!crash_data)
++		return;
++
++	mem_layout = ath10k_coredump_get_mem_layout(ar);
++	if (!mem_layout)
++		return;
++
++	current_region = &mem_layout->region_table.regions[0];
++
++	buf = crash_data->ramdump_buf;
++	buf_len = crash_data->ramdump_buf_len;
++
++	memset(buf, 0, buf_len);
++
++	for (i = 0; i < mem_layout->region_table.size; i++) {
++		count = 0;
++
++		if (current_region->len > buf_len) {
++			ath10k_warn(ar, "memory region %s size %d is larger that remaining ramdump buffer size %zu\n",
++				    current_region->name,
++				    current_region->len,
++				    buf_len);
++			break;
++		}
++
++		/* To get IRAM dump, the host driver needs to switch target
++		 * ram config from DRAM to IRAM.
++		 */
++		if (current_region->type == ATH10K_MEM_REGION_TYPE_IRAM1 ||
++		    current_region->type == ATH10K_MEM_REGION_TYPE_IRAM2) {
++			shift = current_region->start >> 20;
++
++			ret = ath10k_pci_set_ram_config(ar, shift);
++			if (ret) {
++				ath10k_warn(ar, "failed to switch ram config to IRAM for section %s: %d\n",
++					    current_region->name, ret);
++				break;
++			}
++		}
++
++		/* Reserve space for the header. */
++		hdr = (void *)buf;
++		buf += sizeof(*hdr);
++		buf_len -= sizeof(*hdr);
++
++		switch (current_region->type) {
++		case ATH10K_MEM_REGION_TYPE_IOSRAM:
++			count = ath10k_pci_dump_memory_sram(ar, current_region, buf);
++			break;
++		case ATH10K_MEM_REGION_TYPE_IOREG:
++			count = ath10k_pci_dump_memory_reg(ar, current_region, buf);
++			break;
++		default:
++			ret = ath10k_pci_dump_memory_generic(ar, current_region, buf);
++			if (ret < 0)
++				break;
++
++			count = ret;
++			break;
++		}
++
++		hdr->region_type = cpu_to_le32(current_region->type);
++		hdr->start = cpu_to_le32(current_region->start);
++		hdr->length = cpu_to_le32(count);
++
++		if (count == 0)
++			/* Note: the header remains, just with zero length. */
++			break;
++
++		buf += count;
++		buf_len -= count;
++
++		current_region++;
++	}
++}
++
+ static void ath10k_pci_fw_crashed_dump(struct ath10k *ar)
+ {
+ 	struct ath10k_fw_crash_data *crash_data;
+@@ -1469,7 +1749,7 @@ static void ath10k_pci_fw_crashed_dump(s
+ 
+ 	ar->stats.fw_crash_counter++;
+ 
+-	crash_data = ath10k_debug_get_new_fw_crash_data(ar);
++	crash_data = ath10k_coredump_new(ar);
+ 
+ 	if (crash_data)
+ 		scnprintf(guid, sizeof(guid), "%pUl", &crash_data->guid);
+@@ -1480,6 +1760,7 @@ static void ath10k_pci_fw_crashed_dump(s
+ 	ath10k_print_driver_info(ar);
+ 	ath10k_pci_dump_registers(ar, crash_data);
+ 	ath10k_ce_dump_registers(ar, crash_data);
++	ath10k_pci_dump_memory(ar, crash_data);
+ 
+ 	spin_unlock_bh(&ar->data_lock);
+ 
+@@ -1857,7 +2138,7 @@ int ath10k_pci_hif_exchange_bmi_msg(stru
+ 
+ 	ret = ath10k_pci_bmi_wait(ar, ce_tx, ce_rx, &xfer);
+ 	if (ret) {
+-		u32 unused_buffer;
++		dma_addr_t unused_buffer;
+ 		unsigned int unused_nbytes;
+ 		unsigned int unused_id;
+ 
+@@ -1870,7 +2151,7 @@ int ath10k_pci_hif_exchange_bmi_msg(stru
+ 
+ err_resp:
+ 	if (resp) {
+-		u32 unused_buffer;
++		dma_addr_t unused_buffer;
+ 
+ 		ath10k_ce_revoke_recv_next(ce_rx, NULL, &unused_buffer);
+ 		dma_unmap_single(ar->dev, resp_paddr,
+@@ -1976,6 +2257,7 @@ static int ath10k_pci_get_num_banks(stru
+ 	struct ath10k_pci *ar_pci = ath10k_pci_priv(ar);
+ 
+ 	switch (ar_pci->pdev->device) {
++	case QCA988X_2_0_DEVICE_ID_UBNT:
+ 	case QCA988X_2_0_DEVICE_ID:
+ 	case QCA99X0_2_0_DEVICE_ID:
+ 	case QCA9888_2_0_DEVICE_ID:
+@@ -1999,7 +2281,7 @@ static int ath10k_pci_get_num_banks(stru
+ 		}
+ 		break;
+ 	case QCA9377_1_0_DEVICE_ID:
+-		return 4;
++		return 9;
+ 	}
+ 
+ 	ath10k_warn(ar, "unknown number of banks, assuming 1\n");
+@@ -3164,8 +3446,7 @@ int ath10k_pci_setup_resource(struct ath
+ 	spin_lock_init(&ce->ce_lock);
+ 	spin_lock_init(&ar_pci->ps_lock);
+ 
+-	setup_timer(&ar_pci->rx_post_retry, ath10k_pci_rx_replenish_retry,
+-		    (unsigned long)ar);
++	timer_setup(&ar_pci->rx_post_retry, ath10k_pci_rx_replenish_retry, 0);
+ 
+ 	if (QCA_REV_6174(ar) || QCA_REV_9377(ar))
+ 		ath10k_pci_override_ce_config(ar);
+@@ -3201,13 +3482,14 @@ static int ath10k_pci_probe(struct pci_d
+ 	struct ath10k *ar;
+ 	struct ath10k_pci *ar_pci;
+ 	enum ath10k_hw_rev hw_rev;
+-	u32 chip_id;
++	struct ath10k_bus_params bus_params;
+ 	bool pci_ps;
+ 	int (*pci_soft_reset)(struct ath10k *ar);
+ 	int (*pci_hard_reset)(struct ath10k *ar);
+ 	u32 (*targ_cpu_to_ce_addr)(struct ath10k *ar, u32 addr);
+ 
+ 	switch (pci_dev->device) {
++	case QCA988X_2_0_DEVICE_ID_UBNT:
+ 	case QCA988X_2_0_DEVICE_ID:
+ 		hw_rev = ATH10K_HW_QCA988X;
+ 		pci_ps = false;
+@@ -3291,8 +3573,7 @@ static int ath10k_pci_probe(struct pci_d
+ 	ar->id.subsystem_vendor = pdev->subsystem_vendor;
+ 	ar->id.subsystem_device = pdev->subsystem_device;
+ 
+-	setup_timer(&ar_pci->ps_timer, ath10k_pci_ps_timer,
+-		    (unsigned long)ar);
++	timer_setup(&ar_pci->ps_timer, ath10k_pci_ps_timer, 0);
+ 
+ 	ret = ath10k_pci_setup_resource(ar);
+ 	if (ret) {
+@@ -3337,19 +3618,20 @@ static int ath10k_pci_probe(struct pci_d
+ 		goto err_free_irq;
+ 	}
+ 
+-	chip_id = ath10k_pci_soc_read32(ar, SOC_CHIP_ID_ADDRESS);
+-	if (chip_id == 0xffffffff) {
++	bus_params.is_high_latency = false;
++	bus_params.chip_id = ath10k_pci_soc_read32(ar, SOC_CHIP_ID_ADDRESS);
++	if (bus_params.chip_id == 0xffffffff) {
+ 		ath10k_err(ar, "failed to get chip id\n");
+ 		goto err_free_irq;
+ 	}
+ 
+-	if (!ath10k_pci_chip_is_supported(pdev->device, chip_id)) {
++	if (!ath10k_pci_chip_is_supported(pdev->device, bus_params.chip_id)) {
+ 		ath10k_err(ar, "device %04x with chip_id %08x isn't supported\n",
+-			   pdev->device, chip_id);
++			   pdev->device, bus_params.chip_id);
+ 		goto err_free_irq;
+ 	}
+ 
+-	ret = ath10k_core_register(ar, chip_id);
++	ret = ath10k_core_register(ar, &bus_params);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to register driver core: %d\n", ret);
+ 		goto err_free_irq;
+@@ -3497,5 +3779,6 @@ MODULE_FIRMWARE(QCA6174_HW_3_0_FW_DIR "/
+ MODULE_FIRMWARE(QCA6174_HW_3_0_FW_DIR "/" ATH10K_BOARD_API2_FILE);
+ 
+ /* QCA9377 1.0 firmware files */
++MODULE_FIRMWARE(QCA9377_HW_1_0_FW_DIR "/" ATH10K_FW_API6_FILE);
+ MODULE_FIRMWARE(QCA9377_HW_1_0_FW_DIR "/" ATH10K_FW_API5_FILE);
+ MODULE_FIRMWARE(QCA9377_HW_1_0_FW_DIR "/" QCA9377_HW_1_0_BOARD_DATA_FILE);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/pci.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/pci.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/pci.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -278,7 +278,7 @@ void ath10k_pci_hif_power_down(struct at
+ int ath10k_pci_alloc_pipes(struct ath10k *ar);
+ void ath10k_pci_free_pipes(struct ath10k *ar);
+ void ath10k_pci_free_pipes(struct ath10k *ar);
+-void ath10k_pci_rx_replenish_retry(unsigned long ptr);
++void ath10k_pci_rx_replenish_retry(struct timer_list *t);
+ void ath10k_pci_ce_deinit(struct ath10k *ar);
+ void ath10k_pci_init_napi(struct ath10k *ar);
+ int ath10k_pci_init_pipes(struct ath10k *ar);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/rx_desc.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/rx_desc.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/rx_desc.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -210,6 +210,10 @@ struct rx_frag_info {
+ 	u8 ring1_more_count;
+ 	u8 ring2_more_count;
+ 	u8 ring3_more_count;
++	u8 ring4_more_count;
++	u8 ring5_more_count;
++	u8 ring6_more_count;
++	u8 ring7_more_count;
+ } __packed;
+ 
+ /*
+@@ -471,10 +475,16 @@ struct rx_msdu_start_qca99x0 {
+ 	__le32 info2; /* %RX_MSDU_START_INFO2_ */
+ } __packed;
+ 
++struct rx_msdu_start_wcn3990 {
++	__le32 info2; /* %RX_MSDU_START_INFO2_ */
++	__le32 info3; /* %RX_MSDU_START_INFO3_ */
++} __packed;
++
+ struct rx_msdu_start {
+ 	struct rx_msdu_start_common common;
+ 	union {
+ 		struct rx_msdu_start_qca99x0 qca99x0;
++		struct rx_msdu_start_wcn3990 wcn3990;
+ 	} __packed;
+ } __packed;
+ 
+@@ -595,10 +605,23 @@ struct rx_msdu_end_qca99x0 {
+ 	__le32 info2;
+ } __packed;
+ 
++struct rx_msdu_end_wcn3990 {
++	__le32 ipv6_crc;
++	__le32 tcp_seq_no;
++	__le32 tcp_ack_no;
++	__le32 info1;
++	__le32 info2;
++	__le32 rule_indication_0;
++	__le32 rule_indication_1;
++	__le32 rule_indication_2;
++	__le32 rule_indication_3;
++} __packed;
++
+ struct rx_msdu_end {
+ 	struct rx_msdu_end_common common;
+ 	union {
+ 		struct rx_msdu_end_qca99x0 qca99x0;
++		struct rx_msdu_end_wcn3990 wcn3990;
+ 	} __packed;
+ } __packed;
+ 
+@@ -963,6 +986,12 @@ struct rx_pkt_end {
+ 	__le32 phy_timestamp_2;
+ } __packed;
+ 
++struct rx_pkt_end_wcn3990 {
++	__le32 info0; /* %RX_PKT_END_INFO0_ */
++	__le64 phy_timestamp_1;
++	__le64 phy_timestamp_2;
++} __packed;
++
+ #define RX_LOCATION_INFO0_RTT_FAC_LEGACY_MASK		0x00003fff
+ #define RX_LOCATION_INFO0_RTT_FAC_LEGACY_LSB		0
+ #define RX_LOCATION_INFO0_RTT_FAC_VHT_MASK		0x1fff8000
+@@ -998,6 +1027,12 @@ struct rx_location_info {
+ 	__le32 rx_location_info1; /* %RX_LOCATION_INFO1_ */
+ } __packed;
+ 
++struct rx_location_info_wcn3990 {
++	__le32 rx_location_info0; /* %RX_LOCATION_INFO0_ */
++	__le32 rx_location_info1; /* %RX_LOCATION_INFO1_ */
++	__le32 rx_location_info2; /* %RX_LOCATION_INFO2_ */
++} __packed;
++
+ enum rx_phy_ppdu_end_info0 {
+ 	RX_PHY_PPDU_END_INFO0_ERR_RADAR           = BIT(2),
+ 	RX_PHY_PPDU_END_INFO0_ERR_RX_ABORT        = BIT(3),
+@@ -1086,6 +1121,20 @@ struct rx_ppdu_end_qca9984 {
+ 	__le16 info1; /* %RX_PPDU_END_INFO1_ */
+ } __packed;
+ 
++struct rx_ppdu_end_wcn3990 {
++	struct rx_pkt_end_wcn3990 rx_pkt_end;
++	struct rx_location_info_wcn3990 rx_location_info;
++	struct rx_phy_ppdu_end rx_phy_ppdu_end;
++	__le32 rx_timing_offset;
++	__le32 reserved_info_0;
++	__le32 reserved_info_1;
++	__le32 rx_antenna_info;
++	__le32 rx_coex_info;
++	__le32 rx_mpdu_cnt_info;
++	__le64 phy_timestamp_tx;
++	__le32 rx_bb_length;
++} __packed;
++
+ struct rx_ppdu_end {
+ 	struct rx_ppdu_end_common common;
+ 	union {
+@@ -1093,6 +1142,7 @@ struct rx_ppdu_end {
+ 		struct rx_ppdu_end_qca6174 qca6174;
+ 		struct rx_ppdu_end_qca99x0 qca99x0;
+ 		struct rx_ppdu_end_qca9984 qca9984;
++		struct rx_ppdu_end_wcn3990 wcn3990;
+ 	} __packed;
+ } __packed;
+ 
+@@ -1225,4 +1275,19 @@ struct fw_rx_desc_base {
+ 	u8 info0;
+ } __packed;
+ 
++#define FW_RX_DESC_FLAGS_FIRST_MSDU (1 << 0)
++#define FW_RX_DESC_FLAGS_LAST_MSDU  (1 << 1)
++#define FW_RX_DESC_C3_FAILED        (1 << 2)
++#define FW_RX_DESC_C4_FAILED        (1 << 3)
++#define FW_RX_DESC_IPV6             (1 << 4)
++#define FW_RX_DESC_TCP              (1 << 5)
++#define FW_RX_DESC_UDP              (1 << 6)
++
++struct fw_rx_desc_hl {
++	u8 info0;
++	u8 version;
++	u8 len;
++	u8 flags;
++} __packed;
++
+ #endif /* _RX_DESC_H_ */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/sdio.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/sdio.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/sdio.c
+@@ -34,8 +34,26 @@
+ #include "trace.h"
+ #include "sdio.h"
+ 
++#define ATH10K_SDIO_DMA_BUF_SIZE	(32 * 1024)
++#define ATH10K_SDIO_VSG_BUF_SIZE	(32 * 1024)
++
++static int ath10k_sdio_read(struct ath10k *ar, u32 addr, void *buf,
++			    size_t len, bool incr);
++static int ath10k_sdio_write(struct ath10k *ar, u32 addr, const void *buf,
++			     size_t len, bool incr);
++
+ /* inlined helper functions */
+ 
++/* Macro to check if DMA buffer is WORD-aligned and DMA-able.
++ * Most host controllers assume the buffer is DMA'able and will
++ * bug-check otherwise (i.e. buffers on the stack). virt_addr_valid
++ * check fails on stack memory.
++ */
++static inline bool buf_needs_bounce(const u8 *buf)
++{
++	return ((unsigned long)buf & 0x3) || !virt_addr_valid(buf);
++}
++
+ static inline int ath10k_sdio_calc_txrx_padded_len(struct ath10k_sdio *ar_sdio,
+ 						   size_t len)
+ {
+@@ -140,6 +158,7 @@ static int ath10k_sdio_config(struct ath
+ 	struct sdio_func *func = ar_sdio->func;
+ 	unsigned char byte, asyncintdelay = 2;
+ 	int ret;
++	u32 addr;
+ 
+ 	ath10k_dbg(ar, ATH10K_DBG_BOOT, "sdio configuration\n");
+ 
+@@ -168,9 +187,8 @@ static int ath10k_sdio_config(struct ath
+ 		 CCCR_SDIO_DRIVER_STRENGTH_ENABLE_C |
+ 		 CCCR_SDIO_DRIVER_STRENGTH_ENABLE_D);
+ 
+-	ret = ath10k_sdio_func0_cmd52_wr_byte(func->card,
+-					      CCCR_SDIO_DRIVER_STRENGTH_ENABLE_ADDR,
+-					      byte);
++	addr = CCCR_SDIO_DRIVER_STRENGTH_ENABLE_ADDR,
++	ret = ath10k_sdio_func0_cmd52_wr_byte(func->card, addr, byte);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to enable driver strength: %d\n", ret);
+ 		goto out;
+@@ -221,13 +239,16 @@ out:
+ 
+ static int ath10k_sdio_write32(struct ath10k *ar, u32 addr, u32 val)
+ {
+-	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+-	struct sdio_func *func = ar_sdio->func;
++	__le32 *buf;
+ 	int ret;
+ 
+-	sdio_claim_host(func);
++	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
++	if (!buf)
++		return -ENOMEM;
++
++	*buf = cpu_to_le32(val);
+ 
+-	sdio_writel(func, val, addr, &ret);
++	ret = ath10k_sdio_write(ar, addr, &val, sizeof(val), true);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to write 0x%x to address 0x%x: %d\n",
+ 			    val, addr, ret);
+@@ -238,15 +259,13 @@ static int ath10k_sdio_write32(struct at
+ 		   addr, val);
+ 
+ out:
+-	sdio_release_host(func);
++	kfree(buf);
+ 
+ 	return ret;
+ }
+ 
+ static int ath10k_sdio_writesb32(struct ath10k *ar, u32 addr, u32 val)
+ {
+-	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+-	struct sdio_func *func = ar_sdio->func;
+ 	__le32 *buf;
+ 	int ret;
+ 
+@@ -256,9 +275,7 @@ static int ath10k_sdio_writesb32(struct
+ 
+ 	*buf = cpu_to_le32(val);
+ 
+-	sdio_claim_host(func);
+-
+-	ret = sdio_writesb(func, addr, buf, sizeof(*buf));
++	ret = ath10k_sdio_write(ar, addr, buf, sizeof(*buf), false);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to write value 0x%x to fixed sb address 0x%x: %d\n",
+ 			    val, addr, ret);
+@@ -269,8 +286,6 @@ static int ath10k_sdio_writesb32(struct
+ 		   addr, val);
+ 
+ out:
+-	sdio_release_host(func);
+-
+ 	kfree(buf);
+ 
+ 	return ret;
+@@ -278,40 +293,63 @@ out:
+ 
+ static int ath10k_sdio_read32(struct ath10k *ar, u32 addr, u32 *val)
+ {
+-	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+-	struct sdio_func *func = ar_sdio->func;
++	__le32 *buf;
+ 	int ret;
+ 
+-	sdio_claim_host(func);
+-	*val = sdio_readl(func, addr, &ret);
++	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
++	if (!buf)
++		return -ENOMEM;
++
++	ret = ath10k_sdio_read(ar, addr, buf, sizeof(*val), true);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to read from address 0x%x: %d\n",
+ 			    addr, ret);
+ 		goto out;
+ 	}
+ 
++	*val = le32_to_cpu(*buf);
++
+ 	ath10k_dbg(ar, ATH10K_DBG_SDIO, "sdio read32 addr 0x%x val 0x%x\n",
+ 		   addr, *val);
+ 
+ out:
+-	sdio_release_host(func);
++	kfree(buf);
+ 
+ 	return ret;
+ }
+ 
+-static int ath10k_sdio_read(struct ath10k *ar, u32 addr, void *buf, size_t len)
++static int ath10k_sdio_read(struct ath10k *ar, u32 addr, void *buf,
++			    size_t len, bool incr)
+ {
+ 	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+ 	struct sdio_func *func = ar_sdio->func;
++	bool bounced = false;
++	u8 *tbuf = NULL;
+ 	int ret;
+ 
++	if (buf_needs_bounce(buf)) {
++		if (!ar_sdio->dma_buffer)
++			return -ENOMEM;
++		mutex_lock(&ar_sdio->dma_buffer_mutex);
++		tbuf = ar_sdio->dma_buffer;
++		bounced = true;
++	} else {
++		tbuf = buf;
++	}
++
+ 	sdio_claim_host(func);
++	if (incr)
++		ret = sdio_memcpy_fromio(func, tbuf, addr, len);
++	else
++		ret = sdio_readsb(func, tbuf, addr, len);
++	sdio_release_host(func);
+ 
+-	ret = sdio_memcpy_fromio(func, buf, addr, len);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to read from address 0x%x: %d\n",
+ 			    addr, ret);
+ 		goto out;
++	} else if (bounced) {
++		memcpy(buf, tbuf, len);
+ 	}
+ 
+ 	ath10k_dbg(ar, ATH10K_DBG_SDIO, "sdio read addr 0x%x buf 0x%p len %zu\n",
+@@ -319,23 +357,44 @@ static int ath10k_sdio_read(struct ath10
+ 	ath10k_dbg_dump(ar, ATH10K_DBG_SDIO_DUMP, NULL, "sdio read ", buf, len);
+ 
+ out:
+-	sdio_release_host(func);
++	if (bounced)
++		mutex_unlock(&ar_sdio->dma_buffer_mutex);
+ 
+ 	return ret;
+ }
+ 
+-static int ath10k_sdio_write(struct ath10k *ar, u32 addr, const void *buf, size_t len)
++static int ath10k_sdio_write(struct ath10k *ar, u32 addr, const void *buf,
++			     size_t len, bool incr)
+ {
+ 	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+ 	struct sdio_func *func = ar_sdio->func;
++	bool bounced = false;
++	u8 *tbuf = NULL;
+ 	int ret;
+ 
++	if (buf_needs_bounce(buf)) {
++		if (!ar_sdio->dma_buffer)
++			return -ENOMEM;
++		mutex_lock(&ar_sdio->dma_buffer_mutex);
++		tbuf = ar_sdio->dma_buffer;
++		memcpy(tbuf, buf, len);
++		bounced = true;
++	} else {
++		tbuf = (u8 *)buf;
++	}
++
++	if (addr == ar_sdio->mbox_info.htc_addr)
++		addr += (ATH10K_HIF_MBOX_WIDTH - len);
++
+ 	sdio_claim_host(func);
+ 
+ 	/* For some reason toio() doesn't have const for the buffer, need
+ 	 * an ugly hack to workaround that.
+ 	 */
+-	ret = sdio_memcpy_toio(func, addr, (void *)buf, len);
++	if (incr)
++		ret = sdio_memcpy_toio(func, addr, (void *)tbuf, len);
++	else
++		ret = sdio_writesb(func, addr, tbuf, len);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to write to address 0x%x: %d\n",
+ 			    addr, ret);
+@@ -349,32 +408,8 @@ static int ath10k_sdio_write(struct ath1
+ out:
+ 	sdio_release_host(func);
+ 
+-	return ret;
+-}
+-
+-static int ath10k_sdio_readsb(struct ath10k *ar, u32 addr, void *buf, size_t len)
+-{
+-	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+-	struct sdio_func *func = ar_sdio->func;
+-	int ret;
+-
+-	sdio_claim_host(func);
+-
+-	len = round_down(len, ar_sdio->mbox_info.block_size);
+-
+-	ret = sdio_readsb(func, buf, addr, len);
+-	if (ret) {
+-		ath10k_warn(ar, "failed to read from fixed (sb) address 0x%x: %d\n",
+-			    addr, ret);
+-		goto out;
+-	}
+-
+-	ath10k_dbg(ar, ATH10K_DBG_SDIO, "sdio readsb addr 0x%x buf 0x%p len %zu\n",
+-		   addr, buf, len);
+-	ath10k_dbg_dump(ar, ATH10K_DBG_SDIO_DUMP, NULL, "sdio readsb ", buf, len);
+-
+-out:
+-	sdio_release_host(func);
++	if (bounced)
++		mutex_unlock(&ar_sdio->dma_buffer_mutex);
+ 
+ 	return ret;
+ }
+@@ -396,6 +431,7 @@ static int ath10k_sdio_mbox_rx_process_p
+ 	int ret;
+ 
+ 	payload_len = le16_to_cpu(htc_hdr->len);
++	skb->len = payload_len + sizeof(struct ath10k_htc_hdr);
+ 
+ 	if (trailer_present) {
+ 		trailer = skb->data + sizeof(*htc_hdr) +
+@@ -434,12 +470,13 @@ static int ath10k_sdio_mbox_rx_process_p
+ 	enum ath10k_htc_ep_id id;
+ 	int ret, i, *n_lookahead_local;
+ 	u32 *lookaheads_local;
++	int lookahd_idx = 0;
+ 
+ 	for (i = 0; i < ar_sdio->n_rx_pkts; i++) {
+ 		lookaheads_local = lookaheads;
+ 		n_lookahead_local = n_lookahead;
+ 
+-		id = ((struct ath10k_htc_hdr *)&lookaheads[i])->eid;
++		id = ((struct ath10k_htc_hdr *)&lookaheads[lookahd_idx++])->eid;
+ 
+ 		if (id >= ATH10K_HTC_EP_COUNT) {
+ 			ath10k_warn(ar, "invalid endpoint in look-ahead: %d\n",
+@@ -462,6 +499,7 @@ static int ath10k_sdio_mbox_rx_process_p
+ 			/* Only read lookahead's from RX trailers
+ 			 * for the last packet in a bundle.
+ 			 */
++			lookahd_idx--;
+ 			lookaheads_local = NULL;
+ 			n_lookahead_local = NULL;
+ 		}
+@@ -495,11 +533,11 @@ out:
+ 	return ret;
+ }
+ 
+-static int ath10k_sdio_mbox_alloc_pkt_bundle(struct ath10k *ar,
+-					     struct ath10k_sdio_rx_data *rx_pkts,
+-					     struct ath10k_htc_hdr *htc_hdr,
+-					     size_t full_len, size_t act_len,
+-					     size_t *bndl_cnt)
++static int ath10k_sdio_mbox_alloc_bundle(struct ath10k *ar,
++					 struct ath10k_sdio_rx_data *rx_pkts,
++					 struct ath10k_htc_hdr *htc_hdr,
++					 size_t full_len, size_t act_len,
++					 size_t *bndl_cnt)
+ {
+ 	int ret, i;
+ 
+@@ -540,6 +578,7 @@ static int ath10k_sdio_mbox_rx_alloc(str
+ 	size_t full_len, act_len;
+ 	bool last_in_bundle;
+ 	int ret, i;
++	int pkt_cnt = 0;
+ 
+ 	if (n_lookaheads > ATH10K_SDIO_MAX_RX_MSGS) {
+ 		ath10k_warn(ar,
+@@ -582,16 +621,22 @@ static int ath10k_sdio_mbox_rx_alloc(str
+ 			 * optimally fetched as a full bundle.
+ 			 */
+ 			size_t bndl_cnt;
++			struct ath10k_sdio_rx_data *rx_pkts =
++				&ar_sdio->rx_pkts[pkt_cnt];
+ 
+-			ret = ath10k_sdio_mbox_alloc_pkt_bundle(ar,
+-								&ar_sdio->rx_pkts[i],
+-								htc_hdr,
+-								full_len,
+-								act_len,
+-								&bndl_cnt);
++			ret = ath10k_sdio_mbox_alloc_bundle(ar,
++							    rx_pkts,
++							    htc_hdr,
++							    full_len,
++							    act_len,
++							    &bndl_cnt);
++
++			if (ret) {
++				ath10k_warn(ar, "alloc_bundle error %d\n", ret);
++				goto err;
++			}
+ 
+-			n_lookaheads += bndl_cnt;
+-			i += bndl_cnt;
++			pkt_cnt += bndl_cnt;
+ 			/*Next buffer will be the last in the bundle */
+ 			last_in_bundle = true;
+ 		}
+@@ -600,14 +645,18 @@ static int ath10k_sdio_mbox_rx_alloc(str
+ 		 * ATH10K_HTC_FLAG_BUNDLE_MASK flag set, all bundled
+ 		 * packet skb's have been allocated in the previous step.
+ 		 */
+-		ret = ath10k_sdio_mbox_alloc_rx_pkt(&ar_sdio->rx_pkts[i],
++		if (htc_hdr->flags & ATH10K_HTC_FLAGS_RECV_1MORE_BLOCK)
++			full_len += ATH10K_HIF_MBOX_BLOCK_SIZE;
++
++		ret = ath10k_sdio_mbox_alloc_rx_pkt(&ar_sdio->rx_pkts[pkt_cnt],
+ 						    act_len,
+ 						    full_len,
+ 						    last_in_bundle,
+ 						    last_in_bundle);
++		pkt_cnt++;
+ 	}
+ 
+-	ar_sdio->n_rx_pkts = i;
++	ar_sdio->n_rx_pkts = pkt_cnt;
+ 
+ 	return 0;
+ 
+@@ -621,41 +670,71 @@ err:
+ 	return ret;
+ }
+ 
+-static int ath10k_sdio_mbox_rx_packet(struct ath10k *ar,
+-				      struct ath10k_sdio_rx_data *pkt)
++static int ath10k_sdio_mbox_rx_fetch(struct ath10k *ar)
+ {
+ 	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
+-	struct sk_buff *skb = pkt->skb;
++	struct ath10k_sdio_rx_data *pkt = &ar_sdio->rx_pkts[0];
++	struct sk_buff *skb;
+ 	int ret;
+ 
+-	ret = ath10k_sdio_readsb(ar, ar_sdio->mbox_info.htc_addr,
+-				 skb->data, pkt->alloc_len);
+-	pkt->status = ret;
+-	if (!ret)
++	skb = pkt->skb;
++	ret = ath10k_sdio_read(ar, ar_sdio->mbox_info.htc_addr,
++			       skb->data, pkt->alloc_len, false);
++	if (ret) {
++		ar_sdio->n_rx_pkts = 0;
++		ath10k_sdio_mbox_free_rx_pkt(pkt);
++	} else {
++		pkt->status = ret;
+ 		skb_put(skb, pkt->act_len);
++	}
+ 
+ 	return ret;
+ }
+ 
+-static int ath10k_sdio_mbox_rx_fetch(struct ath10k *ar)
++static int ath10k_sdio_mbox_rx_fetch_bundle(struct ath10k *ar)
+ {
+ 	struct ath10k_sdio *ar_sdio = ath10k_sdio_priv(ar);
++	struct ath10k_sdio_rx_data *pkt;
+ 	int ret, i;
++	u32 pkt_offset, virt_pkt_len;
+ 
++	virt_pkt_len = 0;
+ 	for (i = 0; i < ar_sdio->n_rx_pkts; i++) {
+-		ret = ath10k_sdio_mbox_rx_packet(ar,
+-						 &ar_sdio->rx_pkts[i]);
+-		if (ret)
++		virt_pkt_len += ar_sdio->rx_pkts[i].alloc_len;
++	}
++	if (virt_pkt_len < ATH10K_SDIO_DMA_BUF_SIZE) {
++		ret = ath10k_sdio_read(ar, ar_sdio->mbox_info.htc_addr,
++				       ar_sdio->vsg_buffer, virt_pkt_len,
++				       false);
++		if (ret) {
++			i = 0;
+ 			goto err;
++		}
++	} else {
++		ath10k_err(ar, "size exceeding limit %d\n", virt_pkt_len);
++	}
++
++	pkt_offset = 0;
++	for (i = 0; i < ar_sdio->n_rx_pkts; i++) {
++		struct sk_buff *skb = ar_sdio->rx_pkts[i].skb;
++
++		pkt = &ar_sdio->rx_pkts[i];
++		memcpy(skb->data, ar_sdio->vsg_buffer + pkt_offset,
++		       pkt->alloc_len);
++		pkt->status = 0;
++		skb_put(skb, pkt->act_len);
++		pkt_offset += pkt->alloc_len;
+ 	}
+ 
+ 	return 0;
+ 
+ err:
+ 	/* Free all packets that was not successfully fetched. */
+-	for (; i < ar_sdio->n_rx_pkts; i++)
++	for (i = 0; i < ar_sdio->n_rx_pkts; i++)
+ 		ath10k_sdio_mbox_free_rx_pkt(&ar_sdio->rx_pkts[i]);
+ 
++	ar_sdio->n_rx_pkts = 0;
++
+ 	return ret;
+ }
+ 
+@@ -698,7 +777,10 @@ static int ath10k_sdio_mbox_rxmsg_pendin
+ 			 */
+ 			*done = false;
+ 
+-		ret = ath10k_sdio_mbox_rx_fetch(ar);
++		if (ar_sdio->n_rx_pkts > 1)
++			ret = ath10k_sdio_mbox_rx_fetch_bundle(ar);
++		else
++			ret = ath10k_sdio_mbox_rx_fetch(ar);
+ 
+ 		/* Process fetched packets. This will potentially update
+ 		 * n_lookaheads depending on if the packets contain lookahead
+@@ -889,7 +971,7 @@ static int ath10k_sdio_mbox_read_int_sta
+ 	 * registers and the lookahead registers.
+ 	 */
+ 	ret = ath10k_sdio_read(ar, MBOX_HOST_INT_STATUS_ADDRESS,
+-			       irq_proc_reg, sizeof(*irq_proc_reg));
++			       irq_proc_reg, sizeof(*irq_proc_reg), true);
+ 	if (ret)
+ 		goto out;
+ 
+@@ -1102,7 +1184,7 @@ static int ath10k_sdio_bmi_get_rx_lookah
+ 					 MBOX_HOST_INT_STATUS_ADDRESS,
+ 					 &rx_word);
+ 		if (ret) {
+-			ath10k_warn(ar, "unable to read RX_LOOKAHEAD_VALID: %d\n", ret);
++			ath10k_warn(ar, "unable to read rx_lookahd: %d\n", ret);
+ 			return ret;
+ 		}
+ 
+@@ -1134,7 +1216,8 @@ static int ath10k_sdio_bmi_exchange_msg(
+ 		addr = ar_sdio->mbox_info.htc_addr;
+ 
+ 		memcpy(ar_sdio->bmi_buf, req, req_len);
+-		ret = ath10k_sdio_write(ar, addr, ar_sdio->bmi_buf, req_len);
++		ret = ath10k_sdio_write(ar, addr, ar_sdio->bmi_buf, req_len,
++					true);
+ 		if (ret) {
+ 			ath10k_warn(ar,
+ 				    "unable to send the bmi data to the device: %d\n",
+@@ -1198,7 +1281,7 @@ static int ath10k_sdio_bmi_exchange_msg(
+ 
+ 	/* We always read from the start of the mbox address */
+ 	addr = ar_sdio->mbox_info.htc_addr;
+-	ret = ath10k_sdio_read(ar, addr, ar_sdio->bmi_buf, *resp_len);
++	ret = ath10k_sdio_read(ar, addr, ar_sdio->bmi_buf, *resp_len, true);
+ 	if (ret) {
+ 		ath10k_warn(ar,
+ 			    "unable to read the bmi data from the device: %d\n",
+@@ -1255,7 +1338,7 @@ static void __ath10k_sdio_write_async(st
+ 	int ret;
+ 
+ 	skb = req->skb;
+-	ret = ath10k_sdio_write(ar, req->address, skb->data, skb->len);
++	ret = ath10k_sdio_write(ar, req->address, skb->data, skb->len, true);
+ 	if (ret)
+ 		ath10k_warn(ar, "failed to write skb to 0x%x asynchronously: %d",
+ 			    req->address, ret);
+@@ -1362,7 +1445,7 @@ static int ath10k_sdio_hif_disable_intrs
+ 
+ 	memset(regs, 0, sizeof(*regs));
+ 	ret = ath10k_sdio_write(ar, MBOX_INT_STATUS_ENABLE_ADDRESS,
+-				&regs->int_status_en, sizeof(*regs));
++				&regs->int_status_en, sizeof(*regs), true);
+ 	if (ret)
+ 		ath10k_warn(ar, "unable to disable sdio interrupts: %d\n", ret);
+ 
+@@ -1445,7 +1528,7 @@ static int ath10k_sdio_hif_tx_sg(struct
+ 		skb = items[i].transfer_context;
+ 		padded_len = ath10k_sdio_calc_txrx_padded_len(ar_sdio,
+ 							      skb->len);
+-		skb_trim(skb, padded_len);
++		skb->len = padded_len;
+ 
+ 		/* Write TX data to the end of the mbox address space */
+ 		address = ar_sdio->mbox_addr[eid] + ar_sdio->mbox_size[eid] -
+@@ -1473,7 +1556,8 @@ static int ath10k_sdio_hif_enable_intrs(
+ 	/* Enable all but CPU interrupts */
+ 	regs->int_status_en = FIELD_PREP(MBOX_INT_STATUS_ENABLE_ERROR_MASK, 1) |
+ 			      FIELD_PREP(MBOX_INT_STATUS_ENABLE_CPU_MASK, 1) |
+-			      FIELD_PREP(MBOX_INT_STATUS_ENABLE_COUNTER_MASK, 1);
++			      FIELD_PREP(MBOX_INT_STATUS_ENABLE_COUNTER_MASK,
++					 1);
+ 
+ 	/* NOTE: There are some cases where HIF can do detection of
+ 	 * pending mbox messages which is disabled now.
+@@ -1497,7 +1581,7 @@ static int ath10k_sdio_hif_enable_intrs(
+ 			   ATH10K_SDIO_TARGET_DEBUG_INTR_MASK);
+ 
+ 	ret = ath10k_sdio_write(ar, MBOX_INT_STATUS_ENABLE_ADDRESS,
+-				&regs->int_status_en, sizeof(*regs));
++				&regs->int_status_en, sizeof(*regs), true);
+ 	if (ret)
+ 		ath10k_warn(ar,
+ 			    "failed to update mbox interrupt status register : %d\n",
+@@ -1512,7 +1596,8 @@ static int ath10k_sdio_hif_set_mbox_slee
+ 	u32 val;
+ 	int ret;
+ 
+-	ret = ath10k_sdio_read32(ar, ATH10K_FIFO_TIMEOUT_AND_CHIP_CONTROL, &val);
++	ret = ath10k_sdio_read32(ar, ATH10K_FIFO_TIMEOUT_AND_CHIP_CONTROL,
++				 &val);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to read fifo/chip control register: %d\n",
+ 			    ret);
+@@ -1524,7 +1609,8 @@ static int ath10k_sdio_hif_set_mbox_slee
+ 	else
+ 		val |= ATH10K_FIFO_TIMEOUT_AND_CHIP_CONTROL_DISABLE_SLEEP_ON;
+ 
+-	ret = ath10k_sdio_write32(ar, ATH10K_FIFO_TIMEOUT_AND_CHIP_CONTROL, val);
++	ret = ath10k_sdio_write32(ar, ATH10K_FIFO_TIMEOUT_AND_CHIP_CONTROL,
++				  val);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to write to FIFO_TIMEOUT_AND_CHIP_CONTROL: %d",
+ 			    ret);
+@@ -1544,12 +1630,14 @@ static int ath10k_sdio_hif_diag_read(str
+ 	/* set window register to start read cycle */
+ 	ret = ath10k_sdio_write32(ar, MBOX_WINDOW_READ_ADDR_ADDRESS, address);
+ 	if (ret) {
+-		ath10k_warn(ar, "failed to set mbox window read address: %d", ret);
++		ath10k_warn(ar, "failed to set mbox window read address: %d",
++			    ret);
+ 		return ret;
+ 	}
+ 
+ 	/* read the data */
+-	ret = ath10k_sdio_read(ar, MBOX_WINDOW_DATA_ADDRESS, buf, buf_len);
++	ret = ath10k_sdio_read(ar, MBOX_WINDOW_DATA_ADDRESS, buf, buf_len,
++			       true);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to read from mbox window data address: %d\n",
+ 			    ret);
+@@ -1587,7 +1675,8 @@ static int ath10k_sdio_hif_diag_write_me
+ 	int ret;
+ 
+ 	/* set write data */
+-	ret = ath10k_sdio_write(ar, MBOX_WINDOW_DATA_ADDRESS, data, nbytes);
++	ret = ath10k_sdio_write(ar, MBOX_WINDOW_DATA_ADDRESS, data, nbytes,
++				true);
+ 	if (ret) {
+ 		ath10k_warn(ar,
+ 			    "failed to write 0x%p to mbox window data address: %d\n",
+@@ -1598,7 +1687,7 @@ static int ath10k_sdio_hif_diag_write_me
+ 	/* set window register, which starts the write cycle */
+ 	ret = ath10k_sdio_write32(ar, MBOX_WINDOW_WRITE_ADDR_ADDRESS, address);
+ 	if (ret) {
+-		ath10k_warn(ar, "failed to set mbox window write address: %d", ret);
++		ath10k_warn(ar, "failed to set mbox window register: %d", ret);
+ 		return ret;
+ 	}
+ 
+@@ -1648,7 +1737,7 @@ static int ath10k_sdio_hif_start(struct
+ 
+ 	ret = ath10k_sdio_hif_diag_read32(ar, addr, &val);
+ 	if (ret) {
+-		ath10k_warn(ar, "unable to read hi_acs_flags address: %d\n", ret);
++		ath10k_warn(ar, "unable to read hi_acs_flags : %d\n", ret);
+ 		return ret;
+ 	}
+ 
+@@ -1931,7 +2020,8 @@ static int ath10k_sdio_probe(struct sdio
+ 	struct ath10k_sdio *ar_sdio;
+ 	struct ath10k *ar;
+ 	enum ath10k_hw_rev hw_rev;
+-	u32 chip_id, dev_id_base;
++	u32 dev_id_base;
++	struct ath10k_bus_params bus_params;
+ 	int ret, i;
+ 
+ 	/* Assumption: All SDIO based chipsets (so far) are QCA6174 based.
+@@ -1957,25 +2047,38 @@ static int ath10k_sdio_probe(struct sdio
+ 	ar_sdio = ath10k_sdio_priv(ar);
+ 
+ 	ar_sdio->irq_data.irq_proc_reg =
+-		kzalloc(sizeof(struct ath10k_sdio_irq_proc_regs),
+-			GFP_KERNEL);
++		devm_kzalloc(ar->dev, sizeof(struct ath10k_sdio_irq_proc_regs),
++			     GFP_KERNEL);
+ 	if (!ar_sdio->irq_data.irq_proc_reg) {
+ 		ret = -ENOMEM;
+ 		goto err_core_destroy;
+ 	}
+ 
+ 	ar_sdio->irq_data.irq_en_reg =
+-		kzalloc(sizeof(struct ath10k_sdio_irq_enable_regs),
+-			GFP_KERNEL);
++		devm_kzalloc(ar->dev, sizeof(struct ath10k_sdio_irq_enable_regs),
++			     GFP_KERNEL);
+ 	if (!ar_sdio->irq_data.irq_en_reg) {
+ 		ret = -ENOMEM;
+-		goto err_free_proc_reg;
++		goto err_core_destroy;
+ 	}
+ 
+-	ar_sdio->bmi_buf = kzalloc(BMI_MAX_CMDBUF_SIZE, GFP_KERNEL);
++	ar_sdio->bmi_buf = devm_kzalloc(ar->dev, BMI_MAX_CMDBUF_SIZE, GFP_KERNEL);
+ 	if (!ar_sdio->bmi_buf) {
+ 		ret = -ENOMEM;
+-		goto err_free_en_reg;
++		goto err_core_destroy;
++	}
++
++	ar_sdio->dma_buffer = devm_kzalloc(ar->dev, ATH10K_SDIO_DMA_BUF_SIZE,
++					   GFP_KERNEL);
++	if (!ar_sdio->dma_buffer) {
++		ret = -ENOMEM;
++		goto err_core_destroy;
++	}
++
++	ar_sdio->vsg_buffer = devm_kzalloc(ar->dev, ATH10K_SDIO_VSG_BUF_SIZE, GFP_KERNEL);
++	if (!ar_sdio->vsg_buffer) {
++		ret = -ENOMEM;
++		goto err_core_destroy;
+ 	}
+ 
+ 	ar_sdio->func = func;
+@@ -1987,6 +2090,7 @@ static int ath10k_sdio_probe(struct sdio
+ 	spin_lock_init(&ar_sdio->lock);
+ 	spin_lock_init(&ar_sdio->wr_async_lock);
+ 	mutex_init(&ar_sdio->irq_data.mtx);
++	mutex_init(&ar_sdio->dma_buffer_mutex);
+ 
+ 	INIT_LIST_HEAD(&ar_sdio->bus_req_freeq);
+ 	INIT_LIST_HEAD(&ar_sdio->wr_asyncq);
+@@ -1995,7 +2099,7 @@ static int ath10k_sdio_probe(struct sdio
+ 	ar_sdio->workqueue = create_singlethread_workqueue("ath10k_sdio_wq");
+ 	if (!ar_sdio->workqueue) {
+ 		ret = -ENOMEM;
+-		goto err_free_bmi_buf;
++		goto err_core_destroy;
+ 	}
+ 
+ 	for (i = 0; i < ATH10K_SDIO_BUS_REQUEST_MAX_NUM; i++)
+@@ -2011,7 +2115,7 @@ static int ath10k_sdio_probe(struct sdio
+ 		ret = -ENODEV;
+ 		ath10k_err(ar, "unsupported device id %u (0x%x)\n",
+ 			   dev_id_base, id->device);
+-		goto err_free_bmi_buf;
++		goto err_core_destroy;
+ 	}
+ 
+ 	ar->id.vendor = id->vendor;
+@@ -2025,27 +2129,22 @@ static int ath10k_sdio_probe(struct sdio
+ 		goto err_free_wq;
+ 	}
+ 
++	bus_params.is_high_latency = true;
+ 	/* TODO: don't know yet how to get chip_id with SDIO */
+-	chip_id = 0;
+-	ret = ath10k_core_register(ar, chip_id);
++	bus_params.chip_id = 0;
++	ret = ath10k_core_register(ar, &bus_params);
+ 	if (ret) {
+ 		ath10k_err(ar, "failed to register driver core: %d\n", ret);
+ 		goto err_free_wq;
+ 	}
+ 
+ 	/* TODO: remove this once SDIO support is fully implemented */
+-	ath10k_warn(ar, "WARNING: ath10k SDIO support is incomplete, don't expect anything to work!\n");
++	ath10k_warn(ar, "WARNING: ath10k SDIO support is experimental\n");
+ 
+ 	return 0;
+ 
+ err_free_wq:
+ 	destroy_workqueue(ar_sdio->workqueue);
+-err_free_bmi_buf:
+-	kfree(ar_sdio->bmi_buf);
+-err_free_en_reg:
+-	kfree(ar_sdio->irq_data.irq_en_reg);
+-err_free_proc_reg:
+-	kfree(ar_sdio->irq_data.irq_proc_reg);
+ err_core_destroy:
+ 	ath10k_core_destroy(ar);
+ 
+@@ -2065,6 +2164,13 @@ static void ath10k_sdio_remove(struct sd
+ 	cancel_work_sync(&ar_sdio->wr_async_work);
+ 	ath10k_core_unregister(ar);
+ 	ath10k_core_destroy(ar);
++
++	if (ar->is_started && ar->hw_params.start_once) {
++		ath10k_hif_stop(ar);
++		ath10k_hif_power_down(ar);
++	}
++
++	kfree(ar_sdio->dma_buffer);
+ }
+ 
+ static const struct sdio_device_id ath10k_sdio_devices[] = {
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/sdio.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/sdio.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/sdio.h
+@@ -149,8 +149,8 @@ struct ath10k_sdio_irq_proc_regs {
+ 	u8 rx_lookahead_valid;
+ 	u8 host_int_status2;
+ 	u8 gmbox_rx_avail;
+-	__le32 rx_lookahead[2];
+-	__le32 rx_gmbox_lookahead_alias[2];
++	__le32 rx_lookahead[2 * ATH10K_HIF_MBOX_NUM_MAX];
++	__le32 int_status_enable;
+ };
+ 
+ struct ath10k_sdio_irq_enable_regs {
+@@ -207,6 +207,12 @@ struct ath10k_sdio {
+ 	struct ath10k *ar;
+ 	struct ath10k_sdio_irq_data irq_data;
+ 
++	u8 *vsg_buffer;
++	u8 *dma_buffer;
++
++	/* protects access to dma_buffer */
++	struct mutex dma_buffer_mutex;
++
+ 	/* temporary buffer for BMI requests */
+ 	u8 *bmi_buf;
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/spectral.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/spectral.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/spectral.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2013-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -406,7 +406,7 @@ static ssize_t write_file_spectral_count
+ 	if (kstrtoul(buf, 0, &val))
+ 		return -EINVAL;
+ 
+-	if (val < 0 || val > 255)
++	if (val > 255)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&ar->conf_mutex);
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/spectral.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/spectral.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/spectral.h
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2013-2015 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -44,7 +44,7 @@ enum ath10k_spectral_mode {
+ 	SPECTRAL_MANUAL,
+ };
+ 
+-#ifdef CONFIG_ATH10K_DEBUGFS
++#ifdef CONFIG_ATH10K_SPECTRAL
+ 
+ int ath10k_spectral_process_fft(struct ath10k *ar,
+ 				struct wmi_phyerr_ev_arg *phyerr,
+@@ -85,6 +85,6 @@ static inline void ath10k_spectral_destr
+ {
+ }
+ 
+-#endif /* CONFIG_ATH10K_DEBUGFS */
++#endif /* CONFIG_ATH10K_SPECTRAL */
+ 
+ #endif /* SPECTRAL_H */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/swap.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/swap.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/swap.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2015 Qualcomm Atheros, Inc.
++ * Copyright (c) 2015-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/swap.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/swap.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/swap.h
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2015 Qualcomm Atheros, Inc.
++ * Copyright (c) 2015-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/targaddrs.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/targaddrs.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/targaddrs.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/testmode.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/testmode.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/testmode.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/testmode_i.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/testmode_i.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/testmode_i.h
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014,2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/thermal.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/thermal.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/thermal.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014-2015 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/thermal.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/thermal.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/thermal.h
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2014-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/trace.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/trace.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/trace.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -152,10 +152,9 @@ TRACE_EVENT(ath10k_log_dbg_dump,
+ );
+ 
+ TRACE_EVENT(ath10k_wmi_cmd,
+-	TP_PROTO(struct ath10k *ar, int id, const void *buf, size_t buf_len,
+-		 int ret),
++	TP_PROTO(struct ath10k *ar, int id, const void *buf, size_t buf_len),
+ 
+-	TP_ARGS(ar, id, buf, buf_len, ret),
++	TP_ARGS(ar, id, buf, buf_len),
+ 
+ 	TP_STRUCT__entry(
+ 		__string(device, dev_name(ar->dev))
+@@ -163,7 +162,6 @@ TRACE_EVENT(ath10k_wmi_cmd,
+ 		__field(unsigned int, id)
+ 		__field(size_t, buf_len)
+ 		__dynamic_array(u8, buf, buf_len)
+-		__field(int, ret)
+ 	),
+ 
+ 	TP_fast_assign(
+@@ -171,17 +169,15 @@ TRACE_EVENT(ath10k_wmi_cmd,
+ 		__assign_str(driver, dev_driver_string(ar->dev));
+ 		__entry->id = id;
+ 		__entry->buf_len = buf_len;
+-		__entry->ret = ret;
+ 		memcpy(__get_dynamic_array(buf), buf, buf_len);
+ 	),
+ 
+ 	TP_printk(
+-		"%s %s id %d len %zu ret %d",
++		"%s %s id %d len %zu",
+ 		__get_str(driver),
+ 		__get_str(device),
+ 		__entry->id,
+-		__entry->buf_len,
+-		__entry->ret
++		__entry->buf_len
+ 	)
+ );
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/txrx.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/txrx.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/txrx.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2016 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -90,11 +91,12 @@ int ath10k_txrx_tx_unref(struct ath10k_h
+ 
+ 	ath10k_htt_tx_free_msdu_id(htt, tx_done->msdu_id);
+ 	ath10k_htt_tx_dec_pending(htt);
+-	if (htt->num_pending_tx == 0)
++	if (!ar->is_high_latency && (htt->num_pending_tx == 0))
+ 		wake_up(&htt->empty_tx_wq);
+ 	spin_unlock_bh(&htt->tx_lock);
+ 
+-	dma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
++	if (!ar->is_high_latency)
++		dma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
+ 
+ 	ath10k_report_offchan_tx(htt->ar, msdu);
+ 
+@@ -102,11 +104,6 @@ int ath10k_txrx_tx_unref(struct ath10k_h
+ 	memset(&info->status, 0, sizeof(info->status));
+ 	trace_ath10k_txrx_tx_unref(ar, tx_done->msdu_id);
+ 
+-	if (tx_done->status == HTT_TX_COMPL_STATE_DISCARD) {
+-		ieee80211_free_txskb(htt->ar->hw, msdu);
+-		return 0;
+-	}
+-
+ 	if (!(info->flags & IEEE80211_TX_CTL_NO_ACK))
+ 		info->flags |= IEEE80211_TX_STAT_ACK;
+ 
+@@ -117,6 +114,20 @@ int ath10k_txrx_tx_unref(struct ath10k_h
+ 	    (info->flags & IEEE80211_TX_CTL_NO_ACK))
+ 		info->flags |= IEEE80211_TX_STAT_NOACK_TRANSMITTED;
+ 
++	if (tx_done->status == HTT_TX_COMPL_STATE_DISCARD) {
++		if (info->flags & IEEE80211_TX_CTL_NO_ACK)
++			info->flags &= ~IEEE80211_TX_STAT_NOACK_TRANSMITTED;
++		else
++			info->flags &= ~IEEE80211_TX_STAT_ACK;
++	}
++
++	if (tx_done->status == HTT_TX_COMPL_STATE_ACK &&
++	    tx_done->ack_rssi != ATH10K_INVALID_RSSI) {
++		info->status.ack_signal = ATH10K_DEFAULT_NOISE_FLOOR +
++						tx_done->ack_rssi;
++		info->status.is_valid_ack_signal = true;
++	}
++
+ 	ieee80211_tx_status(htt->ar->hw, msdu);
+ 	/* we do not own the msdu anymore */
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/txrx.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/txrx.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/txrx.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2014,2016 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/usb.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/usb.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/usb.c
+@@ -983,7 +983,7 @@ static int ath10k_usb_probe(struct usb_i
+ 	struct usb_device *dev = interface_to_usbdev(interface);
+ 	int ret, vendor_id, product_id;
+ 	enum ath10k_hw_rev hw_rev;
+-	u32 chip_id;
++	struct ath10k_bus_params bus_params;
+ 
+ 	/* Assumption: All USB based chipsets (so far) are QCA9377 based.
+ 	 * If there will be newer chipsets that does not use the hw reg
+@@ -1016,9 +1016,10 @@ static int ath10k_usb_probe(struct usb_i
+ 	ar->id.vendor = vendor_id;
+ 	ar->id.device = product_id;
+ 
++	bus_params.is_high_latency = true;
+ 	/* TODO: don't know yet how to get chip_id with USB */
+-	chip_id = 0;
+-	ret = ath10k_core_register(ar, chip_id);
++	bus_params.chip_id = 0;
++	ret = ath10k_core_register(ar, &bus_params);
+ 	if (ret) {
+ 		ath10k_warn(ar, "failed to register driver core: %d\n", ret);
+ 		goto err;
+@@ -1083,6 +1084,7 @@ static int ath10k_usb_pm_resume(struct u
+ /* table of devices that work with this driver */
+ static struct usb_device_id ath10k_usb_ids[] = {
+ 	{USB_DEVICE(0x13b1, 0x0042)}, /* Linksys WUSB6100M */
++	{USB_DEVICE(0x0cf3, 0x9378)}, /* SparkLAN WPEQ-160ACN */
+ 	{ /* Terminating entry */ },
+ };
+ 
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-ops.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wmi-ops.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-ops.h
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -125,6 +126,9 @@ struct wmi_ops {
+ 					     enum wmi_force_fw_hang_type type,
+ 					     u32 delay_ms);
+ 	struct sk_buff *(*gen_mgmt_tx)(struct ath10k *ar, struct sk_buff *skb);
++	struct sk_buff *(*gen_mgmt_tx_send)(struct ath10k *ar,
++					    struct sk_buff *skb,
++					    dma_addr_t paddr);
+ 	struct sk_buff *(*gen_dbglog_cfg)(struct ath10k *ar, u64 module_enable,
+ 					  u32 log_level);
+ 	struct sk_buff *(*gen_pktlog_enable)(struct ath10k *ar, u32 filter);
+@@ -197,6 +201,9 @@ struct wmi_ops {
+ 					(struct ath10k *ar,
+ 					 enum wmi_bss_survey_req_type type);
+ 	struct sk_buff *(*gen_echo)(struct ath10k *ar, u32 value);
++	struct sk_buff *(*gen_pdev_get_tpc_table_cmdid)(struct ath10k *ar,
++							u32 param);
++
+ };
+ 
+ int ath10k_wmi_cmd_send(struct ath10k *ar, struct sk_buff *skb, u32 cmd_id);
+@@ -372,6 +379,28 @@ ath10k_wmi_get_txbf_conf_scheme(struct a
+ }
+ 
+ static inline int
++ath10k_wmi_mgmt_tx_send(struct ath10k *ar, struct sk_buff *msdu,
++			dma_addr_t paddr)
++{
++	struct sk_buff *skb;
++	int ret;
++
++	if (!ar->wmi.ops->gen_mgmt_tx_send)
++		return -EOPNOTSUPP;
++
++	skb = ar->wmi.ops->gen_mgmt_tx_send(ar, msdu, paddr);
++	if (IS_ERR(skb))
++		return PTR_ERR(skb);
++
++	ret = ath10k_wmi_cmd_send(ar, skb,
++				  ar->wmi.cmd->mgmt_tx_send_cmdid);
++	if (ret)
++		return ret;
++
++	return 0;
++}
++
++static inline int
+ ath10k_wmi_mgmt_tx(struct ath10k *ar, struct sk_buff *msdu)
+ {
+ 	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(msdu);
+@@ -385,7 +414,8 @@ ath10k_wmi_mgmt_tx(struct ath10k *ar, st
+ 	if (IS_ERR(skb))
+ 		return PTR_ERR(skb);
+ 
+-	ret = ath10k_wmi_cmd_send(ar, skb, ar->wmi.cmd->mgmt_tx_cmdid);
++	ret = ath10k_wmi_cmd_send(ar, skb,
++				  ar->wmi.cmd->mgmt_tx_cmdid);
+ 	if (ret)
+ 		return ret;
+ 
+@@ -1418,4 +1448,21 @@ ath10k_wmi_echo(struct ath10k *ar, u32 v
+ 	return ath10k_wmi_cmd_send(ar, skb, wmi->cmd->echo_cmdid);
+ }
+ 
++static inline int
++ath10k_wmi_pdev_get_tpc_table_cmdid(struct ath10k *ar, u32 param)
++{
++	struct sk_buff *skb;
++
++	if (!ar->wmi.ops->gen_pdev_get_tpc_table_cmdid)
++		return -EOPNOTSUPP;
++
++	skb = ar->wmi.ops->gen_pdev_get_tpc_table_cmdid(ar, param);
++
++	if (IS_ERR(skb))
++		return PTR_ERR(skb);
++
++	return ath10k_wmi_cmd_send(ar, skb,
++				   ar->wmi.cmd->pdev_get_tpc_table_cmdid);
++}
++
+ #endif
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-tlv.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wmi-tlv.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-tlv.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -412,6 +413,62 @@ static int ath10k_wmi_tlv_event_tx_pause
+ 	return 0;
+ }
+ 
++static int ath10k_wmi_tlv_event_temperature(struct ath10k *ar,
++					    struct sk_buff *skb)
++{
++	const struct wmi_tlv_pdev_temperature_event *ev;
++
++	ev = (struct wmi_tlv_pdev_temperature_event *)skb->data;
++	if (WARN_ON(skb->len < sizeof(*ev)))
++		return -EPROTO;
++
++	ath10k_thermal_event_temperature(ar, __le32_to_cpu(ev->temperature));
++	return 0;
++}
++
++static void ath10k_wmi_event_tdls_peer(struct ath10k *ar, struct sk_buff *skb)
++{
++	struct ieee80211_sta *station;
++	const struct wmi_tlv_tdls_peer_event *ev;
++	const void **tb;
++	struct ath10k_vif *arvif;
++
++	tb = ath10k_wmi_tlv_parse_alloc(ar, skb->data, skb->len, GFP_ATOMIC);
++	if (IS_ERR(tb)) {
++		ath10k_warn(ar, "tdls peer failed to parse tlv");
++		return;
++	}
++	ev = tb[WMI_TLV_TAG_STRUCT_TDLS_PEER_EVENT];
++	if (!ev) {
++		kfree(tb);
++		ath10k_warn(ar, "tdls peer NULL event");
++		return;
++	}
++
++	switch (__le32_to_cpu(ev->peer_reason)) {
++	case WMI_TDLS_TEARDOWN_REASON_TX:
++	case WMI_TDLS_TEARDOWN_REASON_RSSI:
++	case WMI_TDLS_TEARDOWN_REASON_PTR_TIMEOUT:
++		station = ieee80211_find_sta_by_ifaddr(ar->hw,
++						       ev->peer_macaddr.addr,
++						       NULL);
++		if (!station) {
++			ath10k_warn(ar, "did not find station from tdls peer event");
++			kfree(tb);
++			return;
++		}
++		arvif = ath10k_get_arvif(ar, __le32_to_cpu(ev->vdev_id));
++		ieee80211_tdls_oper_request(
++					arvif->vif, station->addr,
++					NL80211_TDLS_TEARDOWN,
++					WLAN_REASON_TDLS_TEARDOWN_UNREACHABLE,
++					GFP_ATOMIC
++					);
++		break;
++	}
++	kfree(tb);
++}
++
+ /***********/
+ /* TLV ops */
+ /***********/
+@@ -552,6 +609,12 @@ static void ath10k_wmi_tlv_op_rx(struct
+ 	case WMI_TLV_TX_PAUSE_EVENTID:
+ 		ath10k_wmi_tlv_event_tx_pause(ar, skb);
+ 		break;
++	case WMI_TLV_PDEV_TEMPERATURE_EVENTID:
++		ath10k_wmi_tlv_event_temperature(ar, skb);
++		break;
++	case WMI_TLV_TDLS_PEER_EVENTID:
++		ath10k_wmi_event_tdls_peer(ar, skb);
++		break;
+ 	default:
+ 		ath10k_warn(ar, "Unknown eventid: %d\n", id);
+ 		break;
+@@ -917,33 +980,69 @@ ath10k_wmi_tlv_parse_mem_reqs(struct ath
+ 	return -ENOMEM;
+ }
+ 
++struct wmi_tlv_svc_rdy_parse {
++	const struct hal_reg_capabilities *reg;
++	const struct wmi_tlv_svc_rdy_ev *ev;
++	const __le32 *svc_bmap;
++	const struct wlan_host_mem_req *mem_reqs;
++	bool svc_bmap_done;
++	bool dbs_hw_mode_done;
++};
++
++static int ath10k_wmi_tlv_svc_rdy_parse(struct ath10k *ar, u16 tag, u16 len,
++					const void *ptr, void *data)
++{
++	struct wmi_tlv_svc_rdy_parse *svc_rdy = data;
++
++	switch (tag) {
++	case WMI_TLV_TAG_STRUCT_SERVICE_READY_EVENT:
++		svc_rdy->ev = ptr;
++		break;
++	case WMI_TLV_TAG_STRUCT_HAL_REG_CAPABILITIES:
++		svc_rdy->reg = ptr;
++		break;
++	case WMI_TLV_TAG_ARRAY_STRUCT:
++		svc_rdy->mem_reqs = ptr;
++		break;
++	case WMI_TLV_TAG_ARRAY_UINT32:
++		if (!svc_rdy->svc_bmap_done) {
++			svc_rdy->svc_bmap_done = true;
++			svc_rdy->svc_bmap = ptr;
++		} else if (!svc_rdy->dbs_hw_mode_done) {
++			svc_rdy->dbs_hw_mode_done = true;
++		}
++		break;
++	default:
++		break;
++	}
++	return 0;
++}
++
+ static int ath10k_wmi_tlv_op_pull_svc_rdy_ev(struct ath10k *ar,
+ 					     struct sk_buff *skb,
+ 					     struct wmi_svc_rdy_ev_arg *arg)
+ {
+-	const void **tb;
+ 	const struct hal_reg_capabilities *reg;
+ 	const struct wmi_tlv_svc_rdy_ev *ev;
+ 	const __le32 *svc_bmap;
+ 	const struct wlan_host_mem_req *mem_reqs;
++	struct wmi_tlv_svc_rdy_parse svc_rdy = { };
+ 	int ret;
+ 
+-	tb = ath10k_wmi_tlv_parse_alloc(ar, skb->data, skb->len, GFP_ATOMIC);
+-	if (IS_ERR(tb)) {
+-		ret = PTR_ERR(tb);
++	ret = ath10k_wmi_tlv_iter(ar, skb->data, skb->len,
++				  ath10k_wmi_tlv_svc_rdy_parse, &svc_rdy);
++	if (ret) {
+ 		ath10k_warn(ar, "failed to parse tlv: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+-	ev = tb[WMI_TLV_TAG_STRUCT_SERVICE_READY_EVENT];
+-	reg = tb[WMI_TLV_TAG_STRUCT_HAL_REG_CAPABILITIES];
+-	svc_bmap = tb[WMI_TLV_TAG_ARRAY_UINT32];
+-	mem_reqs = tb[WMI_TLV_TAG_ARRAY_STRUCT];
++	ev = svc_rdy.ev;
++	reg = svc_rdy.reg;
++	svc_bmap = svc_rdy.svc_bmap;
++	mem_reqs = svc_rdy.mem_reqs;
+ 
+-	if (!ev || !reg || !svc_bmap || !mem_reqs) {
+-		kfree(tb);
++	if (!ev || !reg || !svc_bmap || !mem_reqs)
+ 		return -EPROTO;
+-	}
+ 
+ 	/* This is an internal ABI compatibility check for WMI TLV so check it
+ 	 * here instead of the generic WMI code.
+@@ -961,7 +1060,6 @@ static int ath10k_wmi_tlv_op_pull_svc_rd
+ 	    __le32_to_cpu(ev->abi.abi_ver_ns1) != WMI_TLV_ABI_VER_NS1 ||
+ 	    __le32_to_cpu(ev->abi.abi_ver_ns2) != WMI_TLV_ABI_VER_NS2 ||
+ 	    __le32_to_cpu(ev->abi.abi_ver_ns3) != WMI_TLV_ABI_VER_NS3) {
+-		kfree(tb);
+ 		return -ENOTSUPP;
+ 	}
+ 
+@@ -982,12 +1080,10 @@ static int ath10k_wmi_tlv_op_pull_svc_rd
+ 	ret = ath10k_wmi_tlv_iter(ar, mem_reqs, ath10k_wmi_tlv_len(mem_reqs),
+ 				  ath10k_wmi_tlv_parse_mem_reqs, arg);
+ 	if (ret) {
+-		kfree(tb);
+ 		ath10k_warn(ar, "failed to parse mem_reqs tlv: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+-	kfree(tb);
+ 	return 0;
+ }
+ 
+@@ -1406,7 +1502,9 @@ static struct sk_buff *ath10k_wmi_tlv_op
+ 	cmd->num_host_mem_chunks = __cpu_to_le32(ar->wmi.num_mem_chunks);
+ 
+ 	cfg->num_vdevs = __cpu_to_le32(TARGET_TLV_NUM_VDEVS);
+-	cfg->num_peers = __cpu_to_le32(TARGET_TLV_NUM_PEERS);
++	cfg->num_peers = __cpu_to_le32(ar->hw_params.num_peers);
++	cfg->ast_skid_limit = __cpu_to_le32(ar->hw_params.ast_skid_limit);
++	cfg->num_wds_entries = __cpu_to_le32(ar->hw_params.num_wds_entries);
+ 
+ 	if (test_bit(WMI_SERVICE_RX_FULL_REORDER, ar->wmi.svc_map)) {
+ 		cfg->num_offload_peers = __cpu_to_le32(TARGET_TLV_NUM_VDEVS);
+@@ -1417,8 +1515,7 @@ static struct sk_buff *ath10k_wmi_tlv_op
+ 	}
+ 
+ 	cfg->num_peer_keys = __cpu_to_le32(2);
+-	cfg->num_tids = __cpu_to_le32(TARGET_TLV_NUM_TIDS);
+-	cfg->ast_skid_limit = __cpu_to_le32(0x10);
++	cfg->num_tids = __cpu_to_le32(ar->hw_params.num_peers * 2);
+ 	cfg->tx_chain_mask = __cpu_to_le32(0x7);
+ 	cfg->rx_chain_mask = __cpu_to_le32(0x7);
+ 	cfg->rx_timeout_pri[0] = __cpu_to_le32(0x64);
+@@ -1434,13 +1531,12 @@ static struct sk_buff *ath10k_wmi_tlv_op
+ 	cfg->num_mcast_table_elems = __cpu_to_le32(0);
+ 	cfg->mcast2ucast_mode = __cpu_to_le32(0);
+ 	cfg->tx_dbg_log_size = __cpu_to_le32(0x400);
+-	cfg->num_wds_entries = __cpu_to_le32(0x20);
+ 	cfg->dma_burst_size = __cpu_to_le32(0);
+ 	cfg->mac_aggr_delim = __cpu_to_le32(0);
+ 	cfg->rx_skip_defrag_timeout_dup_detection_check = __cpu_to_le32(0);
+ 	cfg->vow_config = __cpu_to_le32(0);
+ 	cfg->gtk_offload_max_vdev = __cpu_to_le32(2);
+-	cfg->num_msdu_desc = __cpu_to_le32(TARGET_TLV_NUM_MSDU_DESC);
++	cfg->num_msdu_desc = __cpu_to_le32(ar->htt.max_num_pending_tx);
+ 	cfg->max_frag_entries = __cpu_to_le32(2);
+ 	cfg->num_tdls_vdevs = __cpu_to_le32(TARGET_TLV_NUM_TDLS_VDEVS);
+ 	cfg->num_tdls_conn_table_entries = __cpu_to_le32(0x20);
+@@ -2450,6 +2546,75 @@ ath10k_wmi_tlv_op_gen_request_stats(stru
+ }
+ 
+ static struct sk_buff *
++ath10k_wmi_tlv_op_gen_mgmt_tx_send(struct ath10k *ar, struct sk_buff *msdu,
++				   dma_addr_t paddr)
++{
++	struct ath10k_skb_cb *cb = ATH10K_SKB_CB(msdu);
++	struct wmi_tlv_mgmt_tx_cmd *cmd;
++	struct ieee80211_hdr *hdr;
++	struct ath10k_vif *arvif;
++	u32 buf_len = msdu->len;
++	struct wmi_tlv *tlv;
++	struct sk_buff *skb;
++	u32 vdev_id;
++	void *ptr;
++	int len;
++
++	if (!cb->vif)
++		return ERR_PTR(-EINVAL);
++
++	hdr = (struct ieee80211_hdr *)msdu->data;
++	arvif = (void *)cb->vif->drv_priv;
++	vdev_id = arvif->vdev_id;
++
++	if (WARN_ON_ONCE(!ieee80211_is_mgmt(hdr->frame_control)))
++		return ERR_PTR(-EINVAL);
++
++	len = sizeof(*cmd) + 2 * sizeof(*tlv);
++
++	if ((ieee80211_is_action(hdr->frame_control) ||
++	     ieee80211_is_deauth(hdr->frame_control) ||
++	     ieee80211_is_disassoc(hdr->frame_control)) &&
++	     ieee80211_has_protected(hdr->frame_control)) {
++		len += IEEE80211_CCMP_MIC_LEN;
++		buf_len += IEEE80211_CCMP_MIC_LEN;
++	}
++
++	buf_len = min_t(u32, buf_len, WMI_TLV_MGMT_TX_FRAME_MAX_LEN);
++	buf_len = round_up(buf_len, 4);
++
++	len += buf_len;
++	len = round_up(len, 4);
++	skb = ath10k_wmi_alloc_skb(ar, len);
++	if (!skb)
++		return ERR_PTR(-ENOMEM);
++
++	ptr = (void *)skb->data;
++	tlv = ptr;
++	tlv->tag = __cpu_to_le16(WMI_TLV_TAG_STRUCT_MGMT_TX_CMD);
++	tlv->len = __cpu_to_le16(sizeof(*cmd));
++	cmd = (void *)tlv->value;
++	cmd->vdev_id = __cpu_to_le32(vdev_id);
++	cmd->desc_id = 0;
++	cmd->chanfreq = 0;
++	cmd->buf_len = __cpu_to_le32(buf_len);
++	cmd->frame_len = __cpu_to_le32(msdu->len);
++	cmd->paddr = __cpu_to_le64(paddr);
++
++	ptr += sizeof(*tlv);
++	ptr += sizeof(*cmd);
++
++	tlv = ptr;
++	tlv->tag = __cpu_to_le16(WMI_TLV_TAG_ARRAY_BYTE);
++	tlv->len = __cpu_to_le16(buf_len);
++
++	ptr += sizeof(*tlv);
++	memcpy(ptr, msdu->data, buf_len);
++
++	return skb;
++}
++
++static struct sk_buff *
+ ath10k_wmi_tlv_op_gen_force_fw_hang(struct ath10k *ar,
+ 				    enum wmi_force_fw_hang_type type,
+ 				    u32 delay_ms)
+@@ -2554,6 +2719,25 @@ ath10k_wmi_tlv_op_gen_pktlog_enable(stru
+ }
+ 
+ static struct sk_buff *
++ath10k_wmi_tlv_op_gen_pdev_get_temperature(struct ath10k *ar)
++{
++	struct wmi_tlv_pdev_get_temp_cmd *cmd;
++	struct wmi_tlv *tlv;
++	struct sk_buff *skb;
++
++	skb = ath10k_wmi_alloc_skb(ar, sizeof(*tlv) + sizeof(*cmd));
++	if (!skb)
++		return ERR_PTR(-ENOMEM);
++
++	tlv = (void *)skb->data;
++	tlv->tag = __cpu_to_le16(WMI_TLV_TAG_STRUCT_PDEV_GET_TEMPERATURE_CMD);
++	tlv->len = __cpu_to_le16(sizeof(*cmd));
++	cmd = (void *)tlv->value;
++	ath10k_dbg(ar, ATH10K_DBG_WMI, "wmi pdev get temperature tlv\n");
++	return skb;
++}
++
++static struct sk_buff *
+ ath10k_wmi_tlv_op_gen_pktlog_disable(struct ath10k *ar)
+ {
+ 	struct wmi_tlv_pktlog_disable *cmd;
+@@ -2747,6 +2931,15 @@ ath10k_wmi_tlv_op_gen_update_fw_tdls_sta
+ 	 */
+ 	u32 options = 0;
+ 
++	if (test_bit(WMI_SERVICE_TDLS_UAPSD_BUFFER_STA, ar->wmi.svc_map))
++		options |=  WMI_TLV_TDLS_BUFFER_STA_EN;
++
++	/* WMI_TDLS_ENABLE_ACTIVE_EXTERNAL_CONTROL means firm will handle TDLS
++	 * link inactivity detecting logic.
++	 */
++	if (state == WMI_TDLS_ENABLE_ACTIVE)
++		state = WMI_TDLS_ENABLE_ACTIVE_EXTERNAL_CONTROL;
++
+ 	len = sizeof(*tlv) + sizeof(*cmd);
+ 	skb = ath10k_wmi_alloc_skb(ar, len);
+ 	if (!skb)
+@@ -3258,6 +3451,7 @@ static struct wmi_cmd_map wmi_tlv_cmd_ma
+ 	.bcn_filter_rx_cmdid = WMI_TLV_BCN_FILTER_RX_CMDID,
+ 	.prb_req_filter_rx_cmdid = WMI_TLV_PRB_REQ_FILTER_RX_CMDID,
+ 	.mgmt_tx_cmdid = WMI_TLV_MGMT_TX_CMDID,
++	.mgmt_tx_send_cmdid = WMI_TLV_MGMT_TX_SEND_CMD,
+ 	.prb_tmpl_cmdid = WMI_TLV_PRB_TMPL_CMDID,
+ 	.addba_clear_resp_cmdid = WMI_TLV_ADDBA_CLEAR_RESP_CMDID,
+ 	.addba_send_cmdid = WMI_TLV_ADDBA_SEND_CMDID,
+@@ -3334,7 +3528,7 @@ static struct wmi_cmd_map wmi_tlv_cmd_ma
+ 	.force_fw_hang_cmdid = WMI_TLV_FORCE_FW_HANG_CMDID,
+ 	.gpio_config_cmdid = WMI_TLV_GPIO_CONFIG_CMDID,
+ 	.gpio_output_cmdid = WMI_TLV_GPIO_OUTPUT_CMDID,
+-	.pdev_get_temperature_cmdid = WMI_TLV_CMD_UNSUPPORTED,
++	.pdev_get_temperature_cmdid = WMI_TLV_PDEV_GET_TEMPERATURE_CMDID,
+ 	.vdev_set_wmm_params_cmdid = WMI_TLV_VDEV_SET_WMM_PARAMS_CMDID,
+ 	.tdls_set_state_cmdid = WMI_TLV_TDLS_SET_STATE_CMDID,
+ 	.tdls_peer_update_cmdid = WMI_TLV_TDLS_PEER_UPDATE_CMDID,
+@@ -3592,11 +3786,12 @@ static const struct wmi_ops wmi_tlv_ops
+ 	.gen_request_stats = ath10k_wmi_tlv_op_gen_request_stats,
+ 	.gen_force_fw_hang = ath10k_wmi_tlv_op_gen_force_fw_hang,
+ 	/* .gen_mgmt_tx = not implemented; HTT is used */
++	.gen_mgmt_tx_send = ath10k_wmi_tlv_op_gen_mgmt_tx_send,
+ 	.gen_dbglog_cfg = ath10k_wmi_tlv_op_gen_dbglog_cfg,
+ 	.gen_pktlog_enable = ath10k_wmi_tlv_op_gen_pktlog_enable,
+ 	.gen_pktlog_disable = ath10k_wmi_tlv_op_gen_pktlog_disable,
+ 	/* .gen_pdev_set_quiet_mode not implemented */
+-	/* .gen_pdev_get_temperature not implemented */
++	.gen_pdev_get_temperature = ath10k_wmi_tlv_op_gen_pdev_get_temperature,
+ 	/* .gen_addba_clear_resp not implemented */
+ 	/* .gen_addba_send not implemented */
+ 	/* .gen_addba_set_resp not implemented */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-tlv.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wmi-tlv.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi-tlv.h
+@@ -1,6 +1,6 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2014 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -22,6 +22,7 @@
+ #define WMI_TLV_CMD_UNSUPPORTED 0
+ #define WMI_TLV_PDEV_PARAM_UNSUPPORTED 0
+ #define WMI_TLV_VDEV_PARAM_UNSUPPORTED 0
++#define WMI_TLV_MGMT_TX_FRAME_MAX_LEN	64
+ 
+ enum wmi_tlv_grp_id {
+ 	WMI_TLV_GRP_START = 0x3,
+@@ -132,6 +133,7 @@ enum wmi_tlv_cmd_id {
+ 	WMI_TLV_PRB_REQ_FILTER_RX_CMDID,
+ 	WMI_TLV_MGMT_TX_CMDID,
+ 	WMI_TLV_PRB_TMPL_CMDID,
++	WMI_TLV_MGMT_TX_SEND_CMD,
+ 	WMI_TLV_ADDBA_CLEAR_RESP_CMDID = WMI_TLV_CMD(WMI_TLV_GRP_BA_NEG),
+ 	WMI_TLV_ADDBA_SEND_CMDID,
+ 	WMI_TLV_ADDBA_STATUS_CMDID,
+@@ -890,6 +892,63 @@ enum wmi_tlv_tag {
+ 	WMI_TLV_TAG_STRUCT_SAP_OFL_DEL_STA_EVENT,
+ 	WMI_TLV_TAG_STRUCT_APFIND_CMD_PARAM,
+ 	WMI_TLV_TAG_STRUCT_APFIND_EVENT_HDR,
++	WMI_TLV_TAG_STRUCT_OCB_SET_SCHED_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_SET_SCHED_EVENT,
++	WMI_TLV_TAG_STRUCT_OCB_SET_CONFIG_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_SET_CONFIG_RESP_EVENT,
++	WMI_TLV_TAG_STRUCT_OCB_SET_UTC_TIME_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_START_TIMING_ADVERT_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_STOP_TIMING_ADVERT_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_GET_TSF_TIMER_CMD,
++	WMI_TLV_TAG_STRUCT_OCB_GET_TSF_TIMER_RESP_EVENT,
++	WMI_TLV_TAG_STRUCT_DCC_GET_STATS_CMD,
++	WMI_TLV_TAG_STRUCT_DCC_CHANNEL_STATS_REQUEST,
++	WMI_TLV_TAG_STRUCT_DCC_GET_STATS_RESP_EVENT,
++	WMI_TLV_TAG_STRUCT_DCC_CLEAR_STATS_CMD,
++	WMI_TLV_TAG_STRUCT_DCC_UPDATE_NDL_CMD,
++	WMI_TLV_TAG_STRUCT_DCC_UPDATE_NDL_RESP_EVENT,
++	WMI_TLV_TAG_STRUCT_DCC_STATS_EVENT,
++	WMI_TLV_TAG_STRUCT_OCB_CHANNEL,
++	WMI_TLV_TAG_STRUCT_OCB_SCHEDULE_ELEMENT,
++	WMI_TLV_TAG_STRUCT_DCC_NDL_STATS_PER_CHANNEL,
++	WMI_TLV_TAG_STRUCT_DCC_NDL_CHAN,
++	WMI_TLV_TAG_STRUCT_QOS_PARAMETER,
++	WMI_TLV_TAG_STRUCT_DCC_NDL_ACTIVE_STATE_CONFIG,
++	WMI_TLV_TAG_STRUCT_ROAM_SCAN_EXTENDED_THRESHOLD_PARAM,
++	WMI_TLV_TAG_STRUCT_ROAM_FILTER_FIXED_PARAM,
++	WMI_TLV_TAG_STRUCT_PASSPOINT_CONFIG_CMD,
++	WMI_TLV_TAG_STRUCT_PASSPOINT_EVENT_HDR,
++	WMI_TLV_TAG_STRUCT_EXTSCAN_CONFIGURE_HOTLIST_SSID_MONITOR_CMD,
++	WMI_TLV_TAG_STRUCT_EXTSCAN_HOTLIST_SSID_MATCH_EVENT,
++	WMI_TLV_TAG_STRUCT_VDEV_TSF_TSTAMP_ACTION_CMD,
++	WMI_TLV_TAG_STRUCT_VDEV_TSF_REPORT_EVENT,
++	WMI_TLV_TAG_STRUCT_GET_FW_MEM_DUMP,
++	WMI_TLV_TAG_STRUCT_UPDATE_FW_MEM_DUMP,
++	WMI_TLV_TAG_STRUCT_FW_MEM_DUMP_PARAMS,
++	WMI_TLV_TAG_STRUCT_DEBUG_MESG_FLUSH,
++	WMI_TLV_TAG_STRUCT_DEBUG_MESG_FLUSH_COMPLETE,
++	WMI_TLV_TAG_STRUCT_PEER_SET_RATE_REPORT_CONDITION,
++	WMI_TLV_TAG_STRUCT_ROAM_SUBNET_CHANGE_CONFIG,
++	WMI_TLV_TAG_STRUCT_VDEV_SET_IE_CMD,
++	WMI_TLV_TAG_STRUCT_RSSI_BREACH_MONITOR_CONFIG,
++	WMI_TLV_TAG_STRUCT_RSSI_BREACH_EVENT,
++	WMI_TLV_TAG_STRUCT_EVENT_INITIAL_WAKEUP,
++	WMI_TLV_TAG_STRUCT_SOC_SET_PCL_CMD,
++	WMI_TLV_TAG_STRUCT_SOC_SET_HW_MODE_CMD,
++	WMI_TLV_TAG_STRUCT_SOC_SET_HW_MODE_RESPONSE_EVENT,
++	WMI_TLV_TAG_STRUCT_SOC_HW_MODE_TRANSITION_EVENT,
++	WMI_TLV_TAG_STRUCT_VDEV_TXRX_STREAMS,
++	WMI_TLV_TAG_STRUCT_SOC_SET_HW_MODE_RESPONSE_VDEV_MAC_ENTRY,
++	WMI_TLV_TAG_STRUCT_SOC_SET_DUAL_MAC_CONFIG_CMD,
++	WMI_TLV_TAG_STRUCT_SOC_SET_DUAL_MAC_CONFIG_RESPONSE_EVENT,
++	WMI_TLV_TAG_STRUCT_IOAC_SOCK_PATTERN_T,
++	WMI_TLV_TAG_STRUCT_WOW_ENABLE_ICMPV6_NA_FLT_CMD,
++	WMI_TLV_TAG_STRUCT_DIAG_EVENT_LOG_CONFIG,
++	WMI_TLV_TAG_STRUCT_DIAG_EVENT_LOG_SUPPORTED_EVENT,
++	WMI_TLV_TAG_STRUCT_PACKET_FILTER_CONFIG,
++	WMI_TLV_TAG_STRUCT_PACKET_FILTER_ENABLE,
++	WMI_TLV_TAG_STRUCT_SAP_SET_BLACKLIST_PARAM_CMD,
++	WMI_TLV_TAG_STRUCT_MGMT_TX_CMD,
+ 
+ 	WMI_TLV_TAG_MAX
+ };
+@@ -965,6 +1024,50 @@ enum wmi_tlv_service {
+ 	WMI_TLV_SERVICE_STA_RX_IPA_OFFLOAD_SUPPORT,
+ 	WMI_TLV_SERVICE_MDNS_OFFLOAD,
+ 	WMI_TLV_SERVICE_SAP_AUTH_OFFLOAD,
++	WMI_TLV_SERVICE_DUAL_BAND_SIMULTANEOUS_SUPPORT,
++	WMI_TLV_SERVICE_OCB,
++	WMI_TLV_SERVICE_AP_ARPNS_OFFLOAD,
++	WMI_TLV_SERVICE_PER_BAND_CHAINMASK_SUPPORT,
++	WMI_TLV_SERVICE_PACKET_FILTER_OFFLOAD,
++	WMI_TLV_SERVICE_MGMT_TX_HTT,
++	WMI_TLV_SERVICE_MGMT_TX_WMI,
++	WMI_TLV_SERVICE_EXT_MSG,
++	WMI_TLV_SERVICE_MAWC,
++	WMI_TLV_SERVICE_PEER_ASSOC_CONF,
++	WMI_TLV_SERVICE_EGAP,
++	WMI_TLV_SERVICE_STA_PMF_OFFLOAD,
++	WMI_TLV_SERVICE_UNIFIED_WOW_CAPABILITY,
++	WMI_TLV_SERVICE_ENHANCED_PROXY_STA,
++	WMI_TLV_SERVICE_ATF,
++	WMI_TLV_SERVICE_COEX_GPIO,
++	WMI_TLV_SERVICE_AUX_SPECTRAL_INTF,
++	WMI_TLV_SERVICE_AUX_CHAN_LOAD_INTF,
++	WMI_TLV_SERVICE_BSS_CHANNEL_INFO_64,
++	WMI_TLV_SERVICE_ENTERPRISE_MESH,
++	WMI_TLV_SERVICE_RESTRT_CHNL_SUPPORT,
++	WMI_TLV_SERVICE_BPF_OFFLOAD,
++	WMI_TLV_SERVICE_SYNC_DELETE_CMDS,
++	WMI_TLV_SERVICE_SMART_ANTENNA_SW_SUPPORT,
++	WMI_TLV_SERVICE_SMART_ANTENNA_HW_SUPPORT,
++	WMI_TLV_SERVICE_RATECTRL_LIMIT_MAX_MIN_RATES,
++	WMI_TLV_SERVICE_NAN_DATA,
++	WMI_TLV_SERVICE_NAN_RTT,
++	WMI_TLV_SERVICE_11AX,
++	WMI_TLV_SERVICE_DEPRECATED_REPLACE,
++	WMI_TLV_SERVICE_TDLS_CONN_TRACKER_IN_HOST_MODE,
++	WMI_TLV_SERVICE_ENHANCED_MCAST_FILTER,
++	WMI_TLV_SERVICE_PERIODIC_CHAN_STAT_SUPPORT,
++	WMI_TLV_SERVICE_MESH_11S,
++	WMI_TLV_SERVICE_HALF_RATE_QUARTER_RATE_SUPPORT,
++	WMI_TLV_SERVICE_VDEV_RX_FILTER,
++	WMI_TLV_SERVICE_P2P_LISTEN_OFFLOAD_SUPPORT,
++	WMI_TLV_SERVICE_MARK_FIRST_WAKEUP_PACKET,
++	WMI_TLV_SERVICE_MULTIPLE_MCAST_FILTER_SET,
++	WMI_TLV_SERVICE_HOST_MANAGED_RX_REORDER,
++	WMI_TLV_SERVICE_FLASH_RDWR_SUPPORT,
++	WMI_TLV_SERVICE_WLAN_STATS_REPORT,
++	WMI_TLV_SERVICE_TX_MSDU_ID_NEW_PARTITION_SUPPORT,
++	WMI_TLV_SERVICE_DFS_PHYERR_OFFLOAD,
+ };
+ 
+ #define WMI_SERVICE_IS_ENABLED(wmi_svc_bmap, svc_id, len) \
+@@ -1121,6 +1224,8 @@ wmi_tlv_svc_map(const __le32 *in, unsign
+ 	       WMI_SERVICE_MDNS_OFFLOAD, len);
+ 	SVCMAP(WMI_TLV_SERVICE_SAP_AUTH_OFFLOAD,
+ 	       WMI_SERVICE_SAP_AUTH_OFFLOAD, len);
++	SVCMAP(WMI_TLV_SERVICE_MGMT_TX_WMI,
++	       WMI_SERVICE_MGMT_TX_WMI, len);
+ }
+ 
+ #undef SVCMAP
+@@ -1235,6 +1340,17 @@ struct wmi_tlv_init_cmd {
+ 	__le32 num_host_mem_chunks;
+ } __packed;
+ 
++struct wmi_tlv_pdev_get_temp_cmd {
++	__le32 pdev_id; /* not used */
++} __packed;
++
++struct wmi_tlv_pdev_temperature_event {
++	__le32 tlv_hdr;
++	/* temperature value in Celcius degree */
++	__le32 temperature;
++	__le32 pdev_id;
++} __packed;
++
+ struct wmi_tlv_pdev_set_param_cmd {
+ 	__le32 pdev_id; /* not used yet */
+ 	__le32 param_id;
+@@ -1641,6 +1757,21 @@ struct wmi_tlv_tx_pause_ev {
+ 	__le32 tid_map;
+ } __packed;
+ 
++struct wmi_tlv_tdls_peer_event {
++	struct wmi_mac_addr    peer_macaddr;
++	__le32 peer_status;
++	__le32 peer_reason;
++	__le32 vdev_id;
++} __packed;
++
+ void ath10k_wmi_tlv_attach(struct ath10k *ar);
+ 
++struct wmi_tlv_mgmt_tx_cmd {
++	__le32 vdev_id;
++	__le32 desc_id;
++	__le32 chanfreq;
++	__le64 paddr;
++	__le32 frame_len;
++	__le32 buf_len;
++} __packed;
+ #endif
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wmi.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi.c
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -29,6 +30,7 @@
+ #include "p2p.h"
+ #include "hw.h"
+ #include "hif.h"
++#include "txrx.h"
+ 
+ #define ATH10K_WMI_BARRIER_ECHO_ID 0xBA991E9
+ #define ATH10K_WMI_BARRIER_TIMEOUT_HZ (3 * HZ)
+@@ -195,6 +197,7 @@ static struct wmi_cmd_map wmi_cmd_map =
+ 	.mu_cal_start_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.set_cca_params_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.pdev_bss_chan_info_request_cmdid = WMI_CMD_UNSUPPORTED,
++	.pdev_get_tpc_table_cmdid = WMI_CMD_UNSUPPORTED,
+ };
+ 
+ /* 10.X WMI cmd track */
+@@ -361,6 +364,7 @@ static struct wmi_cmd_map wmi_10x_cmd_ma
+ 	.mu_cal_start_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.set_cca_params_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.pdev_bss_chan_info_request_cmdid = WMI_CMD_UNSUPPORTED,
++	.pdev_get_tpc_table_cmdid = WMI_CMD_UNSUPPORTED,
+ };
+ 
+ /* 10.2.4 WMI cmd track */
+@@ -527,6 +531,7 @@ static struct wmi_cmd_map wmi_10_2_4_cmd
+ 	.set_cca_params_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.pdev_bss_chan_info_request_cmdid =
+ 		WMI_10_2_PDEV_BSS_CHAN_INFO_REQUEST_CMDID,
++	.pdev_get_tpc_table_cmdid = WMI_CMD_UNSUPPORTED,
+ };
+ 
+ /* 10.4 WMI cmd track */
+@@ -1479,6 +1484,7 @@ static struct wmi_cmd_map wmi_10_2_cmd_m
+ 	.pdev_get_ani_cck_config_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.pdev_get_ani_ofdm_config_cmdid = WMI_CMD_UNSUPPORTED,
+ 	.pdev_reserve_ast_entry_cmdid = WMI_CMD_UNSUPPORTED,
++	.pdev_get_tpc_table_cmdid = WMI_CMD_UNSUPPORTED,
+ };
+ 
+ static struct wmi_pdev_param_map wmi_10_4_pdev_param_map = {
+@@ -1741,8 +1747,8 @@ int ath10k_wmi_cmd_send_nowait(struct at
+ 	cmd_hdr->cmd_id = __cpu_to_le32(cmd);
+ 
+ 	memset(skb_cb, 0, sizeof(*skb_cb));
++	trace_ath10k_wmi_cmd(ar, cmd_id, skb->data, skb->len);
+ 	ret = ath10k_htc_send(&ar->htc, ar->wmi.eid, skb);
+-	trace_ath10k_wmi_cmd(ar, cmd_id, skb->data, skb->len, ret);
+ 
+ 	if (ret)
+ 		goto err_pull;
+@@ -2408,7 +2414,8 @@ int ath10k_wmi_event_mgmt_rx(struct ath1
+ 		   status->freq, status->band, status->signal,
+ 		   status->rate_idx);
+ 
+-	ieee80211_rx(ar->hw, skb);
++	ieee80211_rx_ni(ar->hw, skb);
++
+ 	return 0;
+ }
+ 
+@@ -2702,6 +2709,28 @@ ath10k_wmi_10_4_pull_peer_stats(const st
+ 	dst->peer_rx_rate = __le32_to_cpu(src->peer_rx_rate);
+ }
+ 
++static void
++ath10k_wmi_10_4_pull_vdev_stats(const struct wmi_vdev_stats_extd *src,
++				struct ath10k_fw_stats_vdev_extd *dst)
++{
++	dst->vdev_id = __le32_to_cpu(src->vdev_id);
++	dst->ppdu_aggr_cnt = __le32_to_cpu(src->ppdu_aggr_cnt);
++	dst->ppdu_noack = __le32_to_cpu(src->ppdu_noack);
++	dst->mpdu_queued = __le32_to_cpu(src->mpdu_queued);
++	dst->ppdu_nonaggr_cnt = __le32_to_cpu(src->ppdu_nonaggr_cnt);
++	dst->mpdu_sw_requeued = __le32_to_cpu(src->mpdu_sw_requeued);
++	dst->mpdu_suc_retry = __le32_to_cpu(src->mpdu_suc_retry);
++	dst->mpdu_suc_multitry = __le32_to_cpu(src->mpdu_suc_multitry);
++	dst->mpdu_fail_retry = __le32_to_cpu(src->mpdu_fail_retry);
++	dst->tx_ftm_suc = __le32_to_cpu(src->tx_ftm_suc);
++	dst->tx_ftm_suc_retry = __le32_to_cpu(src->tx_ftm_suc_retry);
++	dst->tx_ftm_fail = __le32_to_cpu(src->tx_ftm_fail);
++	dst->rx_ftmr_cnt = __le32_to_cpu(src->rx_ftmr_cnt);
++	dst->rx_ftmr_dup_cnt = __le32_to_cpu(src->rx_ftmr_dup_cnt);
++	dst->rx_iftmr_cnt = __le32_to_cpu(src->rx_iftmr_cnt);
++	dst->rx_iftmr_dup_cnt = __le32_to_cpu(src->rx_iftmr_dup_cnt);
++}
++
+ static int ath10k_wmi_main_op_pull_fw_stats(struct ath10k *ar,
+ 					    struct sk_buff *skb,
+ 					    struct ath10k_fw_stats *stats)
+@@ -3041,7 +3070,16 @@ static int ath10k_wmi_10_4_op_pull_fw_st
+ 		 */
+ 	}
+ 
+-	/* fw doesn't implement vdev stats */
++	for (i = 0; i < num_vdev_stats; i++) {
++		const struct wmi_vdev_stats *src;
++
++		/* Ignore vdev stats here as it has only vdev id. Actual vdev
++		 * stats will be retrieved from vdev extended stats.
++		 */
++		src = (void *)skb->data;
++		if (!skb_pull(skb, sizeof(*src)))
++			return -EPROTO;
++	}
+ 
+ 	for (i = 0; i < num_peer_stats; i++) {
+ 		const struct wmi_10_4_peer_stats *src;
+@@ -3073,26 +3111,43 @@ static int ath10k_wmi_10_4_op_pull_fw_st
+ 		 */
+ 	}
+ 
+-	if ((stats_id & WMI_10_4_STAT_PEER_EXTD) == 0)
+-		return 0;
++	if (stats_id & WMI_10_4_STAT_PEER_EXTD) {
++		stats->extended = true;
+ 
+-	stats->extended = true;
++		for (i = 0; i < num_peer_stats; i++) {
++			const struct wmi_10_4_peer_extd_stats *src;
++			struct ath10k_fw_extd_stats_peer *dst;
++
++			src = (void *)skb->data;
++			if (!skb_pull(skb, sizeof(*src)))
++				return -EPROTO;
++
++			dst = kzalloc(sizeof(*dst), GFP_ATOMIC);
++			if (!dst)
++				continue;
+ 
+-	for (i = 0; i < num_peer_stats; i++) {
+-		const struct wmi_10_4_peer_extd_stats *src;
+-		struct ath10k_fw_extd_stats_peer *dst;
+-
+-		src = (void *)skb->data;
+-		if (!skb_pull(skb, sizeof(*src)))
+-			return -EPROTO;
+-
+-		dst = kzalloc(sizeof(*dst), GFP_ATOMIC);
+-		if (!dst)
+-			continue;
++			ether_addr_copy(dst->peer_macaddr,
++					src->peer_macaddr.addr);
++			dst->rx_duration = __le32_to_cpu(src->rx_duration);
++			list_add_tail(&dst->list, &stats->peers_extd);
++		}
++	}
+ 
+-		ether_addr_copy(dst->peer_macaddr, src->peer_macaddr.addr);
+-		dst->rx_duration = __le32_to_cpu(src->rx_duration);
+-		list_add_tail(&dst->list, &stats->peers_extd);
++	if (stats_id & WMI_10_4_STAT_VDEV_EXTD) {
++		for (i = 0; i < num_vdev_stats; i++) {
++			const struct wmi_vdev_stats_extd *src;
++			struct ath10k_fw_stats_vdev_extd *dst;
++
++			src = (void *)skb->data;
++			if (!skb_pull(skb, sizeof(*src)))
++				return -EPROTO;
++
++			dst = kzalloc(sizeof(*dst), GFP_ATOMIC);
++			if (!dst)
++				continue;
++			ath10k_wmi_10_4_pull_vdev_stats(src, dst);
++			list_add_tail(&dst->list, &stats->vdevs);
++		}
+ 	}
+ 
+ 	return 0;
+@@ -4303,7 +4358,7 @@ static void ath10k_tpc_config_disp_table
+ 							    rate_code[i],
+ 							    type);
+ 			snprintf(buff, sizeof(buff), "%8d ", tpc[j]);
+-			strncat(tpc_value, buff, strlen(buff));
++			strlcat(tpc_value, buff, sizeof(tpc_value));
+ 		}
+ 		tpc_stats->tpc_table[type].pream_idx[i] = pream_idx;
+ 		tpc_stats->tpc_table[type].rate_code[i] = rate_code[i];
+@@ -4312,19 +4367,11 @@ static void ath10k_tpc_config_disp_table
+ 	}
+ }
+ 
+-void ath10k_wmi_event_pdev_tpc_config(struct ath10k *ar, struct sk_buff *skb)
++void ath10k_wmi_tpc_config_get_rate_code(u8 *rate_code, u16 *pream_table,
++					 u32 num_tx_chain)
+ {
+-	u32 i, j, pream_idx, num_tx_chain;
+-	u8 rate_code[WMI_TPC_RATE_MAX], rate_idx;
+-	u16 pream_table[WMI_TPC_PREAM_TABLE_MAX];
+-	struct wmi_pdev_tpc_config_event *ev;
+-	struct ath10k_tpc_stats *tpc_stats;
+-
+-	ev = (struct wmi_pdev_tpc_config_event *)skb->data;
+-
+-	tpc_stats = kzalloc(sizeof(*tpc_stats), GFP_ATOMIC);
+-	if (!tpc_stats)
+-		return;
++	u32 i, j, pream_idx;
++	u8 rate_idx;
+ 
+ 	/* Create the rate code table based on the chains supported */
+ 	rate_idx = 0;
+@@ -4348,8 +4395,6 @@ void ath10k_wmi_event_pdev_tpc_config(st
+ 	pream_table[pream_idx] = rate_idx;
+ 	pream_idx++;
+ 
+-	num_tx_chain = __le32_to_cpu(ev->num_tx_chain);
+-
+ 	/* Fill HT20 rate code */
+ 	for (i = 0; i < num_tx_chain; i++) {
+ 		for (j = 0; j < 8; j++) {
+@@ -4373,7 +4418,7 @@ void ath10k_wmi_event_pdev_tpc_config(st
+ 	pream_idx++;
+ 
+ 	/* Fill VHT20 rate code */
+-	for (i = 0; i < __le32_to_cpu(ev->num_tx_chain); i++) {
++	for (i = 0; i < num_tx_chain; i++) {
+ 		for (j = 0; j < 10; j++) {
+ 			rate_code[rate_idx] =
+ 			ATH10K_HW_RATECODE(j, i, WMI_RATE_PREAMBLE_VHT);
+@@ -4417,6 +4462,26 @@ void ath10k_wmi_event_pdev_tpc_config(st
+ 		ATH10K_HW_RATECODE(0, 0, WMI_RATE_PREAMBLE_OFDM);
+ 
+ 	pream_table[pream_idx] = ATH10K_TPC_PREAM_TABLE_END;
++}
++
++void ath10k_wmi_event_pdev_tpc_config(struct ath10k *ar, struct sk_buff *skb)
++{
++	u32 num_tx_chain;
++	u8 rate_code[WMI_TPC_RATE_MAX];
++	u16 pream_table[WMI_TPC_PREAM_TABLE_MAX];
++	struct wmi_pdev_tpc_config_event *ev;
++	struct ath10k_tpc_stats *tpc_stats;
++
++	ev = (struct wmi_pdev_tpc_config_event *)skb->data;
++
++	tpc_stats = kzalloc(sizeof(*tpc_stats), GFP_ATOMIC);
++	if (!tpc_stats)
++		return;
++
++	num_tx_chain = __le32_to_cpu(ev->num_tx_chain);
++
++	ath10k_wmi_tpc_config_get_rate_code(rate_code, pream_table,
++					    num_tx_chain);
+ 
+ 	tpc_stats->chan_freq = __le32_to_cpu(ev->chan_freq);
+ 	tpc_stats->phy_mode = __le32_to_cpu(ev->phy_mode);
+@@ -4456,6 +4521,314 @@ void ath10k_wmi_event_pdev_tpc_config(st
+ 		   __le32_to_cpu(ev->rate_max));
+ }
+ 
++static u8
++ath10k_wmi_tpc_final_get_rate(struct ath10k *ar,
++			      struct wmi_pdev_tpc_final_table_event *ev,
++			      u32 rate_idx, u32 num_chains,
++			      u32 rate_code, u8 type, u32 pream_idx)
++{
++	u8 tpc, num_streams, preamble, ch, stm_idx;
++	s8 pow_agcdd, pow_agstbc, pow_agtxbf;
++	int pream;
++
++	num_streams = ATH10K_HW_NSS(rate_code);
++	preamble = ATH10K_HW_PREAMBLE(rate_code);
++	ch = num_chains - 1;
++	stm_idx = num_streams - 1;
++	pream = -1;
++
++	if (__le32_to_cpu(ev->chan_freq) <= 2483) {
++		switch (pream_idx) {
++		case WMI_TPC_PREAM_2GHZ_CCK:
++			pream = 0;
++			break;
++		case WMI_TPC_PREAM_2GHZ_OFDM:
++			pream = 1;
++			break;
++		case WMI_TPC_PREAM_2GHZ_HT20:
++		case WMI_TPC_PREAM_2GHZ_VHT20:
++			pream = 2;
++			break;
++		case WMI_TPC_PREAM_2GHZ_HT40:
++		case WMI_TPC_PREAM_2GHZ_VHT40:
++			pream = 3;
++			break;
++		case WMI_TPC_PREAM_2GHZ_VHT80:
++			pream = 4;
++			break;
++		default:
++			pream = -1;
++			break;
++		}
++	}
++
++	if (__le32_to_cpu(ev->chan_freq) >= 5180) {
++		switch (pream_idx) {
++		case WMI_TPC_PREAM_5GHZ_OFDM:
++			pream = 0;
++			break;
++		case WMI_TPC_PREAM_5GHZ_HT20:
++		case WMI_TPC_PREAM_5GHZ_VHT20:
++			pream = 1;
++			break;
++		case WMI_TPC_PREAM_5GHZ_HT40:
++		case WMI_TPC_PREAM_5GHZ_VHT40:
++			pream = 2;
++			break;
++		case WMI_TPC_PREAM_5GHZ_VHT80:
++			pream = 3;
++			break;
++		case WMI_TPC_PREAM_5GHZ_HTCUP:
++			pream = 4;
++			break;
++		default:
++			pream = -1;
++			break;
++		}
++	}
++
++	if (pream == 4)
++		tpc = min_t(u8, ev->rates_array[rate_idx],
++			    ev->max_reg_allow_pow[ch]);
++	else
++		tpc = min_t(u8, min_t(u8, ev->rates_array[rate_idx],
++				      ev->max_reg_allow_pow[ch]),
++			    ev->ctl_power_table[0][pream][stm_idx]);
++
++	if (__le32_to_cpu(ev->num_tx_chain) <= 1)
++		goto out;
++
++	if (preamble == WMI_RATE_PREAMBLE_CCK)
++		goto out;
++
++	if (num_chains <= num_streams)
++		goto out;
++
++	switch (type) {
++	case WMI_TPC_TABLE_TYPE_STBC:
++		pow_agstbc = ev->max_reg_allow_pow_agstbc[ch - 1][stm_idx];
++		if (pream == 4)
++			tpc = min_t(u8, tpc, pow_agstbc);
++		else
++			tpc = min_t(u8, min_t(u8, tpc, pow_agstbc),
++				    ev->ctl_power_table[0][pream][stm_idx]);
++		break;
++	case WMI_TPC_TABLE_TYPE_TXBF:
++		pow_agtxbf = ev->max_reg_allow_pow_agtxbf[ch - 1][stm_idx];
++		if (pream == 4)
++			tpc = min_t(u8, tpc, pow_agtxbf);
++		else
++			tpc = min_t(u8, min_t(u8, tpc, pow_agtxbf),
++				    ev->ctl_power_table[1][pream][stm_idx]);
++		break;
++	case WMI_TPC_TABLE_TYPE_CDD:
++		pow_agcdd = ev->max_reg_allow_pow_agcdd[ch - 1][stm_idx];
++		if (pream == 4)
++			tpc = min_t(u8, tpc, pow_agcdd);
++		else
++			tpc = min_t(u8, min_t(u8, tpc, pow_agcdd),
++				    ev->ctl_power_table[0][pream][stm_idx]);
++		break;
++	default:
++		ath10k_warn(ar, "unknown wmi tpc final table type: %d\n", type);
++		tpc = 0;
++		break;
++	}
++
++out:
++	return tpc;
++}
++
++static void
++ath10k_wmi_tpc_stats_final_disp_tables(struct ath10k *ar,
++				       struct wmi_pdev_tpc_final_table_event *ev,
++				       struct ath10k_tpc_stats_final *tpc_stats,
++				       u8 *rate_code, u16 *pream_table, u8 type)
++{
++	u32 i, j, pream_idx, flags;
++	u8 tpc[WMI_TPC_TX_N_CHAIN];
++	char tpc_value[WMI_TPC_TX_N_CHAIN * WMI_TPC_BUF_SIZE];
++	char buff[WMI_TPC_BUF_SIZE];
++
++	flags = __le32_to_cpu(ev->flags);
++
++	switch (type) {
++	case WMI_TPC_TABLE_TYPE_CDD:
++		if (!(flags & WMI_TPC_CONFIG_EVENT_FLAG_TABLE_CDD)) {
++			ath10k_dbg(ar, ATH10K_DBG_WMI, "CDD not supported\n");
++			tpc_stats->flag[type] = ATH10K_TPC_TABLE_TYPE_FLAG;
++			return;
++		}
++		break;
++	case WMI_TPC_TABLE_TYPE_STBC:
++		if (!(flags & WMI_TPC_CONFIG_EVENT_FLAG_TABLE_STBC)) {
++			ath10k_dbg(ar, ATH10K_DBG_WMI, "STBC not supported\n");
++			tpc_stats->flag[type] = ATH10K_TPC_TABLE_TYPE_FLAG;
++			return;
++		}
++		break;
++	case WMI_TPC_TABLE_TYPE_TXBF:
++		if (!(flags & WMI_TPC_CONFIG_EVENT_FLAG_TABLE_TXBF)) {
++			ath10k_dbg(ar, ATH10K_DBG_WMI, "TXBF not supported\n");
++			tpc_stats->flag[type] = ATH10K_TPC_TABLE_TYPE_FLAG;
++			return;
++		}
++		break;
++	default:
++		ath10k_dbg(ar, ATH10K_DBG_WMI,
++			   "invalid table type in wmi tpc event: %d\n", type);
++		return;
++	}
++
++	pream_idx = 0;
++	for (i = 0; i < __le32_to_cpu(ev->rate_max); i++) {
++		memset(tpc_value, 0, sizeof(tpc_value));
++		memset(buff, 0, sizeof(buff));
++		if (i == pream_table[pream_idx])
++			pream_idx++;
++
++		for (j = 0; j < WMI_TPC_TX_N_CHAIN; j++) {
++			if (j >= __le32_to_cpu(ev->num_tx_chain))
++				break;
++
++			tpc[j] = ath10k_wmi_tpc_final_get_rate(ar, ev, i, j + 1,
++							       rate_code[i],
++							       type, pream_idx);
++			snprintf(buff, sizeof(buff), "%8d ", tpc[j]);
++			strlcat(tpc_value, buff, sizeof(tpc_value));
++		}
++		tpc_stats->tpc_table_final[type].pream_idx[i] = pream_idx;
++		tpc_stats->tpc_table_final[type].rate_code[i] = rate_code[i];
++		memcpy(tpc_stats->tpc_table_final[type].tpc_value[i],
++		       tpc_value, sizeof(tpc_value));
++	}
++}
++
++void ath10k_wmi_event_tpc_final_table(struct ath10k *ar, struct sk_buff *skb)
++{
++	u32 num_tx_chain;
++	u8 rate_code[WMI_TPC_FINAL_RATE_MAX];
++	u16 pream_table[WMI_TPC_PREAM_TABLE_MAX];
++	struct wmi_pdev_tpc_final_table_event *ev;
++	struct ath10k_tpc_stats_final *tpc_stats;
++
++	ev = (struct wmi_pdev_tpc_final_table_event *)skb->data;
++
++	tpc_stats = kzalloc(sizeof(*tpc_stats), GFP_ATOMIC);
++	if (!tpc_stats)
++		return;
++
++	num_tx_chain = __le32_to_cpu(ev->num_tx_chain);
++
++	ath10k_wmi_tpc_config_get_rate_code(rate_code, pream_table,
++					    num_tx_chain);
++
++	tpc_stats->chan_freq = __le32_to_cpu(ev->chan_freq);
++	tpc_stats->phy_mode = __le32_to_cpu(ev->phy_mode);
++	tpc_stats->ctl = __le32_to_cpu(ev->ctl);
++	tpc_stats->reg_domain = __le32_to_cpu(ev->reg_domain);
++	tpc_stats->twice_antenna_gain = a_sle32_to_cpu(ev->twice_antenna_gain);
++	tpc_stats->twice_antenna_reduction =
++		__le32_to_cpu(ev->twice_antenna_reduction);
++	tpc_stats->power_limit = __le32_to_cpu(ev->power_limit);
++	tpc_stats->twice_max_rd_power = __le32_to_cpu(ev->twice_max_rd_power);
++	tpc_stats->num_tx_chain = __le32_to_cpu(ev->num_tx_chain);
++	tpc_stats->rate_max = __le32_to_cpu(ev->rate_max);
++
++	ath10k_wmi_tpc_stats_final_disp_tables(ar, ev, tpc_stats,
++					       rate_code, pream_table,
++					       WMI_TPC_TABLE_TYPE_CDD);
++	ath10k_wmi_tpc_stats_final_disp_tables(ar, ev,  tpc_stats,
++					       rate_code, pream_table,
++					       WMI_TPC_TABLE_TYPE_STBC);
++	ath10k_wmi_tpc_stats_final_disp_tables(ar, ev, tpc_stats,
++					       rate_code, pream_table,
++					       WMI_TPC_TABLE_TYPE_TXBF);
++
++	ath10k_debug_tpc_stats_final_process(ar, tpc_stats);
++
++	ath10k_dbg(ar, ATH10K_DBG_WMI,
++		   "wmi event tpc final table channel %d mode %d ctl %d regd %d gain %d %d limit %d max_power %d tx_chanins %d rates %d\n",
++		   __le32_to_cpu(ev->chan_freq),
++		   __le32_to_cpu(ev->phy_mode),
++		   __le32_to_cpu(ev->ctl),
++		   __le32_to_cpu(ev->reg_domain),
++		   a_sle32_to_cpu(ev->twice_antenna_gain),
++		   __le32_to_cpu(ev->twice_antenna_reduction),
++		   __le32_to_cpu(ev->power_limit),
++		   __le32_to_cpu(ev->twice_max_rd_power) / 2,
++		   __le32_to_cpu(ev->num_tx_chain),
++		   __le32_to_cpu(ev->rate_max));
++}
++
++static void
++ath10k_wmi_handle_tdls_peer_event(struct ath10k *ar, struct sk_buff *skb)
++{
++	struct wmi_tdls_peer_event *ev;
++	struct ath10k_peer *peer;
++	struct ath10k_vif *arvif;
++	int vdev_id;
++	int peer_status;
++	int peer_reason;
++	u8 reason;
++
++	if (skb->len < sizeof(*ev)) {
++		ath10k_err(ar, "received tdls peer event with invalid size (%d bytes)\n",
++			   skb->len);
++		return;
++	}
++
++	ev = (struct wmi_tdls_peer_event *)skb->data;
++	vdev_id = __le32_to_cpu(ev->vdev_id);
++	peer_status = __le32_to_cpu(ev->peer_status);
++	peer_reason = __le32_to_cpu(ev->peer_reason);
++
++	spin_lock_bh(&ar->data_lock);
++	peer = ath10k_peer_find(ar, vdev_id, ev->peer_macaddr.addr);
++	spin_unlock_bh(&ar->data_lock);
++
++	if (!peer) {
++		ath10k_warn(ar, "failed to find peer entry for %pM\n",
++			    ev->peer_macaddr.addr);
++		return;
++	}
++
++	switch (peer_status) {
++	case WMI_TDLS_SHOULD_TEARDOWN:
++		switch (peer_reason) {
++		case WMI_TDLS_TEARDOWN_REASON_PTR_TIMEOUT:
++		case WMI_TDLS_TEARDOWN_REASON_NO_RESPONSE:
++		case WMI_TDLS_TEARDOWN_REASON_RSSI:
++			reason = WLAN_REASON_TDLS_TEARDOWN_UNREACHABLE;
++			break;
++		default:
++			reason = WLAN_REASON_TDLS_TEARDOWN_UNSPECIFIED;
++			break;
++		}
++
++		arvif = ath10k_get_arvif(ar, vdev_id);
++		if (!arvif) {
++			ath10k_warn(ar, "received tdls peer event for invalid vdev id %u\n",
++				    vdev_id);
++			return;
++		}
++
++		ieee80211_tdls_oper_request(arvif->vif, ev->peer_macaddr.addr,
++					    NL80211_TDLS_TEARDOWN, reason,
++					    GFP_ATOMIC);
++
++		ath10k_dbg(ar, ATH10K_DBG_WMI,
++			   "received tdls teardown event for peer %pM reason %u\n",
++			   ev->peer_macaddr.addr, peer_reason);
++		break;
++	default:
++		ath10k_dbg(ar, ATH10K_DBG_WMI,
++			   "received unknown tdls peer event %u\n",
++			   peer_status);
++		break;
++	}
++}
++
+ void ath10k_wmi_event_pdev_ftm_intg(struct ath10k *ar, struct sk_buff *skb)
+ {
+ 	ath10k_dbg(ar, ATH10K_DBG_WMI, "WMI_PDEV_FTM_INTG_EVENTID\n");
+@@ -5462,6 +5835,7 @@ static void ath10k_wmi_10_4_op_rx(struct
+ 	case WMI_10_4_WOW_WAKEUP_HOST_EVENTID:
+ 	case WMI_10_4_PEER_RATECODE_LIST_EVENTID:
+ 	case WMI_10_4_WDS_PEER_EVENTID:
++	case WMI_10_4_DEBUG_FATAL_CONDITION_EVENTID:
+ 		ath10k_dbg(ar, ATH10K_DBG_WMI,
+ 			   "received event id %d not implemented\n", id);
+ 		break;
+@@ -5477,6 +5851,12 @@ static void ath10k_wmi_10_4_op_rx(struct
+ 	case WMI_10_4_PDEV_TPC_CONFIG_EVENTID:
+ 		ath10k_wmi_event_pdev_tpc_config(ar, skb);
+ 		break;
++	case WMI_10_4_TDLS_PEER_EVENTID:
++		ath10k_wmi_handle_tdls_peer_event(ar, skb);
++		break;
++	case WMI_10_4_PDEV_TPC_TABLE_EVENTID:
++		ath10k_wmi_event_tpc_final_table(ar, skb);
++		break;
+ 	default:
+ 		ath10k_warn(ar, "Unknown eventid: %d\n", id);
+ 		break;
+@@ -7269,7 +7649,7 @@ ath10k_wmi_10_2_4_op_gen_pdev_get_tpc_co
+ 	cmd->param = __cpu_to_le32(param);
+ 
+ 	ath10k_dbg(ar, ATH10K_DBG_WMI,
+-		   "wmi pdev get tcp config param:%d\n", param);
++		   "wmi pdev get tpc config param %d\n", param);
+ 	return skb;
+ }
+ 
+@@ -7389,7 +7769,7 @@ ath10k_wmi_fw_pdev_tx_stats_fill(const s
+ 	len += scnprintf(buf + len, buf_len - len, "%30s %10d\n",
+ 			 "HW rate", pdev->data_rc);
+ 	len += scnprintf(buf + len, buf_len - len, "%30s %10d\n",
+-			 "Sched self tiggers", pdev->self_triggers);
++			 "Sched self triggers", pdev->self_triggers);
+ 	len += scnprintf(buf + len, buf_len - len, "%30s %10d\n",
+ 			 "Dropped due to SW retries",
+ 			 pdev->sw_retry_failure);
+@@ -7673,6 +8053,72 @@ ath10k_wmi_op_gen_pdev_enable_adaptive_c
+ 	return skb;
+ }
+ 
++static void
++ath10k_wmi_fw_vdev_stats_extd_fill(const struct ath10k_fw_stats_vdev_extd *vdev,
++				   char *buf, u32 *length)
++{
++	u32 len = *length;
++	u32 buf_len = ATH10K_FW_STATS_BUF_SIZE;
++	u32 val;
++
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "vdev id", vdev->vdev_id);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "ppdu aggr count", vdev->ppdu_aggr_cnt);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "ppdu noack", vdev->ppdu_noack);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "mpdu queued", vdev->mpdu_queued);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "ppdu nonaggr count", vdev->ppdu_nonaggr_cnt);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "mpdu sw requeued", vdev->mpdu_sw_requeued);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "mpdu success retry", vdev->mpdu_suc_retry);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "mpdu success multitry", vdev->mpdu_suc_multitry);
++	len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++			 "mpdu fail retry", vdev->mpdu_fail_retry);
++	val = vdev->tx_ftm_suc;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "tx ftm success",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->tx_ftm_suc_retry;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "tx ftm success retry",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->tx_ftm_fail;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "tx ftm fail",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->rx_ftmr_cnt;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "rx ftm request count",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->rx_ftmr_dup_cnt;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "rx ftm request dup count",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->rx_iftmr_cnt;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "rx initial ftm req count",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	val = vdev->rx_iftmr_dup_cnt;
++	if (val & WMI_VDEV_STATS_FTM_COUNT_VALID)
++		len += scnprintf(buf + len, buf_len - len, "%30s %u\n",
++				 "rx initial ftm req dup cnt",
++				 MS(val, WMI_VDEV_STATS_FTM_COUNT));
++	len += scnprintf(buf + len, buf_len - len, "\n");
++
++	*length = len;
++}
++
+ void ath10k_wmi_10_4_op_fw_stats_fill(struct ath10k *ar,
+ 				      struct ath10k_fw_stats *fw_stats,
+ 				      char *buf)
+@@ -7680,7 +8126,7 @@ void ath10k_wmi_10_4_op_fw_stats_fill(st
+ 	u32 len = 0;
+ 	u32 buf_len = ATH10K_FW_STATS_BUF_SIZE;
+ 	const struct ath10k_fw_stats_pdev *pdev;
+-	const struct ath10k_fw_stats_vdev *vdev;
++	const struct ath10k_fw_stats_vdev_extd *vdev;
+ 	const struct ath10k_fw_stats_peer *peer;
+ 	size_t num_peers;
+ 	size_t num_vdevs;
+@@ -7733,9 +8179,8 @@ void ath10k_wmi_10_4_op_fw_stats_fill(st
+ 			"ath10k VDEV stats", num_vdevs);
+ 	len += scnprintf(buf + len, buf_len - len, "%30s\n\n",
+ 				"=================");
+-
+ 	list_for_each_entry(vdev, &fw_stats->vdevs, list) {
+-		ath10k_wmi_fw_vdev_stats_fill(vdev, buf, &len);
++		ath10k_wmi_fw_vdev_stats_extd_fill(vdev, buf, &len);
+ 	}
+ 
+ 	len += scnprintf(buf + len, buf_len - len, "\n");
+@@ -7870,7 +8315,8 @@ ath10k_wmi_10_4_gen_update_fw_tdls_state
+ 	if (!skb)
+ 		return ERR_PTR(-ENOMEM);
+ 
+-	if (test_bit(WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY, ar->wmi.svc_map))
++	if (test_bit(WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY, ar->wmi.svc_map) &&
++	    state == WMI_TDLS_ENABLE_ACTIVE)
+ 		state = WMI_TDLS_ENABLE_PASSIVE;
+ 
+ 	if (test_bit(WMI_SERVICE_TDLS_UAPSD_BUFFER_STA, ar->wmi.svc_map))
+@@ -7917,6 +8363,24 @@ static u32 ath10k_wmi_prepare_peer_qos(u
+ }
+ 
+ static struct sk_buff *
++ath10k_wmi_10_4_op_gen_pdev_get_tpc_table_cmdid(struct ath10k *ar, u32 param)
++{
++	struct wmi_pdev_get_tpc_table_cmd *cmd;
++	struct sk_buff *skb;
++
++	skb = ath10k_wmi_alloc_skb(ar, sizeof(*cmd));
++	if (!skb)
++		return ERR_PTR(-ENOMEM);
++
++	cmd = (struct wmi_pdev_get_tpc_table_cmd *)skb->data;
++	cmd->param = __cpu_to_le32(param);
++
++	ath10k_dbg(ar, ATH10K_DBG_WMI,
++		   "wmi pdev get tpc table param:%d\n", param);
++	return skb;
++}
++
++static struct sk_buff *
+ ath10k_wmi_10_4_gen_tdls_peer_update(struct ath10k *ar,
+ 				     const struct wmi_tdls_peer_update_cmd_arg *arg,
+ 				     const struct wmi_tdls_peer_capab_arg *cap,
+@@ -8357,6 +8821,8 @@ static const struct wmi_ops wmi_10_4_ops
+ 	.ext_resource_config = ath10k_wmi_10_4_ext_resource_config,
+ 	.gen_update_fw_tdls_state = ath10k_wmi_10_4_gen_update_fw_tdls_state,
+ 	.gen_tdls_peer_update = ath10k_wmi_10_4_gen_tdls_peer_update,
++	.gen_pdev_get_tpc_table_cmdid =
++			ath10k_wmi_10_4_op_gen_pdev_get_tpc_table_cmdid,
+ 
+ 	/* shared with 10.2 */
+ 	.pull_echo_ev = ath10k_wmi_op_pull_echo_ev,
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wmi.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wmi.h
+@@ -1,6 +1,7 @@
+ /*
+  * Copyright (c) 2005-2011 Atheros Communications Inc.
+- * Copyright (c) 2011-2013 Qualcomm Atheros, Inc.
++ * Copyright (c) 2011-2017 Qualcomm Atheros, Inc.
++ * Copyright (c) 2018, The Linux Foundation. All rights reserved.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+@@ -195,6 +196,12 @@ enum wmi_service {
+ 	WMI_SERVICE_SMART_LOGGING_SUPPORT,
+ 	WMI_SERVICE_TDLS_CONN_TRACKER_IN_HOST_MODE,
+ 	WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY,
++	WMI_SERVICE_MGMT_TX_WMI,
++	WMI_SERVICE_TDLS_WIDER_BANDWIDTH,
++	WMI_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
++	WMI_SERVICE_HOST_DFS_CHECK_SUPPORT,
++	WMI_SERVICE_TPC_STATS_FINAL,
++	WMI_SERVICE_RESET_CHIP,
+ 
+ 	/* keep last */
+ 	WMI_SERVICE_MAX,
+@@ -232,6 +239,8 @@ enum wmi_10x_service {
+ 	WMI_10X_SERVICE_MESH,
+ 	WMI_10X_SERVICE_EXT_RES_CFG_SUPPORT,
+ 	WMI_10X_SERVICE_PEER_STATS,
++	WMI_10X_SERVICE_RESET_CHIP,
++	WMI_10X_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
+ };
+ 
+ enum wmi_main_service {
+@@ -336,6 +345,10 @@ enum wmi_10_4_service {
+ 	WMI_10_4_SERVICE_TDLS_UAPSD_SLEEP_STA,
+ 	WMI_10_4_SERVICE_TDLS_CONN_TRACKER_IN_HOST_MODE,
+ 	WMI_10_4_SERVICE_TDLS_EXPLICIT_MODE_ONLY,
++	WMI_10_4_SERVICE_TDLS_WIDER_BANDWIDTH,
++	WMI_10_4_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
++	WMI_10_4_SERVICE_HOST_DFS_CHECK_SUPPORT,
++	WMI_10_4_SERVICE_TPC_STATS_FINAL,
+ };
+ 
+ static inline char *wmi_service_name(int service_id)
+@@ -444,6 +457,10 @@ static inline char *wmi_service_name(int
+ 	SVCSTR(WMI_SERVICE_SMART_LOGGING_SUPPORT);
+ 	SVCSTR(WMI_SERVICE_TDLS_CONN_TRACKER_IN_HOST_MODE);
+ 	SVCSTR(WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY);
++	SVCSTR(WMI_SERVICE_TDLS_WIDER_BANDWIDTH);
++	SVCSTR(WMI_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS);
++	SVCSTR(WMI_SERVICE_HOST_DFS_CHECK_SUPPORT);
++	SVCSTR(WMI_SERVICE_TPC_STATS_FINAL);
+ 	default:
+ 		return NULL;
+ 	}
+@@ -534,6 +551,10 @@ static inline void wmi_10x_svc_map(const
+ 	       WMI_SERVICE_EXT_RES_CFG_SUPPORT, len);
+ 	SVCMAP(WMI_10X_SERVICE_PEER_STATS,
+ 	       WMI_SERVICE_PEER_STATS, len);
++	SVCMAP(WMI_10X_SERVICE_RESET_CHIP,
++	       WMI_SERVICE_RESET_CHIP, len);
++	SVCMAP(WMI_10X_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
++	       WMI_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS, len);
+ }
+ 
+ static inline void wmi_main_svc_map(const __le32 *in, unsigned long *out,
+@@ -740,6 +761,14 @@ static inline void wmi_10_4_svc_map(cons
+ 	       WMI_SERVICE_TDLS_CONN_TRACKER_IN_HOST_MODE, len);
+ 	SVCMAP(WMI_10_4_SERVICE_TDLS_EXPLICIT_MODE_ONLY,
+ 	       WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY, len);
++	SVCMAP(WMI_10_4_SERVICE_TDLS_WIDER_BANDWIDTH,
++	       WMI_SERVICE_TDLS_WIDER_BANDWIDTH, len);
++	SVCMAP(WMI_10_4_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS,
++	       WMI_SERVICE_HTT_MGMT_TX_COMP_VALID_FLAGS, len);
++	SVCMAP(WMI_10_4_SERVICE_HOST_DFS_CHECK_SUPPORT,
++	       WMI_SERVICE_HOST_DFS_CHECK_SUPPORT, len);
++	SVCMAP(WMI_10_4_SERVICE_TPC_STATS_FINAL,
++	       WMI_SERVICE_TPC_STATS_FINAL, len);
+ }
+ 
+ #undef SVCMAP
+@@ -797,6 +826,7 @@ struct wmi_cmd_map {
+ 	u32 bcn_filter_rx_cmdid;
+ 	u32 prb_req_filter_rx_cmdid;
+ 	u32 mgmt_tx_cmdid;
++	u32 mgmt_tx_send_cmdid;
+ 	u32 prb_tmpl_cmdid;
+ 	u32 addba_clear_resp_cmdid;
+ 	u32 addba_send_cmdid;
+@@ -2922,7 +2952,7 @@ struct wmi_ext_resource_config_10_4_cmd
+ 	__le32 max_tdls_concurrent_buffer_sta;
+ };
+ 
+-/* strucutre describing host memory chunk. */
++/* structure describing host memory chunk. */
+ struct host_memory_chunk {
+ 	/* id of the request that is passed up in service ready */
+ 	__le32 req_id;
+@@ -3986,10 +4016,12 @@ struct wmi_pdev_get_tpc_config_cmd {
+ 
+ #define WMI_TPC_CONFIG_PARAM		1
+ #define WMI_TPC_RATE_MAX		160
++#define WMI_TPC_FINAL_RATE_MAX		240
+ #define WMI_TPC_TX_N_CHAIN		4
+ #define WMI_TPC_PREAM_TABLE_MAX		10
+ #define WMI_TPC_FLAG			3
+ #define WMI_TPC_BUF_SIZE		10
++#define WMI_TPC_BEAMFORMING		2
+ 
+ enum wmi_tpc_table_type {
+ 	WMI_TPC_TABLE_TYPE_CDD = 0,
+@@ -4032,6 +4064,51 @@ enum wmi_tp_scale {
+ 	WMI_TP_SCALE_SIZE   = 5,	/* max num of enum     */
+ };
+ 
++struct wmi_pdev_tpc_final_table_event {
++	__le32 reg_domain;
++	__le32 chan_freq;
++	__le32 phy_mode;
++	__le32 twice_antenna_reduction;
++	__le32 twice_max_rd_power;
++	a_sle32 twice_antenna_gain;
++	__le32 power_limit;
++	__le32 rate_max;
++	__le32 num_tx_chain;
++	__le32 ctl;
++	__le32 flags;
++	s8 max_reg_allow_pow[WMI_TPC_TX_N_CHAIN];
++	s8 max_reg_allow_pow_agcdd[WMI_TPC_TX_N_CHAIN][WMI_TPC_TX_N_CHAIN];
++	s8 max_reg_allow_pow_agstbc[WMI_TPC_TX_N_CHAIN][WMI_TPC_TX_N_CHAIN];
++	s8 max_reg_allow_pow_agtxbf[WMI_TPC_TX_N_CHAIN][WMI_TPC_TX_N_CHAIN];
++	u8 rates_array[WMI_TPC_FINAL_RATE_MAX];
++	u8 ctl_power_table[WMI_TPC_BEAMFORMING][WMI_TPC_TX_N_CHAIN]
++	   [WMI_TPC_TX_N_CHAIN];
++} __packed;
++
++struct wmi_pdev_get_tpc_table_cmd {
++	__le32 param;
++} __packed;
++
++enum wmi_tpc_pream_2ghz {
++	WMI_TPC_PREAM_2GHZ_CCK = 0,
++	WMI_TPC_PREAM_2GHZ_OFDM,
++	WMI_TPC_PREAM_2GHZ_HT20,
++	WMI_TPC_PREAM_2GHZ_HT40,
++	WMI_TPC_PREAM_2GHZ_VHT20,
++	WMI_TPC_PREAM_2GHZ_VHT40,
++	WMI_TPC_PREAM_2GHZ_VHT80,
++};
++
++enum wmi_tpc_pream_5ghz {
++	WMI_TPC_PREAM_5GHZ_OFDM = 1,
++	WMI_TPC_PREAM_5GHZ_HT20,
++	WMI_TPC_PREAM_5GHZ_HT40,
++	WMI_TPC_PREAM_5GHZ_VHT20,
++	WMI_TPC_PREAM_5GHZ_VHT40,
++	WMI_TPC_PREAM_5GHZ_VHT80,
++	WMI_TPC_PREAM_5GHZ_HTCUP,
++};
++
+ struct wmi_pdev_chanlist_update_event {
+ 	/* number of channels */
+ 	__le32 num_chan;
+@@ -4343,6 +4420,7 @@ enum wmi_10_4_stats_id {
+ 	WMI_10_4_STAT_AP		= BIT(1),
+ 	WMI_10_4_STAT_INST		= BIT(2),
+ 	WMI_10_4_STAT_PEER_EXTD		= BIT(3),
++	WMI_10_4_STAT_VDEV_EXTD		= BIT(4),
+ };
+ 
+ struct wlan_inst_rssi_args {
+@@ -4482,12 +4560,36 @@ struct wmi_10_4_pdev_stats {
+ 
+ /*
+  * VDEV statistics
+- * TODO: add all VDEV stats here
+  */
++
++#define WMI_VDEV_STATS_FTM_COUNT_VALID	BIT(31)
++#define WMI_VDEV_STATS_FTM_COUNT_LSB	0
++#define WMI_VDEV_STATS_FTM_COUNT_MASK	0x7fffffff
++
+ struct wmi_vdev_stats {
+ 	__le32 vdev_id;
+ } __packed;
+ 
++struct wmi_vdev_stats_extd {
++	__le32 vdev_id;
++	__le32 ppdu_aggr_cnt;
++	__le32 ppdu_noack;
++	__le32 mpdu_queued;
++	__le32 ppdu_nonaggr_cnt;
++	__le32 mpdu_sw_requeued;
++	__le32 mpdu_suc_retry;
++	__le32 mpdu_suc_multitry;
++	__le32 mpdu_fail_retry;
++	__le32 tx_ftm_suc;
++	__le32 tx_ftm_suc_retry;
++	__le32 tx_ftm_fail;
++	__le32 rx_ftmr_cnt;
++	__le32 rx_ftmr_dup_cnt;
++	__le32 rx_iftmr_cnt;
++	__le32 rx_iftmr_dup_cnt;
++	__le32 reserved[6];
++} __packed;
++
+ /*
+  * peer statistics.
+  * TODO: add more stats
+@@ -4751,6 +4853,7 @@ struct wmi_key_seq_counter {
+ #define WMI_CIPHER_WAPI     0x5
+ #define WMI_CIPHER_CKIP     0x6
+ #define WMI_CIPHER_AES_CMAC 0x7
++#define WMI_CIPHER_AES_GCM  0x8
+ 
+ struct wmi_vdev_install_key_cmd {
+ 	__le32 vdev_id;
+@@ -6721,6 +6824,7 @@ enum wmi_tdls_state {
+ 	WMI_TDLS_DISABLE,
+ 	WMI_TDLS_ENABLE_PASSIVE,
+ 	WMI_TDLS_ENABLE_ACTIVE,
++	WMI_TDLS_ENABLE_ACTIVE_EXTERNAL_CONTROL,
+ };
+ 
+ enum wmi_tdls_peer_state {
+@@ -6971,5 +7075,8 @@ void ath10k_wmi_10_4_op_fw_stats_fill(st
+ int ath10k_wmi_op_get_vdev_subtype(struct ath10k *ar,
+ 				   enum wmi_vdev_subtype subtype);
+ int ath10k_wmi_barrier(struct ath10k *ar);
++void ath10k_wmi_tpc_config_get_rate_code(u8 *rate_code, u16 *pream_table,
++					 u32 num_tx_chain);
++void ath10k_wmi_event_tpc_final_table(struct ath10k *ar, struct sk_buff *skb);
+ 
+ #endif /* _WMI_H_ */
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wow.c
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wow.c
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wow.c
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2015 Qualcomm Atheros, Inc.
++ * Copyright (c) 2015-2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/drivers/net/wireless/ath/ath10k/wow.h
+===================================================================
+--- linux-4.14.54.orig/drivers/net/wireless/ath/ath10k/wow.h
++++ linux-4.14.54/drivers/net/wireless/ath/ath10k/wow.h
+@@ -1,5 +1,5 @@
+ /*
+- * Copyright (c) 2015 Qualcomm Atheros, Inc.
++ * Copyright (c) 2015,2017 Qualcomm Atheros, Inc.
+  *
+  * Permission to use, copy, modify, and/or distribute this software for any
+  * purpose with or without fee is hereby granted, provided that the above
+Index: linux-4.14.54/include/linux/pci_ids.h
+===================================================================
+--- linux-4.14.54.orig/include/linux/pci_ids.h
++++ linux-4.14.54/include/linux/pci_ids.h
+@@ -45,6 +45,7 @@
+ #define PCI_CLASS_MULTIMEDIA_VIDEO	0x0400
+ #define PCI_CLASS_MULTIMEDIA_AUDIO	0x0401
+ #define PCI_CLASS_MULTIMEDIA_PHONE	0x0402
++#define PCI_CLASS_MULTIMEDIA_HD_AUDIO	0x0403
+ #define PCI_CLASS_MULTIMEDIA_OTHER	0x0480
+ 
+ #define PCI_BASE_CLASS_MEMORY		0x05
+@@ -149,6 +150,8 @@
+ #define PCI_VENDOR_ID_DYNALINK		0x0675
+ #define PCI_DEVICE_ID_DYNALINK_IS64PH	0x1702
+ 
++#define PCI_VENDOR_ID_UBIQUITI		0x0777
++
+ #define PCI_VENDOR_ID_BERKOM			0x0871
+ #define PCI_DEVICE_ID_BERKOM_A1T		0xffa1
+ #define PCI_DEVICE_ID_BERKOM_T_CONCEPT		0xffa2
+@@ -1331,6 +1334,7 @@
+ #define PCI_DEVICE_ID_IMS_TT3D		0x9135
+ 
+ #define PCI_VENDOR_ID_AMCC		0x10e8
++#define PCI_VENDOR_ID_AMPERE		0x1def
+ 
+ #define PCI_VENDOR_ID_INTERG		0x10ea
+ #define PCI_DEVICE_ID_INTERG_1682	0x1682
+@@ -1559,6 +1563,8 @@
+ #define PCI_DEVICE_ID_SERVERWORKS_CSB6LPC 0x0227
+ #define PCI_DEVICE_ID_SERVERWORKS_HT1100LD 0x0408
+ 
++#define PCI_VENDOR_ID_ALTERA		0x1172
++
+ #define PCI_VENDOR_ID_SBE		0x1176
+ #define PCI_DEVICE_ID_SBE_WANXL100	0x0301
+ #define PCI_DEVICE_ID_SBE_WANXL200	0x0302
+@@ -2381,6 +2387,8 @@
+ 
+ #define PCI_VENDOR_ID_LENOVO		0x17aa
+ 
++#define PCI_VENDOR_ID_CDNS		0x17cd
++
+ #define PCI_VENDOR_ID_ARECA		0x17d3
+ #define PCI_DEVICE_ID_ARECA_1110	0x1110
+ #define PCI_DEVICE_ID_ARECA_1120	0x1120
+@@ -2552,6 +2560,9 @@
+ #define PCI_DEVICE_ID_TEHUTI_3010	0x3010
+ #define PCI_DEVICE_ID_TEHUTI_3014	0x3014
+ 
++#define PCI_VENDOR_ID_SUNIX		0x1fd4
++#define PCI_DEVICE_ID_SUNIX_1999	0x1999
++
+ #define PCI_VENDOR_ID_HINT             0x3388
+ #define PCI_DEVICE_ID_HINT_VXPROII_IDE 0x8013
+ 
+Index: linux-4.14.54/include/net/mac80211.h
+===================================================================
+--- linux-4.14.54.orig/include/net/mac80211.h
++++ linux-4.14.54/include/net/mac80211.h
+@@ -6,6 +6,7 @@
+  * Copyright 2007-2010	Johannes Berg <johannes@sipsolutions.net>
+  * Copyright 2013-2014  Intel Mobile Communications GmbH
+  * Copyright (C) 2015 - 2017 Intel Deutschland GmbH
++ * Copyright (C) 2018        Intel Corporation
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License version 2 as
+@@ -301,6 +302,8 @@ struct ieee80211_vif_chanctx_switch {
+  * @BSS_CHANGED_MU_GROUPS: VHT MU-MIMO group id or user position changed
+  * @BSS_CHANGED_KEEP_ALIVE: keep alive options (idle period or protected
+  *	keep alive) changed.
++ * @BSS_CHANGED_MCAST_RATE: Multicast Rate setting changed for this interface
++ *
+  */
+ enum ieee80211_bss_change {
+ 	BSS_CHANGED_ASSOC		= 1<<0,
+@@ -328,6 +331,7 @@ enum ieee80211_bss_change {
+ 	BSS_CHANGED_OCB                 = 1<<22,
+ 	BSS_CHANGED_MU_GROUPS		= 1<<23,
+ 	BSS_CHANGED_KEEP_ALIVE		= 1<<24,
++	BSS_CHANGED_MCAST_RATE		= 1<<25,
+ 
+ 	/* when adding here, make sure to change ieee80211_reconfig */
+ };
+@@ -934,6 +938,7 @@ struct ieee80211_tx_info {
+ 			u8 ampdu_len;
+ 			u8 antenna;
+ 			u16 tx_time;
++			bool is_valid_ack_signal;
+ 			void *status_driver_data[19 / sizeof(void *)];
+ 		} status;
+ 		struct {
+@@ -1098,6 +1103,9 @@ ieee80211_tx_info_clear_status(struct ie
+  *	the first subframe.
+  * @RX_FLAG_ICV_STRIPPED: The ICV is stripped from this frame. CRC checking must
+  *	be done in the hardware.
++ * @RX_FLAG_AMPDU_EOF_BIT: Value of the EOF bit in the A-MPDU delimiter for this
++ *	frame
++ * @RX_FLAG_AMPDU_EOF_BIT_KNOWN: The EOF value is known
+  */
+ enum mac80211_rx_flags {
+ 	RX_FLAG_MMIC_ERROR		= BIT(0),
+@@ -1124,6 +1132,8 @@ enum mac80211_rx_flags {
+ 	RX_FLAG_MIC_STRIPPED		= BIT(21),
+ 	RX_FLAG_ALLOW_SAME_PN		= BIT(22),
+ 	RX_FLAG_ICV_STRIPPED		= BIT(23),
++	RX_FLAG_AMPDU_EOF_BIT		= BIT(24),
++	RX_FLAG_AMPDU_EOF_BIT_KNOWN	= BIT(25),
+ };
+ 
+ /**
+@@ -1552,6 +1562,9 @@ struct wireless_dev *ieee80211_vif_to_wd
+  * @IEEE80211_KEY_FLAG_RESERVE_TAILROOM: This flag should be set by the
+  *	driver for a key to indicate that sufficient tailroom must always
+  *	be reserved for ICV or MIC, even when HW encryption is enabled.
++ * @IEEE80211_KEY_FLAG_PUT_MIC_SPACE: This flag should be set by the driver for
++ *	a TKIP key if it only requires MIC space. Do not set together with
++ *	@IEEE80211_KEY_FLAG_GENERATE_MMIC on the same key.
+  */
+ enum ieee80211_key_flags {
+ 	IEEE80211_KEY_FLAG_GENERATE_IV_MGMT	= BIT(0),
+@@ -1562,6 +1575,7 @@ enum ieee80211_key_flags {
+ 	IEEE80211_KEY_FLAG_PUT_IV_SPACE		= BIT(5),
+ 	IEEE80211_KEY_FLAG_RX_MGMT		= BIT(6),
+ 	IEEE80211_KEY_FLAG_RESERVE_TAILROOM	= BIT(7),
++	IEEE80211_KEY_FLAG_PUT_MIC_SPACE	= BIT(8),
+ };
+ 
+ /**
+@@ -1593,8 +1607,8 @@ struct ieee80211_key_conf {
+ 	u8 icv_len;
+ 	u8 iv_len;
+ 	u8 hw_key_idx;
+-	u8 flags;
+ 	s8 keyidx;
++	u16 flags;
+ 	u8 keylen;
+ 	u8 key[0];
+ };
+@@ -2056,6 +2070,20 @@ struct ieee80211_txq {
+  *	The stack will not do fragmentation.
+  *	The callback for @set_frag_threshold should be set as well.
+  *
++ * @IEEE80211_HW_SUPPORTS_TDLS_BUFFER_STA: Hardware supports buffer STA on
++ *	TDLS links.
++ *
++ * @IEEE80211_HW_DEAUTH_NEED_MGD_TX_PREP: The driver requires the
++ *	mgd_prepare_tx() callback to be called before transmission of a
++ *	deauthentication frame in case the association was completed but no
++ *	beacon was heard. This is required in multi-channel scenarios, where the
++ *	virtual interface might not be given air time for the transmission of
++ *	the frame, as it is not synced with the AP/P2P GO yet, and thus the
++ *	deauthentication frame might not be transmitted.
++ >
++ * @IEEE80211_HW_DOESNT_SUPPORT_QOS_NDP: The driver (or firmware) doesn't
++ *	support QoS NDP for AP probing - that's most likely a driver bug.
++ *
+  * @NUM_IEEE80211_HW_FLAGS: number of hardware flags, used for sizing arrays
+  */
+ enum ieee80211_hw_flags {
+@@ -2098,6 +2126,9 @@ enum ieee80211_hw_flags {
+ 	IEEE80211_HW_TX_FRAG_LIST,
+ 	IEEE80211_HW_REPORTS_LOW_ACK,
+ 	IEEE80211_HW_SUPPORTS_TX_FRAG,
++	IEEE80211_HW_SUPPORTS_TDLS_BUFFER_STA,
++	IEEE80211_HW_DEAUTH_NEED_MGD_TX_PREP,
++	IEEE80211_HW_DOESNT_SUPPORT_QOS_NDP,
+ 
+ 	/* keep last, obviously */
+ 	NUM_IEEE80211_HW_FLAGS
+@@ -3342,6 +3373,9 @@ enum ieee80211_reconfig_type {
+  *	management frame prior to having successfully associated to allow the
+  *	driver to give it channel time for the transmission, to get a response
+  *	and to be able to synchronize with the GO.
++ *	For drivers that set %IEEE80211_HW_DEAUTH_NEED_MGD_TX_PREP, mac80211
++ *	would also call this function before transmitting a deauthentication
++ *	frame in case that no beacon was heard from the AP/P2P GO.
+  *	The callback will be called before each transmission and upon return
+  *	mac80211 will transmit the frame right away.
+  *	The callback is optional and can (should!) sleep.
+@@ -5447,8 +5481,14 @@ void ieee80211_mark_rx_ba_filtered_frame
+  */
+ void ieee80211_send_bar(struct ieee80211_vif *vif, u8 *ra, u16 tid, u16 ssn);
+ 
++/**
++ * ieee80211_manage_rx_ba_offl - helper to queue an RX BA work
++ * @vif: &struct ieee80211_vif pointer from the add_interface callback
++ * @addr: station mac address
++ * @tid: the rx tid
++ */
+ void ieee80211_manage_rx_ba_offl(struct ieee80211_vif *vif, const u8 *addr,
+-				 unsigned int bit);
++				 unsigned int tid);
+ 
+ /**
+  * ieee80211_start_rx_ba_session_offl - start a Rx BA session
diff --git a/target/linux/mvebu/patches-4.14/9084-net-phy-Cosmetic-fixes-to-phylink-sfp-sfp-bus.c.patch b/target/linux/mvebu/patches-4.14/9084-net-phy-Cosmetic-fixes-to-phylink-sfp-sfp-bus.c.patch
new file mode 100644
index 0000000..10c8bc1
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/9084-net-phy-Cosmetic-fixes-to-phylink-sfp-sfp-bus.c.patch
@@ -0,0 +1,233 @@
+From 1d798ffc4fbd94775e415445366f4d30df595c11 Mon Sep 17 00:00:00 2001
+From: Florian Fainelli <f.fainelli@gmail.com>
+Date: Mon, 30 Oct 2017 21:42:57 -0700
+Subject: [PATCH 84/90] net: phy: Cosmetic fixes to phylink/sfp/sfp-bus.c
+
+Perform a number of stylistic changes to phylink.c, sfp.c and sfp-bus.c:
+
+- align with netdev-style comments
+- align function arguments to the opening parenthesis
+- remove blank lines
+- fixup a few lines over 80 columns
+
+Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ drivers/net/phy/phylink.c | 13 ++++++-------
+ drivers/net/phy/sfp-bus.c | 11 +++--------
+ drivers/net/phy/sfp.c     | 27 +++++++++++++--------------
+ 3 files changed, 22 insertions(+), 29 deletions(-)
+
+Index: linux-4.14.50/drivers/net/phy/phylink.c
+===================================================================
+--- linux-4.14.50.orig/drivers/net/phy/phylink.c
++++ linux-4.14.50/drivers/net/phy/phylink.c
+@@ -357,7 +357,7 @@ static void phylink_get_fixed_state(stru
+  *    1     1       0     1     TX
+  */
+ static void phylink_resolve_flow(struct phylink *pl,
+-	struct phylink_link_state *state)
++				 struct phylink_link_state *state)
+ {
+ 	int new_pause = 0;
+ 
+@@ -506,7 +506,8 @@ static int phylink_register_sfp(struct p
+ }
+ 
+ struct phylink *phylink_create(struct net_device *ndev, struct device_node *np,
+-	phy_interface_t iface, const struct phylink_mac_ops *ops)
++			       phy_interface_t iface,
++			       const struct phylink_mac_ops *ops)
+ {
+ 	struct phylink *pl;
+ 	int ret;
+@@ -586,7 +587,7 @@ void phylink_phy_change(struct phy_devic
+ 	phylink_run_resolve(pl);
+ 
+ 	netdev_dbg(pl->netdev, "phy link %s %s/%s/%s\n", up ? "up" : "down",
+-	           phy_modes(phydev->interface),
++		   phy_modes(phydev->interface),
+ 		   phy_speed_to_str(phydev->speed),
+ 		   phy_duplex_to_str(phydev->duplex));
+ }
+@@ -825,7 +826,7 @@ static void phylink_get_ksettings(const
+ }
+ 
+ int phylink_ethtool_ksettings_get(struct phylink *pl,
+-	struct ethtool_link_ksettings *kset)
++				  struct ethtool_link_ksettings *kset)
+ {
+ 	struct phylink_link_state link_state;
+ 
+@@ -872,7 +873,7 @@ int phylink_ethtool_ksettings_get(struct
+ EXPORT_SYMBOL_GPL(phylink_ethtool_ksettings_get);
+ 
+ int phylink_ethtool_ksettings_set(struct phylink *pl,
+-	const struct ethtool_link_ksettings *kset)
++				  const struct ethtool_link_ksettings *kset)
+ {
+ 	struct ethtool_link_ksettings our_kset;
+ 	struct phylink_link_state config;
+@@ -1340,8 +1341,6 @@ int phylink_mii_ioctl(struct phylink *pl
+ }
+ EXPORT_SYMBOL_GPL(phylink_mii_ioctl);
+ 
+-
+-
+ static int phylink_sfp_module_insert(void *upstream,
+ 				     const struct sfp_eeprom_id *id)
+ {
+Index: linux-4.14.50/drivers/net/phy/sfp-bus.c
+===================================================================
+--- linux-4.14.50.orig/drivers/net/phy/sfp-bus.c
++++ linux-4.14.50/drivers/net/phy/sfp-bus.c
+@@ -26,7 +26,6 @@ struct sfp_bus {
+ 	bool started;
+ };
+ 
+-
+ int sfp_parse_port(struct sfp_bus *bus, const struct sfp_eeprom_id *id,
+ 		   unsigned long *support)
+ {
+@@ -208,7 +207,6 @@ void sfp_parse_support(struct sfp_bus *b
+ }
+ EXPORT_SYMBOL_GPL(sfp_parse_support);
+ 
+-
+ static LIST_HEAD(sfp_buses);
+ static DEFINE_MUTEX(sfp_mutex);
+ 
+@@ -295,7 +293,6 @@ static void sfp_unregister_bus(struct sf
+ 	bus->registered = false;
+ }
+ 
+-
+ int sfp_get_module_info(struct sfp_bus *bus, struct ethtool_modinfo *modinfo)
+ {
+ 	if (!bus->registered)
+@@ -305,7 +302,7 @@ int sfp_get_module_info(struct sfp_bus *
+ EXPORT_SYMBOL_GPL(sfp_get_module_info);
+ 
+ int sfp_get_module_eeprom(struct sfp_bus *bus, struct ethtool_eeprom *ee,
+-	u8 *data)
++			  u8 *data)
+ {
+ 	if (!bus->registered)
+ 		return -ENOIOCTLCMD;
+@@ -330,8 +327,8 @@ void sfp_upstream_stop(struct sfp_bus *b
+ EXPORT_SYMBOL_GPL(sfp_upstream_stop);
+ 
+ struct sfp_bus *sfp_register_upstream(struct device_node *np,
+-	struct net_device *ndev, void *upstream,
+-	const struct sfp_upstream_ops *ops)
++				      struct net_device *ndev, void *upstream,
++				      const struct sfp_upstream_ops *ops)
+ {
+ 	struct sfp_bus *bus = sfp_bus_get(np);
+ 	int ret = 0;
+@@ -369,7 +366,6 @@ void sfp_unregister_upstream(struct sfp_
+ }
+ EXPORT_SYMBOL_GPL(sfp_unregister_upstream);
+ 
+-
+ /* Socket driver entry points */
+ int sfp_add_phy(struct sfp_bus *bus, struct phy_device *phydev)
+ {
+@@ -396,7 +392,6 @@ void sfp_remove_phy(struct sfp_bus *bus)
+ }
+ EXPORT_SYMBOL_GPL(sfp_remove_phy);
+ 
+-
+ void sfp_link_up(struct sfp_bus *bus)
+ {
+ 	const struct sfp_upstream_ops *ops = sfp_get_upstream_ops(bus);
+Index: linux-4.14.50/drivers/net/phy/sfp.c
+===================================================================
+--- linux-4.14.50.orig/drivers/net/phy/sfp.c
++++ linux-4.14.50/drivers/net/phy/sfp.c
+@@ -88,15 +88,12 @@ static const enum gpiod_flags gpio_flags
+ #define T_PROBE_INIT	msecs_to_jiffies(300)
+ #define T_PROBE_RETRY	msecs_to_jiffies(100)
+ 
+-/*
+- * SFP modules appear to always have their PHY configured for bus address
++/* SFP modules appear to always have their PHY configured for bus address
+  * 0x56 (which with mdio-i2c, translates to a PHY address of 22).
+  */
+ #define SFP_PHY_ADDR	22
+ 
+-/*
+- * Give this long for the PHY to reset.
+- */
++/* Give this long for the PHY to reset. */
+ #define T_PHY_RESET_MS	50
+ 
+ static DEFINE_MUTEX(sfp_mutex);
+@@ -150,10 +147,10 @@ static void sfp_gpio_set_state(struct sf
+ 		/* If the module is present, drive the signals */
+ 		if (sfp->gpio[GPIO_TX_DISABLE])
+ 			gpiod_direction_output(sfp->gpio[GPIO_TX_DISABLE],
+-						state & SFP_F_TX_DISABLE);
++					       state & SFP_F_TX_DISABLE);
+ 		if (state & SFP_F_RATE_SELECT)
+ 			gpiod_direction_output(sfp->gpio[GPIO_RATE_SELECT],
+-						state & SFP_F_RATE_SELECT);
++					       state & SFP_F_RATE_SELECT);
+ 	} else {
+ 		/* Otherwise, let them float to the pull-ups */
+ 		if (sfp->gpio[GPIO_TX_DISABLE])
+@@ -164,7 +161,7 @@ static void sfp_gpio_set_state(struct sf
+ }
+ 
+ static int sfp__i2c_read(struct i2c_adapter *i2c, u8 bus_addr, u8 dev_addr,
+-	void *buf, size_t len)
++			 void *buf, size_t len)
+ {
+ 	struct i2c_msg msgs[2];
+ 	int ret;
+@@ -186,7 +183,7 @@ static int sfp__i2c_read(struct i2c_adap
+ }
+ 
+ static int sfp_i2c_read(struct sfp *sfp, bool a2, u8 addr, void *buf,
+-	size_t len)
++			size_t len)
+ {
+ 	return sfp__i2c_read(sfp->i2c, a2 ? 0x51 : 0x50, addr, buf, len);
+ }
+@@ -220,7 +217,6 @@ static int sfp_i2c_configure(struct sfp
+ 	return 0;
+ }
+ 
+-
+ /* Interface */
+ static unsigned int sfp_get_state(struct sfp *sfp)
+ {
+@@ -295,7 +291,8 @@ static void sfp_sm_next(struct sfp *sfp,
+ 	sfp_sm_set_timer(sfp, timeout);
+ }
+ 
+-static void sfp_sm_ins_next(struct sfp *sfp, unsigned int state, unsigned int timeout)
++static void sfp_sm_ins_next(struct sfp *sfp, unsigned int state,
++			    unsigned int timeout)
+ {
+ 	sfp->sm_mod_state = state;
+ 	sfp_sm_set_timer(sfp, timeout);
+@@ -370,7 +367,8 @@ static void sfp_sm_link_check_los(struct
+ static void sfp_sm_fault(struct sfp *sfp, bool warn)
+ {
+ 	if (sfp->sm_retries && !--sfp->sm_retries) {
+-		dev_err(sfp->dev, "module persistently indicates fault, disabling\n");
++		dev_err(sfp->dev,
++			"module persistently indicates fault, disabling\n");
+ 		sfp_sm_next(sfp, SFP_S_TX_DISABLE, 0);
+ 	} else {
+ 		if (warn)
+@@ -653,7 +651,7 @@ static int sfp_module_info(struct sfp *s
+ }
+ 
+ static int sfp_module_eeprom(struct sfp *sfp, struct ethtool_eeprom *ee,
+-	u8 *data)
++			     u8 *data)
+ {
+ 	unsigned int first, last, len;
+ 	int ret;
diff --git a/target/linux/mvebu/patches-4.14/9085-sfp-fix-sparse-warning.patch b/target/linux/mvebu/patches-4.14/9085-sfp-fix-sparse-warning.patch
new file mode 100644
index 0000000..8ddaac4
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/9085-sfp-fix-sparse-warning.patch
@@ -0,0 +1,30 @@
+From 9e5400f52f431012cbeaaa72c6dc3bb5d8cfd44d Mon Sep 17 00:00:00 2001
+From: Russell King <rmk+kernel@armlinux.org.uk>
+Date: Fri, 1 Dec 2017 10:24:58 +0000
+Subject: [PATCH 85/90] sfp: fix sparse warning
+
+drivers/net/phy/sfp-bus.c:298:13: warning: context imbalance in 'sfp_bus_release' - wrong count at exit
+
+Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
+Reviewed-by: Florian Fainelli <f.fainelli@gmail.com>
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ drivers/net/phy/sfp-bus.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/net/phy/sfp-bus.c b/drivers/net/phy/sfp-bus.c
+index f8c0327e6ad7..f2513b9e00cd 100644
+--- a/drivers/net/phy/sfp-bus.c
++++ b/drivers/net/phy/sfp-bus.c
+@@ -246,7 +246,7 @@ static struct sfp_bus *sfp_bus_get(struct device_node *np)
+ 	return found;
+ }
+ 
+-static void sfp_bus_release(struct kref *kref) __releases(sfp_mutex)
++static void sfp_bus_release(struct kref *kref)
+ {
+ 	struct sfp_bus *bus = container_of(kref, struct sfp_bus, kref);
+ 
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/9086-sfp-don-t-guess-support-from-connector-type.patch b/target/linux/mvebu/patches-4.14/9086-sfp-don-t-guess-support-from-connector-type.patch
new file mode 100644
index 0000000..1de9002
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/9086-sfp-don-t-guess-support-from-connector-type.patch
@@ -0,0 +1,57 @@
+From bcd854af26d87f8b5b874ec0a97b7fd1b7365223 Mon Sep 17 00:00:00 2001
+From: Russell King <rmk+kernel@armlinux.org.uk>
+Date: Fri, 29 Dec 2017 12:15:17 +0000
+Subject: [PATCH 86/90] sfp: don't guess support from connector type
+
+Don't try to guess the support mask from the connector type - this is
+mostly irrelevant to the speeds that the transceiver supports.
+
+Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ drivers/net/phy/sfp-bus.c | 29 -----------------------------
+ 1 file changed, 29 deletions(-)
+
+diff --git a/drivers/net/phy/sfp-bus.c b/drivers/net/phy/sfp-bus.c
+index f2513b9e00cd..43dfe187378f 100644
+--- a/drivers/net/phy/sfp-bus.c
++++ b/drivers/net/phy/sfp-bus.c
+@@ -175,35 +175,6 @@ void sfp_parse_support(struct sfp_bus *bus, const struct sfp_eeprom_id *id,
+ 		if (id->base.br_nominal >= 12)
+ 			phylink_set(support, 1000baseX_Full);
+ 	}
+-
+-	switch (id->base.connector) {
+-	case SFP_CONNECTOR_SC:
+-	case SFP_CONNECTOR_FIBERJACK:
+-	case SFP_CONNECTOR_LC:
+-	case SFP_CONNECTOR_MT_RJ:
+-	case SFP_CONNECTOR_MU:
+-	case SFP_CONNECTOR_OPTICAL_PIGTAIL:
+-		break;
+-
+-	case SFP_CONNECTOR_UNSPEC:
+-		if (id->base.e1000_base_t)
+-			break;
+-
+-	case SFP_CONNECTOR_SG: /* guess */
+-	case SFP_CONNECTOR_MPO_1X12:
+-	case SFP_CONNECTOR_MPO_2X16:
+-	case SFP_CONNECTOR_HSSDC_II:
+-	case SFP_CONNECTOR_COPPER_PIGTAIL:
+-	case SFP_CONNECTOR_NOSEPARATE:
+-	case SFP_CONNECTOR_MXC_2X16:
+-	default:
+-		/* a guess at the supported link modes */
+-		dev_warn(bus->sfp_dev,
+-			 "Guessing link modes, please report...\n");
+-		phylink_set(support, 1000baseT_Half);
+-		phylink_set(support, 1000baseT_Full);
+-		break;
+-	}
+ }
+ EXPORT_SYMBOL_GPL(sfp_parse_support);
+ 
+-- 
+2.17.1
+
diff --git a/target/linux/mvebu/patches-4.14/9087-sfp-add-support-for-1000Base-PX-and-1000Base-BX10.patch b/target/linux/mvebu/patches-4.14/9087-sfp-add-support-for-1000Base-PX-and-1000Base-BX10.patch
new file mode 100644
index 0000000..cf911e1
--- /dev/null
+++ b/target/linux/mvebu/patches-4.14/9087-sfp-add-support-for-1000Base-PX-and-1000Base-BX10.patch
@@ -0,0 +1,60 @@
+From a5077f273e43d670e8213f866351eeaeecece5d7 Mon Sep 17 00:00:00 2001
+From: Russell King <rmk+kernel@armlinux.org.uk>
+Date: Fri, 29 Dec 2017 12:15:23 +0000
+Subject: [PATCH 87/90] sfp: add support for 1000Base-PX and 1000Base-BX10
+
+Add support for decoding the transceiver information for 1000Base-PX and
+1000Base-BX10.  These use 1000BASE-X protocol.
+
+Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
+Signed-off-by: David S. Miller <davem@davemloft.net>
+---
+ drivers/net/phy/sfp-bus.c | 21 +++++++++++++++++++++
+ 1 file changed, 21 insertions(+)
+
+diff --git a/drivers/net/phy/sfp-bus.c b/drivers/net/phy/sfp-bus.c
+index 43dfe187378f..deade7d45331 100644
+--- a/drivers/net/phy/sfp-bus.c
++++ b/drivers/net/phy/sfp-bus.c
+@@ -120,10 +120,26 @@ EXPORT_SYMBOL_GPL(sfp_parse_interface);
+ void sfp_parse_support(struct sfp_bus *bus, const struct sfp_eeprom_id *id,
+ 		       unsigned long *support)
+ {
++	unsigned int br_min, br_nom, br_max;
++
+ 	phylink_set(support, Autoneg);
+ 	phylink_set(support, Pause);
+ 	phylink_set(support, Asym_Pause);
+ 
++	/* Decode the bitrate information to MBd */
++	br_min = br_nom = br_max = 0;
++	if (id->base.br_nominal) {
++		if (id->base.br_nominal != 255) {
++			br_nom = id->base.br_nominal * 100;
++			br_min = br_nom + id->base.br_nominal * id->ext.br_min;
++			br_max = br_nom + id->base.br_nominal * id->ext.br_max;
++		} else if (id->ext.br_max) {
++			br_nom = 250 * id->ext.br_max;
++			br_max = br_nom + br_nom * id->ext.br_min / 100;
++			br_min = br_nom - br_nom * id->ext.br_min / 100;
++		}
++	}
++
+ 	/* Set ethtool support from the compliance fields. */
+ 	if (id->base.e10g_base_sr)
+ 		phylink_set(support, 10000baseSR_Full);
+@@ -142,6 +158,11 @@ void sfp_parse_support(struct sfp_bus *bus, const struct sfp_eeprom_id *id,
+ 		phylink_set(support, 1000baseT_Full);
+ 	}
+ 
++	/* 1000Base-PX or 1000Base-BX10 */
++	if ((id->base.e_base_px || id->base.e_base_bx10) &&
++	    br_min <= 1300 && br_max >= 1200)
++		phylink_set(support, 1000baseX_Full);
++
+ 	switch (id->base.extended_cc) {
+ 	case 0x00: /* Unspecified */
+ 		break;
+-- 
+2.17.1
+
-- 
2.18.0

